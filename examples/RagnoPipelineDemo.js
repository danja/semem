/**
 * RagnoPipelineDemo.js - Complete Ragno Knowledge Graph Pipeline Demo
 * 
 * This script demonstrates the full Ragno pipeline for knowledge graph construction:
 * 
 * 1. **Corpus Decomposition**: Breaks down text documents into semantic units and entities
 * 2. **Attribute Augmentation**: Generates descriptive attributes for important graph nodes
 * 3. **Community Detection**: Identifies clusters of related entities and concepts
 * 4. **Embedding Enrichment**: Creates vector representations for semantic similarity
 * 5. **Similarity Linking**: Connects semantically similar elements in the graph
 * 
 * The Ragno Pipeline Process:
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚   Text Corpus   â”‚â”€â”€â”€â–¶â”‚  Decomposition  â”‚â”€â”€â”€â–¶â”‚  Knowledge Graph â”‚
 * â”‚   (Documents)   â”‚    â”‚   (Units+Ents)  â”‚    â”‚  (Nodes+Edges)  â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *                                                        â”‚
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚  Enriched KG    â”‚â—€â”€â”€â”€â”‚   Embeddings    â”‚â—€â”€â”€â”€â”‚   Augmentation  â”‚
 * â”‚ (w/ Similarities)â”‚    â”‚  (Vectors)      â”‚    â”‚ (Attributes+Comm)â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 * 
 * Technologies Used:
 * - **Ollama**: Local LLM for text processing and attribute generation
 * - **nomic-embed-text**: Vector embeddings for semantic similarity
 * - **Ragno Ontology**: RDF vocabulary for knowledge representation
 * - **Graph Algorithms**: Community detection and centrality analysis
 * 
 * Prerequisites:
 * - Ollama running with qwen2:1.5b and nomic-embed-text models
 * - docs/ragno/ragno-config.json configuration file
 * - All Ragno pipeline modules properly installed
 */

import logger from 'loglevel'
import { decomposeCorpus } from '../src/ragno/decomposeCorpus.js'
import { augmentWithAttributes } from '../src/ragno/augmentWithAttributes.js'
import { aggregateCommunities } from '../src/ragno/aggregateCommunities.js'
import { enrichWithEmbeddings } from '../src/ragno/enrichWithEmbeddings.js'
import LLMHandler from '../src/handlers/LLMHandler.js'
import EmbeddingHandler from '../src/handlers/EmbeddingHandler.js'
import OllamaConnector from '../src/connectors/OllamaConnector.js'
import loadRagnoConfig from './loadRagnoConfig.js'

// Configure logging
logger.setLevel('debug')

let ollamaConnector = null

async function shutdown(signal) {
    logger.info(`\\nğŸ”Œ Received ${signal}, starting graceful shutdown...`)
    if (ollamaConnector) {
        try {
            // Cleanup any Ollama connections if needed
            logger.info('ğŸ§¹ Cleaning up Ollama connector...')
            logger.info('âœ… Cleanup complete')
            process.exit(0)
        } catch (error) {
            logger.error('âŒ Error during cleanup:', error)
            process.exit(1)
        }
    } else {
        process.exit(0)
    }
}

// Setup graceful shutdown handlers
process.on('SIGTERM', () => shutdown('SIGTERM'))
process.on('SIGINT', () => shutdown('SIGINT'))
process.on('uncaughtException', async (error) => {
    logger.error('ğŸ’¥ Uncaught Exception:', error)
    await shutdown('uncaughtException')
})
process.on('unhandledRejection', async (reason, promise) => {
    logger.error('ğŸ’¥ Unhandled Rejection at:', promise, 'reason:', reason)
    await shutdown('unhandledRejection')
})

function createSampleCorpus() {
    return [
        {
            content: "Geoffrey Hinton is often called the 'Godfather of AI' for his pioneering work in deep learning and neural networks. He developed the backpropagation algorithm which revolutionized machine learning. Hinton worked at the University of Toronto and Google, making breakthrough contributions to artificial intelligence research.",
            source: "ai_pioneers_hinton.txt",
            metadata: { author: "AI Research Journal", year: 2023, topic: "Deep Learning" }
        },
        {
            content: "Yann LeCun developed convolutional neural networks (CNNs) which became fundamental to computer vision. His work on LeNet-5 laid the groundwork for modern image recognition systems. LeCun serves as Chief AI Scientist at Meta and is a professor at NYU, continuing to advance the field of artificial intelligence.",
            source: "ai_pioneers_lecun.txt", 
            metadata: { author: "AI Research Journal", year: 2023, topic: "Computer Vision" }
        },
        {
            content: "Yoshua Bengio made significant contributions to deep learning, particularly in the areas of neural language models and representation learning. His research on attention mechanisms and generative models has influenced modern AI architectures. Bengio co-founded Element AI and leads the Montreal Institute for Learning Algorithms (MILA).",
            source: "ai_pioneers_bengio.txt",
            metadata: { author: "AI Research Journal", year: 2023, topic: "NLP" }
        },
        {
            content: "The transformer architecture, introduced in 'Attention Is All You Need', revolutionized natural language processing. Transformers use self-attention mechanisms to process sequences in parallel, enabling the development of large language models like GPT and BERT. This architecture has become the foundation for modern AI systems.",
            source: "transformer_architecture.txt",
            metadata: { author: "Google Research", year: 2017, topic: "Transformers" }
        },
        {
            content: "Large Language Models (LLMs) like GPT-4, Claude, and LLaMA represent the current state-of-the-art in natural language processing. These models demonstrate emergent capabilities including reasoning, code generation, and creative writing. They are trained on massive text corpora using transformer architectures and have billions of parameters.",
            source: "modern_llms.txt",
            metadata: { author: "AI Today", year: 2024, topic: "LLMs" }
        }
    ]
}

async function displayGraphStats(graph, step) {
    logger.info(`\\nğŸ“Š Knowledge Graph Statistics - ${step}:`)
    logger.info(`   ğŸ“ Units: ${graph.units?.length || 0}`)
    logger.info(`   ğŸ·ï¸  Entities: ${graph.entities?.length || 0}`)
    logger.info(`   ğŸ”— Relationships: ${graph.relationships?.length || 0}`)
    if (graph.attributes) {
        logger.info(`   ğŸ“‹ Attributes: ${Object.keys(graph.attributes).length}`)
    }
    if (graph.communities) {
        logger.info(`   ğŸ‘¥ Communities: ${graph.communities.length}`)
    }
    if (graph.embeddings) {
        logger.info(`   ğŸ§  Embeddings: ${Object.keys(graph.embeddings).length}`)
    }
    if (graph.similarityLinks) {
        logger.info(`   ğŸ”— Similarity Links: ${graph.similarityLinks.length}`)
    }
}

async function main() {
    logger.info('ğŸš€ Starting Ragno Knowledge Graph Pipeline Demo')
    logger.info('=' .repeat(70))
    
    try {
        // Step 1: Load Configuration
        logger.info('âš™ï¸  Step 1: Loading Ragno configuration...')
        const config = await loadRagnoConfig()
        logger.info('âœ… Configuration loaded successfully')
        logger.info(`ğŸ·ï¸  Version: ${config.version}`)
        logger.info(`ğŸ¤– LLM Model: ${config.decomposition.llm.model} â†’ qwen2:1.5b (override)`)
        logger.info(`ğŸ§  Embedding Model: ${config.enrichment.embedding.model}`)
        logger.info(`ğŸ“ Embedding Dimensions: ${config.enrichment.embedding.dimensions}`)

        // Step 2: Initialize Ollama Connector
        logger.info('\\nğŸ¤– Step 2: Initializing Ollama connector...')
        ollamaConnector = new OllamaConnector('http://localhost:11434', 'qwen2:1.5b')
        await ollamaConnector.initialize()
        logger.info('âœ… Ollama connector initialized successfully')

        // Step 3: Setup Handlers
        logger.info('\\nğŸ”§ Step 3: Setting up LLM and Embedding handlers...')
        
        const llmProvider = {
            generateChat: ollamaConnector.generateChat.bind(ollamaConnector),
            generateCompletion: ollamaConnector.generateCompletion.bind(ollamaConnector),
            generateEmbedding: ollamaConnector.generateEmbedding.bind(ollamaConnector)
        }

        const embeddingProvider = {
            generateEmbedding: ollamaConnector.generateEmbedding.bind(ollamaConnector)
        }

        const cacheManager = { 
            get: () => undefined, 
            set: () => {},
            has: () => false,
            delete: () => false,
            clear: () => {}
        }

        const llmHandler = new LLMHandler(
            llmProvider,
            'qwen2:1.5b', // Use available model
            config.decomposition.llm.temperature
        )

        const embeddingHandler = new EmbeddingHandler(
            embeddingProvider,
            'nomic-embed-text', // Use available model
            config.enrichment.embedding.dimensions,
            cacheManager
        )

        const embeddingFn = text => embeddingHandler.generateEmbedding(text)
        logger.info('âœ… Handlers configured successfully')

        // Step 4: Prepare Sample Corpus
        logger.info('\\nğŸ“š Step 4: Preparing sample corpus...')
        const textChunks = createSampleCorpus()
        logger.info(`ğŸ“„ Created corpus with ${textChunks.length} documents`)
        textChunks.forEach((chunk, i) => {
            logger.info(`   ${i+1}. ${chunk.source} (${chunk.content.length} chars)`)
        })

        // Step 5: Decompose Corpus
        logger.info('\\nğŸ” Step 5: Decomposing corpus into semantic units and entities...')
        logger.info('âš™ï¸  Running entity extraction and relationship identification...')
        const startDecomp = Date.now()
        const knowledgeGraph = await decomposeCorpus(textChunks, llmHandler)
        const decompTime = Date.now() - startDecomp
        logger.info(`âœ… Decomposition completed in ${decompTime}ms`)
        await displayGraphStats(knowledgeGraph, 'After Decomposition')

        // Step 6: Augment with Attributes  
        logger.info('\\nğŸ“‹ Step 6: Augmenting important nodes with descriptive attributes...')
        logger.info('ğŸ” Identifying high-importance nodes for attribute generation...')
        const startAugment = Date.now()
        const { attributes } = await augmentWithAttributes(knowledgeGraph, llmHandler, { 
            topK: 5 // Generate attributes for top 5 most important nodes
        })
        knowledgeGraph.attributes = attributes
        const augmentTime = Date.now() - startAugment
        logger.info(`âœ… Attribute augmentation completed in ${augmentTime}ms`)
        await displayGraphStats(knowledgeGraph, 'After Augmentation')

        // Display some generated attributes
        if (attributes && Object.keys(attributes).length > 0) {
            logger.info('ğŸ·ï¸  Sample generated attributes:')
            Object.entries(attributes).slice(0, 3).forEach(([nodeId, nodeAttrs]) => {
                logger.info(`   ğŸ“ ${nodeId}: ${nodeAttrs.length} attributes`)
                nodeAttrs.slice(0, 2).forEach(attr => {
                    logger.info(`      â€¢ ${attr.type}: ${attr.description.substring(0, 60)}...`)
                })
            })
        }

        // Step 7: Community Detection
        logger.info('\\nğŸ‘¥ Step 7: Detecting communities and generating community attributes...')
        logger.info('ğŸ” Running graph clustering to identify related concept groups...')
        const startCommunity = Date.now()
        const { communities, attributes: commAttrs } = await aggregateCommunities(knowledgeGraph, llmHandler, { 
            minCommunitySize: 2 
        })
        knowledgeGraph.communities = communities
        knowledgeGraph.communityAttributes = commAttrs
        const communityTime = Date.now() - startCommunity
        logger.info(`âœ… Community detection completed in ${communityTime}ms`)
        await displayGraphStats(knowledgeGraph, 'After Community Detection')

        // Display community information
        if (communities && communities.length > 0) {
            logger.info(`ğŸ¯ Detected ${communities.length} communities:`)
            communities.forEach((community, i) => {
                logger.info(`   ${i+1}. Community ${community.id}: ${community.members.length} members`)
                logger.info(`      Members: [${community.members.slice(0, 3).join(', ')}${community.members.length > 3 ? '...' : ''}]`)
            })
        }

        // Step 8: Embedding Enrichment
        logger.info('\\nğŸ§  Step 8: Generating embeddings and computing similarity links...')
        logger.info('âš™ï¸  Creating vector representations for semantic search...')
        const startEmbedding = Date.now()
        const { embeddings, similarityLinks } = await enrichWithEmbeddings(knowledgeGraph, embeddingFn, { 
            similarityThreshold: 0.3 // Only keep high-similarity links
        })
        knowledgeGraph.embeddings = embeddings
        knowledgeGraph.similarityLinks = similarityLinks
        const embeddingTime = Date.now() - startEmbedding
        logger.info(`âœ… Embedding enrichment completed in ${embeddingTime}ms`)
        await displayGraphStats(knowledgeGraph, 'Final Knowledge Graph')

        // Display similarity links
        if (similarityLinks && similarityLinks.length > 0) {
            logger.info(`ğŸ”— Generated ${similarityLinks.length} similarity links:`)
            similarityLinks.slice(0, 5).forEach((link, i) => {
                logger.info(`   ${i+1}. ${link.source} â†” ${link.target} (similarity: ${link.similarity.toFixed(3)})`)
            })
            if (similarityLinks.length > 5) {
                logger.info(`   ... and ${similarityLinks.length - 5} more links`)
            }
        }

        // Final Summary
        logger.info('\\nğŸ‰ Ragno Pipeline Completed Successfully!')
        logger.info('=' .repeat(70))
        logger.info('ğŸ“Š Final Knowledge Graph Statistics:')
        logger.info(`   ğŸ“ Semantic Units: ${knowledgeGraph.units?.length || 0}`)
        logger.info(`   ğŸ·ï¸  Named Entities: ${knowledgeGraph.entities?.length || 0}`) 
        logger.info(`   ğŸ”— Relationships: ${knowledgeGraph.relationships?.length || 0}`)
        logger.info(`   ğŸ“‹ Node Attributes: ${Object.keys(knowledgeGraph.attributes || {}).length}`)
        logger.info(`   ğŸ‘¥ Communities: ${knowledgeGraph.communities?.length || 0}`)
        logger.info(`   ğŸ§  Embeddings: ${Object.keys(knowledgeGraph.embeddings || {}).length}`)
        logger.info(`   ğŸ”— Similarity Links: ${knowledgeGraph.similarityLinks?.length || 0}`)
        
        const totalTime = decompTime + augmentTime + communityTime + embeddingTime
        logger.info(`\\nâ±ï¸  Total Processing Time: ${totalTime}ms`)
        logger.info(`   ğŸ” Decomposition: ${decompTime}ms`)
        logger.info(`   ğŸ“‹ Augmentation: ${augmentTime}ms`) 
        logger.info(`   ğŸ‘¥ Communities: ${communityTime}ms`)
        logger.info(`   ğŸ§  Embeddings: ${embeddingTime}ms`)

        logger.info('\\nğŸ’¡ The knowledge graph is now ready for:')
        logger.info('   ğŸ” Semantic search and similarity queries')
        logger.info('   ğŸ“Š Graph analytics and community analysis') 
        logger.info('   ğŸ¤– AI-powered question answering')
        logger.info('   ğŸ“ˆ Knowledge discovery and insights')
        logger.info('   ğŸ—„ï¸  Export to RDF/SPARQL triple stores')

        // Sample of actual extracted data
        logger.info('\\nğŸ“‹ Sample Extracted Entities:')
        if (knowledgeGraph.entities && knowledgeGraph.entities.length > 0) {
            knowledgeGraph.entities.slice(0, 5).forEach((entity, i) => {
                logger.info(`   ${i+1}. ${entity.name} (${entity.type || 'Unknown'})`)
            })
        }

        logger.info('\\nğŸ“ Sample Semantic Units:')
        if (knowledgeGraph.units && knowledgeGraph.units.length > 0) {
            knowledgeGraph.units.slice(0, 3).forEach((unit, i) => {
                logger.info(`   ${i+1}. "${unit.content.substring(0, 80)}..."`)
            })
        }

    } catch (error) {
        logger.error('ğŸ’¥ Fatal error in Ragno pipeline:', error.message)
        logger.error('ğŸ“‹ Stack trace:', error.stack)
        throw error
    }
}

// Run the demo
main().catch(async (error) => {
    logger.error('ğŸ’¥ Demo failed:', error.message)
    await shutdown('fatal error')
})