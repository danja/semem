# Feedback is All You Need (for multi-hop RAG)

Yesterday I got to an unplanned proof-of-concept with [Semem](https://github.com/danja/semem) (*"Semantic Web Memory for Intelligent Agents"*). I haven't looked closely at code detail yet, so Claude might be tricking me here and there, but the following is how I believe things are working. *I am in the process of refactoring the code to be more modular/reusable, so any bits of delusion should soon become clear.*

I was experimenting with a workflow on real data, from the [BeerQA](https://beerqa.github.io/) dataset. This contains a stack of multi-hop questions with answers, intended as training data, plus a set of questions without answers for test purposes. I was looking at the test questions.
A construct that has proven key is `ragno:Corpuscle`, where a *corpuscle* is a small part of a corpus. It's quite loosely defined, probably best though of as a bag of *stuff*.

## Version 1 : concept extraction; Wikipedia augmentation

The workflow went like this :

1. Read the data and create initial question corpuscles in the knowledge graph
2. Enhance corpuscles with vector embeddings and extracted concepts (using LLM)
3. Search Wikipedia on the concepts, with HyDE* fallback
4. Identify Wikipedia targets that may relate to questions using similarity search and concept matching
5. Fetch full Wikipedia pages, convert to markdown, generate embeddings and update corpuscles accordingly
6. Filter corpuscles according to relevance (using Zoom, Pan, Tilt navigation)
7. Create a prompt from the initial question with context from the discovered corpuscles, and get the LLM to complete

\* the HyDE algorithm is about generating a hypothetical answer to a question, and using this to find additional relevant information via similarity matching. Here it was used to generate more candidate concepts for Wikipedia search.

This workflow kind-of worked. The only *slight* issue was that out of the 100 questions attempted, it only looked like one or two had even barely useful answers. This wasn't expected because there should have been enough info to produce at least a few reasonable responses.

The implementation of this workflow is described in [BeerQA Workflow](https://danja.github.io/semem/manual/beerqa.html).

## Version 2 : as above plus relationships and analytics

The operations involved here are a bit mixed up, but the first part was about augmenting the graph.
It used the delightfully named `RelationshipBuilder.js` which has the ability to :

* find similarity relationships (using the already-created embeddings)
* match entities between question and content
* make connections between extracted concepts
* create community bridges to enhance graph connectivity

The second part did relevance ranking on what was in the store so far, doing :

* K-core decomposition to identify structurally important nodes
* Centrality Analysis to calculate "betweenness"
* Composite Scoring - weighted combination of the above

The last two stages followed the same pattern as those in Version 1, the ZPT idea used to filter the available info, this provided context for an LLM to (hopefully) answer the question.

The implementation of this workflow is described in [BeerQA Enhanced Workflow](https://danja.github.io/semem/manual/beerqa-2.html).

Summary : results were more promising, a couple (out of 100) of questions were given moderately acceptable answers. It was fairly clear what was lacking...

**Need more datas!**

## Version 3 : as above, plus Wikidata

Again, this built on the above, with an additional step inserted to query Wikidata on concepts extracted by the above (rather confusingly in implementation, it went into the ZPT navigation step).

Results were considerably better, in a sense. The addition of Wikidata info made a big difference in the quality of the final result. A couple more questions did get more tolerable answers, but more significantly - **observation** - many of the results described what information they lacked in order to answer the questions.

Implementation described in [BeerQA Enhanced Workflow v2 + Wikidata Integration](https://danja.github.io/semem/manual/beerqa-wikidata.html).

## Version 4 : as above, plus feedback loops

The observation made this a necessary thing to try. Ask the LLM looking at the results to judge if there was information missing - which was pretty obvious from the shape of the responses. If so, **rewrite those parts as new questions** and resubmit into the sequence for further augmentation with info from Wikipedia & Wikidata.

This is a very time consuming procedure (I do need to check what delays I've got in place, I did go mega-cautious on rate limiting to be on the safe side). Because of this, so far I've only run the full workflow on 3 questions. But the answers to those were **very compelling**.

I'm not going to try a longer run until after refactoring, checking the limits etc, so everything is a lot more controllable, a little more optimal timewise. But I'm reasonably confident that the approach - that I hadn't even thought of when I started with the workflow at the top of this page - works.

Current implementation described in [BeerQA Iterative Feedback Workflow (v3)](https://danja.github.io/semem/manual/beerqa-feedback.html)
