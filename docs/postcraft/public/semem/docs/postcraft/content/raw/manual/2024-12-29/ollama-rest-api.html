<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <link rel="stylesheet" href="/css/fonts.css" type="text/css"/>
        <link rel="stylesheet" href="/css/grid-columns.css" type="text/css"/>
        <link rel="stylesheet" href="/css/style.css" type="text/css"/>
        <title>API</title>
    </head>
    <!-- POST PAGE TEMPLATE -->
    <body>
        <header id="entry-header">
            <h1 class="post-title h-cinzel">
                
            </h1>
        </header>
        <h1>API</h1>
<h2>Endpoints</h2>
<ul>
<li><a href=\"#generate-a-completion\">Generate a completion</a></li>
<li><a href=\"#generate-a-chat-completion\">Generate a chat completion</a></li>
<li><a href=\"#create-a-model\">Create a Model</a></li>
<li><a href=\"#list-local-models\">List Local Models</a></li>
<li><a href=\"#show-model-information\">Show Model Information</a></li>
<li><a href=\"#copy-a-model\">Copy a Model</a></li>
<li><a href=\"#delete-a-model\">Delete a Model</a></li>
<li><a href=\"#pull-a-model\">Pull a Model</a></li>
<li><a href=\"#push-a-model\">Push a Model</a></li>
<li><a href=\"#generate-embeddings\">Generate Embeddings</a></li>
<li><a href=\"#list-running-models\">List Running Models</a></li>
</ul>
<h2>Conventions</h2>
<h3>Model names</h3>
<p>Model names follow a <code>model:tag</code> format, where <code>model</code> can have an optional namespace such as <code>example/model</code>. Some examples are <code>orca-mini:3b-q4_1</code> and <code>llama3:70b</code>. The tag is optional and, if not provided, will default to <code>latest</code>. The tag is used to identify a specific version.</p>
<h3>Durations</h3>
<p>All durations are returned in nanoseconds.</p>
<h3>Streaming responses</h3>
<p>Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing <code>{&quot;stream&quot;: false}</code> for these endpoints.</p>
<h2>Generate a completion</h2>
<pre><code class=\"language-shell\">POST /api/generate
</code></pre>
<p>Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: (required) the <a href=\"#model-names\">model name</a></li>
<li><code>prompt</code>: the prompt to generate a response for</li>
<li><code>suffix</code>: the text after the model response</li>
<li><code>images</code>: (optional) a list of base64-encoded images (for multimodal models such as <code>llava</code>)</li>
</ul>
<p>Advanced parameters (optional):</p>
<ul>
<li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema</li>
<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>
<li><code>system</code>: system message to (overrides what is defined in the <code>Modelfile</code>)</li>
<li><code>template</code>: the prompt template to use (overrides what is defined in the <code>Modelfile</code>)</li>
<li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>
<li><code>raw</code>: if <code>true</code> no formatting will be applied to the prompt. You may choose to use the <code>raw</code> parameter if you are specifying a full templated prompt in your request to the API</li>
<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>
<li><code>context</code> (deprecated): the context parameter returned from a previous request to <code>/generate</code>, this can be used to keep a short conversational memory</li>
</ul>
<h4>Structured outputs</h4>
<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the <a href=\"#request-structured-outputs\">structured outputs</a> example below.</p>
<h4>JSON mode</h4>
<p>Enable JSON mode by setting the <code>format</code> parameter to <code>json</code>. This will structure the response as a valid JSON object. See the JSON mode <a href=\"#request-json-mode\">example</a> below.</p>
<blockquote>
<p>[!IMPORTANT]
It&#39;s important to instruct the model to use JSON in the <code>prompt</code>. Otherwise, the model may generate large amounts whitespace.</p>
</blockquote>
<h3>Examples</h3>
<h4>Generate request (Streaming)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;
}&#39;
</code></pre>
<h5>Response</h5>
<p>A stream of JSON objects is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,
  &quot;response&quot;: &quot;The&quot;,
  &quot;done&quot;: false
}
</code></pre>
<p>The final response in the stream also includes additional data about the generation:</p>
<ul>
<li><code>total_duration</code>: time spent generating the response</li>
<li><code>load_duration</code>: time spent in nanoseconds loading the model</li>
<li><code>prompt_eval_count</code>: number of tokens in the prompt</li>
<li><code>prompt_eval_duration</code>: time spent in nanoseconds evaluating the prompt</li>
<li><code>eval_count</code>: number of tokens in the response</li>
<li><code>eval_duration</code>: time in nanoseconds spent generating the response</li>
<li><code>context</code>: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory</li>
<li><code>response</code>: empty if the response was streamed, if not streamed, this will contain the full response</li>
</ul>
<p>To calculate how fast the response is generated in tokens per second (token/s), divide <code>eval_count</code> / <code>eval_duration</code> * <code>10^9</code>.</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,
  &quot;response&quot;: &quot;&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 10706818083,
  &quot;load_duration&quot;: 6338219291,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 130079000,
  &quot;eval_count&quot;: 259,
  &quot;eval_duration&quot;: 4232710000
}
</code></pre>
<h4>Request (No streaming)</h4>
<h5>Request</h5>
<p>A response can be received in one reply when streaming is off.</p>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,
  &quot;stream&quot;: false
}&#39;
</code></pre>
<h5>Response</h5>
<p>If <code>stream</code> is set to <code>false</code>, the response will be a single JSON object:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,
  &quot;response&quot;: &quot;The sky is blue because it is the color of the sky.&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 5043500667,
  &quot;load_duration&quot;: 5025959,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 325953000,
  &quot;eval_count&quot;: 290,
  &quot;eval_duration&quot;: 4709213000
}
</code></pre>
<h4>Request (with suffix)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;codellama:code&quot;,
  &quot;prompt&quot;: &quot;def compute_gcd(a, b):&quot;,
  &quot;suffix&quot;: &quot;    return result&quot;,
  &quot;options&quot;: {
    &quot;temperature&quot;: 0
  },
  &quot;stream&quot;: false
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;codellama:code&quot;,
  &quot;created_at&quot;: &quot;2024-07-22T20:47:51.147561Z&quot;,
  &quot;response&quot;: &quot;\\n  if a == 0:\\n    return b\\n  else:\\n    return compute_gcd(b % a, a)\\n\\ndef compute_lcm(a, b):\\n  result = (a * b) / compute_gcd(a, b)\\n&quot;,
  &quot;done&quot;: true,
  &quot;done_reason&quot;: &quot;stop&quot;,
  &quot;context&quot;: [...],
  &quot;total_duration&quot;: 1162761250,
  &quot;load_duration&quot;: 6683708,
  &quot;prompt_eval_count&quot;: 17,
  &quot;prompt_eval_duration&quot;: 201222000,
  &quot;eval_count&quot;: 63,
  &quot;eval_duration&quot;: 953997000
}
</code></pre>
<h4>Request (Structured outputs)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl -X POST http://localhost:11434/api/generate -H &quot;Content-Type: application/json&quot; -d &#39;{
  &quot;model&quot;: &quot;llama3.1:8b&quot;,
  &quot;prompt&quot;: &quot;Ollama is 22 years old and is busy saving the world. Respond using JSON&quot;,
  &quot;stream&quot;: false,
  &quot;format&quot;: {
    &quot;type&quot;: &quot;object&quot;,
    &quot;properties&quot;: {
      &quot;age&quot;: {
        &quot;type&quot;: &quot;integer&quot;
      },
      &quot;available&quot;: {
        &quot;type&quot;: &quot;boolean&quot;
      }
    },
    &quot;required&quot;: [
      &quot;age&quot;,
      &quot;available&quot;
    ]
  }
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.1:8b&quot;,
  &quot;created_at&quot;: &quot;2024-12-06T00:48:09.983619Z&quot;,
  &quot;response&quot;: &quot;{\\n  \\&quot;age\\&quot;: 22,\\n  \\&quot;available\\&quot;: true\\n}&quot;,
  &quot;done&quot;: true,
  &quot;done_reason&quot;: &quot;stop&quot;,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 1075509083,
  &quot;load_duration&quot;: 567678166,
  &quot;prompt_eval_count&quot;: 28,
  &quot;prompt_eval_duration&quot;: 236000000,
  &quot;eval_count&quot;: 16,
  &quot;eval_duration&quot;: 269000000
}
</code></pre>
<h4>Request (JSON mode)</h4>
<blockquote>
<p>[!IMPORTANT]
When <code>format</code> is set to <code>json</code>, the output will always be a well-formed JSON object. It&#39;s important to also instruct the model to respond in JSON.</p>
</blockquote>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;: &quot;What color is the sky at different times of the day? Respond using JSON&quot;,
  &quot;format&quot;: &quot;json&quot;,
  &quot;stream&quot;: false
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-11-09T21:07:55.186497Z&quot;,
  &quot;response&quot;: &quot;{\\n\\&quot;morning\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;blue\\&quot;\\n},\\n\\&quot;noon\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;blue-gray\\&quot;\\n},\\n\\&quot;afternoon\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;warm gray\\&quot;\\n},\\n\\&quot;evening\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;orange\\&quot;\\n}\\n}\\n&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 4648158584,
  &quot;load_duration&quot;: 4071084,
  &quot;prompt_eval_count&quot;: 36,
  &quot;prompt_eval_duration&quot;: 439038000,
  &quot;eval_count&quot;: 180,
  &quot;eval_duration&quot;: 4196918000
}
</code></pre>
<p>The value of <code>response</code> will be a string containing JSON similar to:</p>
<pre><code class=\"language-json\">{
  &quot;morning&quot;: {
    &quot;color&quot;: &quot;blue&quot;
  },
  &quot;noon&quot;: {
    &quot;color&quot;: &quot;blue-gray&quot;
  },
  &quot;afternoon&quot;: {
    &quot;color&quot;: &quot;warm gray&quot;
  },
  &quot;evening&quot;: {
    &quot;color&quot;: &quot;orange&quot;
  }
}
</code></pre>
<h4>Request (with images)</h4>
<p>To submit images to multimodal models such as <code>llava</code> or <code>bakllava</code>, provide a list of base64-encoded <code>images</code>:</p>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llava&quot;,
  &quot;prompt&quot;:&quot;What is in this picture?&quot;,
  &quot;stream&quot;: false,
  &quot;images&quot;: [&quot;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&quot;]
}&#39;
</code></pre>
<h4>Response</h4>
<pre><code>{
  &quot;model&quot;: &quot;llava&quot;,
  &quot;created_at&quot;: &quot;2023-11-03T15:36:02.583064Z&quot;,
  &quot;response&quot;: &quot;A happy cartoon character, which is cute and cheerful.&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 2938432250,
  &quot;load_duration&quot;: 2559292,
  &quot;prompt_eval_count&quot;: 1,
  &quot;prompt_eval_duration&quot;: 2195557000,
  &quot;eval_count&quot;: 44,
  &quot;eval_duration&quot;: 736432000
}
</code></pre>
<h4>Request (Raw Mode)</h4>
<p>In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the <code>raw</code> parameter to disable templating. Also note that raw mode will not return a context.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;mistral&quot;,
  &quot;prompt&quot;: &quot;[INST] why is the sky blue? [/INST]&quot;,
  &quot;raw&quot;: true,
  &quot;stream&quot;: false
}&#39;
</code></pre>
<h4>Request (Reproducible outputs)</h4>
<p>For reproducible outputs, set <code>seed</code> to a number:</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;mistral&quot;,
  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,
  &quot;options&quot;: {
    &quot;seed&quot;: 123
  }
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;mistral&quot;,
  &quot;created_at&quot;: &quot;2023-11-03T15:36:02.583064Z&quot;,
  &quot;response&quot;: &quot; The sky appears blue because of a phenomenon called Rayleigh scattering.&quot;,
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 8493852375,
  &quot;load_duration&quot;: 6589624375,
  &quot;prompt_eval_count&quot;: 14,
  &quot;prompt_eval_duration&quot;: 119039000,
  &quot;eval_count&quot;: 110,
  &quot;eval_duration&quot;: 1779061000
}
</code></pre>
<h4>Generate request (With options)</h4>
<p>If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the <code>options</code> parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,
  &quot;stream&quot;: false,
  &quot;options&quot;: {
    &quot;num_keep&quot;: 5,
    &quot;seed&quot;: 42,
    &quot;num_predict&quot;: 100,
    &quot;top_k&quot;: 20,
    &quot;top_p&quot;: 0.9,
    &quot;min_p&quot;: 0.0,
    &quot;typical_p&quot;: 0.7,
    &quot;repeat_last_n&quot;: 33,
    &quot;temperature&quot;: 0.8,
    &quot;repeat_penalty&quot;: 1.2,
    &quot;presence_penalty&quot;: 1.5,
    &quot;frequency_penalty&quot;: 1.0,
    &quot;mirostat&quot;: 1,
    &quot;mirostat_tau&quot;: 0.8,
    &quot;mirostat_eta&quot;: 0.6,
    &quot;penalize_newline&quot;: true,
    &quot;stop&quot;: [&quot;\\n&quot;, &quot;user:&quot;],
    &quot;numa&quot;: false,
    &quot;num_ctx&quot;: 1024,
    &quot;num_batch&quot;: 2,
    &quot;num_gpu&quot;: 1,
    &quot;main_gpu&quot;: 0,
    &quot;low_vram&quot;: false,
    &quot;vocab_only&quot;: false,
    &quot;use_mmap&quot;: true,
    &quot;use_mlock&quot;: false,
    &quot;num_thread&quot;: 8
  }
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,
  &quot;response&quot;: &quot;The sky is blue because it is the color of the sky.&quot;,
  &quot;done&quot;: true,
  &quot;context&quot;: [1, 2, 3],
  &quot;total_duration&quot;: 4935886791,
  &quot;load_duration&quot;: 534986708,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 107345000,
  &quot;eval_count&quot;: 237,
  &quot;eval_duration&quot;: 4289432000
}
</code></pre>
<h4>Load a model</h4>
<p>If an empty prompt is provided, the model will be loaded into memory.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;
}&#39;
</code></pre>
<h5>Response</h5>
<p>A single JSON object is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-12-18T19:52:07.071755Z&quot;,
  &quot;response&quot;: &quot;&quot;,
  &quot;done&quot;: true
}
</code></pre>
<h4>Unload a model</h4>
<p>If an empty prompt is provided and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;keep_alive&quot;: 0
}&#39;
</code></pre>
<h5>Response</h5>
<p>A single JSON object is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2024-09-12T03:54:03.516566Z&quot;,
  &quot;response&quot;: &quot;&quot;,
  &quot;done&quot;: true,
  &quot;done_reason&quot;: &quot;unload&quot;
}
</code></pre>
<h2>Generate a chat completion</h2>
<pre><code class=\"language-shell\">POST /api/chat
</code></pre>
<p>Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using <code>&quot;stream&quot;: false</code>. The final response object will include statistics and additional data from the request.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: (required) the <a href=\"#model-names\">model name</a></li>
<li><code>messages</code>: the messages of the chat, this can be used to keep a chat memory</li>
<li><code>tools</code>: tools for the model to use if supported. Requires <code>stream</code> to be set to <code>false</code></li>
</ul>
<p>The <code>message</code> object has the following fields:</p>
<ul>
<li><code>role</code>: the role of the message, either <code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code></li>
<li><code>content</code>: the content of the message</li>
<li><code>images</code> (optional): a list of images to include in the message (for multimodal models such as <code>llava</code>)</li>
<li><code>tool_calls</code> (optional): a list of tools the model wants to use</li>
</ul>
<p>Advanced parameters (optional):</p>
<ul>
<li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema. </li>
<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>
<li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>
<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>
</ul>
<h3>Structured outputs</h3>
<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the <a href=\"#chat-request-structured-outputs\">Chat request (Structured outputs)</a> example below.</p>
<h3>Examples</h3>
<h4>Chat Request (Streaming)</h4>
<h5>Request</h5>
<p>Send a chat message with a streaming response.</p>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;why is the sky blue?&quot;
    }
  ]
}&#39;
</code></pre>
<h5>Response</h5>
<p>A stream of JSON objects is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;The&quot;,
    &quot;images&quot;: null
  },
  &quot;done&quot;: false
}
</code></pre>
<p>Final response:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 4883583458,
  &quot;load_duration&quot;: 1334875,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 342546000,
  &quot;eval_count&quot;: 282,
  &quot;eval_duration&quot;: 4535599000
}
</code></pre>
<h4>Chat request (No streaming)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;why is the sky blue?&quot;
    }
  ],
  &quot;stream&quot;: false
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-12-12T14:13:43.416799Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;Hello! How are you today?&quot;
  },
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 5191566416,
  &quot;load_duration&quot;: 2154458,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 383809000,
  &quot;eval_count&quot;: 298,
  &quot;eval_duration&quot;: 4799921000
}
</code></pre>
<h4>Chat request (Structured outputs)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl -X POST http://localhost:11434/api/chat -H &quot;Content-Type: application/json&quot; -d &#39;{
  &quot;model&quot;: &quot;llama3.1&quot;,
  &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.&quot;}],
  &quot;stream&quot;: false,
  &quot;format&quot;: {
    &quot;type&quot;: &quot;object&quot;,
    &quot;properties&quot;: {
      &quot;age&quot;: {
        &quot;type&quot;: &quot;integer&quot;
      },
      &quot;available&quot;: {
        &quot;type&quot;: &quot;boolean&quot;
      }
    },
    &quot;required&quot;: [
      &quot;age&quot;,
      &quot;available&quot;
    ]
  },
  &quot;options&quot;: {
    &quot;temperature&quot;: 0
  }
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.1&quot;,
  &quot;created_at&quot;: &quot;2024-12-06T00:46:58.265747Z&quot;,
  &quot;message&quot;: { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;{\\&quot;age\\&quot;: 22, \\&quot;available\\&quot;: false}&quot; },
  &quot;done_reason&quot;: &quot;stop&quot;,
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 2254970291,
  &quot;load_duration&quot;: 574751416,
  &quot;prompt_eval_count&quot;: 34,
  &quot;prompt_eval_duration&quot;: 1502000000,
  &quot;eval_count&quot;: 12,
  &quot;eval_duration&quot;: 175000000
}
</code></pre>
<h4>Chat request (With History)</h4>
<p>Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;why is the sky blue?&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;due to rayleigh scattering.&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;how is that different than mie scattering?&quot;
    }
  ]
}&#39;
</code></pre>
<h5>Response</h5>
<p>A stream of JSON objects is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;The&quot;
  },
  &quot;done&quot;: false
}
</code></pre>
<p>Final response:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 8113331500,
  &quot;load_duration&quot;: 6396458,
  &quot;prompt_eval_count&quot;: 61,
  &quot;prompt_eval_duration&quot;: 398801000,
  &quot;eval_count&quot;: 468,
  &quot;eval_duration&quot;: 7701267000
}
</code></pre>
<h4>Chat request (with images)</h4>
<h5>Request</h5>
<p>Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.</p>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llava&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;what is in this image?&quot;,
      &quot;images&quot;: [&quot;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&quot;]
    }
  ]
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llava&quot;,
  &quot;created_at&quot;: &quot;2023-12-13T22:42:50.203334Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot; The image features a cute, little pig with an angry facial expression. It&#39;s wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.&quot;,
    &quot;images&quot;: null
  },
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 1668506709,
  &quot;load_duration&quot;: 1986209,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 359682000,
  &quot;eval_count&quot;: 83,
  &quot;eval_duration&quot;: 1303285000
}
</code></pre>
<h4>Chat request (Reproducible outputs)</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Hello!&quot;
    }
  ],
  &quot;options&quot;: {
    &quot;seed&quot;: 101,
    &quot;temperature&quot;: 0
  }
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2023-12-12T14:13:43.416799Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;Hello! How are you today?&quot;
  },
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 5191566416,
  &quot;load_duration&quot;: 2154458,
  &quot;prompt_eval_count&quot;: 26,
  &quot;prompt_eval_duration&quot;: 383809000,
  &quot;eval_count&quot;: 298,
  &quot;eval_duration&quot;: 4799921000
}
</code></pre>
<h4>Chat request (with tools)</h4>
<h5>Request</h5>
<pre><code>curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;What is the weather today in Paris?&quot;
    }
  ],
  &quot;stream&quot;: false,
  &quot;tools&quot;: [
    {
      &quot;type&quot;: &quot;function&quot;,
      &quot;function&quot;: {
        &quot;name&quot;: &quot;get_current_weather&quot;,
        &quot;description&quot;: &quot;Get the current weather for a location&quot;,
        &quot;parameters&quot;: {
          &quot;type&quot;: &quot;object&quot;,
          &quot;properties&quot;: {
            &quot;location&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;description&quot;: &quot;The location to get the weather for, e.g. San Francisco, CA&quot;
            },
            &quot;format&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;description&quot;: &quot;The format to return the weather in, e.g. &#39;celsius&#39; or &#39;fahrenheit&#39;&quot;,
              &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]
            }
          },
          &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;]
        }
      }
    }
  ]
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;: &quot;2024-07-22T20:33:28.123648Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;&quot;,
    &quot;tool_calls&quot;: [
      {
        &quot;function&quot;: {
          &quot;name&quot;: &quot;get_current_weather&quot;,
          &quot;arguments&quot;: {
            &quot;format&quot;: &quot;celsius&quot;,
            &quot;location&quot;: &quot;Paris, FR&quot;
          }
        }
      }
    ]
  },
  &quot;done_reason&quot;: &quot;stop&quot;,
  &quot;done&quot;: true,
  &quot;total_duration&quot;: 885095291,
  &quot;load_duration&quot;: 3753500,
  &quot;prompt_eval_count&quot;: 122,
  &quot;prompt_eval_duration&quot;: 328493000,
  &quot;eval_count&quot;: 33,
  &quot;eval_duration&quot;: 552222000
}
</code></pre>
<h4>Load a model</h4>
<p>If the messages array is empty, the model will be loaded into memory.</p>
<h5>Request</h5>
<pre><code>curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: []
}&#39;
</code></pre>
<h5>Response</h5>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;:&quot;2024-09-12T21:17:29.110811Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;&quot;
  },
  &quot;done_reason&quot;: &quot;load&quot;,
  &quot;done&quot;: true
}
</code></pre>
<h4>Unload a model</h4>
<p>If the messages array is empty and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>
<h5>Request</h5>
<pre><code>curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [],
  &quot;keep_alive&quot;: 0
}&#39;
</code></pre>
<h5>Response</h5>
<p>A single JSON object is returned:</p>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;created_at&quot;:&quot;2024-09-12T21:33:17.547535Z&quot;,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;&quot;
  },
  &quot;done_reason&quot;: &quot;unload&quot;,
  &quot;done&quot;: true
}
</code></pre>
<h2>Create a Model</h2>
<pre><code class=\"language-shell\">POST /api/create
</code></pre>
<p>Create a model from a <a href=\"./modelfile.md\"><code>Modelfile</code></a>. It is recommended to set <code>modelfile</code> to the content of the Modelfile rather than just set <code>path</code>. This is a requirement for remote create. Remote model creation must also create any file blobs, fields such as <code>FROM</code> and <code>ADAPTER</code>, explicitly with the server using <a href=\"#create-a-blob\">Create a Blob</a> and the value to the path indicated in the response.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of the model to create</li>
<li><code>modelfile</code> (optional): contents of the Modelfile</li>
<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>
<li><code>path</code> (optional): path to the Modelfile</li>
<li><code>quantize</code> (optional): quantize a non-quantized (e.g. float16) model</li>
</ul>
<h4>Quantization types</h4>
<table>
<thead>
<tr>
<th>Type</th>
<th align=\"center\">Recommended</th>
</tr>
</thead>
<tbody><tr>
<td>q2_K</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q3_K_L</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q3_K_M</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q3_K_S</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q4_0</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q4_1</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q4_K_M</td>
<td align=\"center\">*</td>
</tr>
<tr>
<td>q4_K_S</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q5_0</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q5_1</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q5_K_M</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q5_K_S</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q6_K</td>
<td align=\"center\"></td>
</tr>
<tr>
<td>q8_0</td>
<td align=\"center\">*</td>
</tr>
</tbody></table>
<h3>Examples</h3>
<h4>Create a new model</h4>
<p>Create a new model from a <code>Modelfile</code>.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/create -d &#39;{
  &quot;model&quot;: &quot;mario&quot;,
  &quot;modelfile&quot;: &quot;FROM llama3\\nSYSTEM You are mario from Super Mario Bros.&quot;
}&#39;
</code></pre>
<h5>Response</h5>
<p>A stream of JSON objects is returned:</p>
<pre><code class=\"language-json\">{&quot;status&quot;:&quot;reading model metadata&quot;}
{&quot;status&quot;:&quot;creating system layer&quot;}
{&quot;status&quot;:&quot;using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2&quot;}
{&quot;status&quot;:&quot;using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b&quot;}
{&quot;status&quot;:&quot;using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d&quot;}
{&quot;status&quot;:&quot;using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988&quot;}
{&quot;status&quot;:&quot;using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9&quot;}
{&quot;status&quot;:&quot;writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42&quot;}
{&quot;status&quot;:&quot;writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690&quot;}
{&quot;status&quot;:&quot;writing manifest&quot;}
{&quot;status&quot;:&quot;success&quot;}
</code></pre>
<h4>Quantize a model</h4>
<p>Quantize a non-quantized model.</p>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/create -d &#39;{
  &quot;model&quot;: &quot;llama3.1:quantized&quot;,
  &quot;modelfile&quot;: &quot;FROM llama3.1:8b-instruct-fp16&quot;,
  &quot;quantize&quot;: &quot;q4_K_M&quot;
}&#39;
</code></pre>
<h5>Response</h5>
<p>A stream of JSON objects is returned:</p>
<pre><code>{&quot;status&quot;:&quot;quantizing F16 model to Q4_K_M&quot;}
{&quot;status&quot;:&quot;creating new layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29&quot;}
{&quot;status&quot;:&quot;using existing layer sha256:11ce4ee3e170f6adebac9a991c22e22ab3f8530e154ee669954c4bc73061c258&quot;}
{&quot;status&quot;:&quot;using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177&quot;}
{&quot;status&quot;:&quot;using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb&quot;}
{&quot;status&quot;:&quot;creating new layer sha256:455f34728c9b5dd3376378bfb809ee166c145b0b4c1f1a6feca069055066ef9a&quot;}
{&quot;status&quot;:&quot;writing manifest&quot;}
{&quot;status&quot;:&quot;success&quot;}
</code></pre>
<h3>Check if a Blob Exists</h3>
<pre><code class=\"language-shell\">HEAD /api/blobs/:digest
</code></pre>
<p>Ensures that the file blob used for a FROM or ADAPTER field exists on the server. This is checking your Ollama server and not ollama.com.</p>
<h4>Query Parameters</h4>
<ul>
<li><code>digest</code>: the SHA256 digest of the blob</li>
</ul>
<h4>Examples</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
</code></pre>
<h5>Response</h5>
<p>Return 200 OK if the blob exists, 404 Not Found if it does not.</p>
<h3>Create a Blob</h3>
<pre><code class=\"language-shell\">POST /api/blobs/:digest
</code></pre>
<p>Create a blob from a file on the server. Returns the server file path.</p>
<h4>Query Parameters</h4>
<ul>
<li><code>digest</code>: the expected SHA256 digest of the file</li>
</ul>
<h4>Examples</h4>
<h5>Request</h5>
<pre><code class=\"language-shell\">curl -T model.bin -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
</code></pre>
<h5>Response</h5>
<p>Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.</p>
<h2>List Local Models</h2>
<pre><code class=\"language-shell\">GET /api/tags
</code></pre>
<p>List models that are available locally.</p>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/tags
</code></pre>
<h4>Response</h4>
<p>A single JSON object will be returned.</p>
<pre><code class=\"language-json\">{
  &quot;models&quot;: [
    {
      &quot;name&quot;: &quot;codellama:13b&quot;,
      &quot;modified_at&quot;: &quot;2023-11-04T14:56:49.277302595-07:00&quot;,
      &quot;size&quot;: 7365960935,
      &quot;digest&quot;: &quot;9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697&quot;,
      &quot;details&quot;: {
        &quot;format&quot;: &quot;gguf&quot;,
        &quot;family&quot;: &quot;llama&quot;,
        &quot;families&quot;: null,
        &quot;parameter_size&quot;: &quot;13B&quot;,
        &quot;quantization_level&quot;: &quot;Q4_0&quot;
      }
    },
    {
      &quot;name&quot;: &quot;llama3:latest&quot;,
      &quot;modified_at&quot;: &quot;2023-12-07T09:32:18.757212583-08:00&quot;,
      &quot;size&quot;: 3825819519,
      &quot;digest&quot;: &quot;fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e&quot;,
      &quot;details&quot;: {
        &quot;format&quot;: &quot;gguf&quot;,
        &quot;family&quot;: &quot;llama&quot;,
        &quot;families&quot;: null,
        &quot;parameter_size&quot;: &quot;7B&quot;,
        &quot;quantization_level&quot;: &quot;Q4_0&quot;
      }
    }
  ]
}
</code></pre>
<h2>Show Model Information</h2>
<pre><code class=\"language-shell\">POST /api/show
</code></pre>
<p>Show information about a model including details, modelfile, template, parameters, license, system prompt.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of the model to show</li>
<li><code>verbose</code>: (optional) if set to <code>true</code>, returns full data for verbose response fields</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/show -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<pre><code class=\"language-json\">{
  &quot;modelfile&quot;: &quot;# Modelfile generated by \\&quot;ollama show\\&quot;\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\&quot;\\&quot;\\&quot;{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\&quot;\\&quot;\\&quot;\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\&quot;\\u003c/s\\u003e\\&quot;\\nPARAMETER stop \\&quot;USER:\\&quot;\\nPARAMETER stop \\&quot;ASSISTANT:\\&quot;&quot;,
  &quot;parameters&quot;: &quot;num_keep                       24\\nstop                           \\&quot;&lt;|start_header_id|&gt;\\&quot;\\nstop                           \\&quot;&lt;|end_header_id|&gt;\\&quot;\\nstop                           \\&quot;&lt;|eot_id|&gt;\\&quot;&quot;,
  &quot;template&quot;: &quot;{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n{{ .Response }}&lt;|eot_id|&gt;&quot;,
  &quot;details&quot;: {
    &quot;parent_model&quot;: &quot;&quot;,
    &quot;format&quot;: &quot;gguf&quot;,
    &quot;family&quot;: &quot;llama&quot;,
    &quot;families&quot;: [
      &quot;llama&quot;
    ],
    &quot;parameter_size&quot;: &quot;8.0B&quot;,
    &quot;quantization_level&quot;: &quot;Q4_0&quot;
  },
  &quot;model_info&quot;: {
    &quot;general.architecture&quot;: &quot;llama&quot;,
    &quot;general.file_type&quot;: 2,
    &quot;general.parameter_count&quot;: 8030261248,
    &quot;general.quantization_version&quot;: 2,
    &quot;llama.attention.head_count&quot;: 32,
    &quot;llama.attention.head_count_kv&quot;: 8,
    &quot;llama.attention.layer_norm_rms_epsilon&quot;: 0.00001,
    &quot;llama.block_count&quot;: 32,
    &quot;llama.context_length&quot;: 8192,
    &quot;llama.embedding_length&quot;: 4096,
    &quot;llama.feed_forward_length&quot;: 14336,
    &quot;llama.rope.dimension_count&quot;: 128,
    &quot;llama.rope.freq_base&quot;: 500000,
    &quot;llama.vocab_size&quot;: 128256,
    &quot;tokenizer.ggml.bos_token_id&quot;: 128000,
    &quot;tokenizer.ggml.eos_token_id&quot;: 128009,
    &quot;tokenizer.ggml.merges&quot;: [],            // populates if `verbose=true`
    &quot;tokenizer.ggml.model&quot;: &quot;gpt2&quot;,
    &quot;tokenizer.ggml.pre&quot;: &quot;llama-bpe&quot;,
    &quot;tokenizer.ggml.token_type&quot;: [],        // populates if `verbose=true`
    &quot;tokenizer.ggml.tokens&quot;: []             // populates if `verbose=true`
  }
}
</code></pre>
<h2>Copy a Model</h2>
<pre><code class=\"language-shell\">POST /api/copy
</code></pre>
<p>Copy a model. Creates a model with another name from an existing model.</p>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/copy -d &#39;{
  &quot;source&quot;: &quot;llama3.2&quot;,
  &quot;destination&quot;: &quot;llama3-backup&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<p>Returns a 200 OK if successful, or a 404 Not Found if the source model doesn&#39;t exist.</p>
<h2>Delete a Model</h2>
<pre><code class=\"language-shell\">DELETE /api/delete
</code></pre>
<p>Delete a model and its data.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: model name to delete</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl -X DELETE http://localhost:11434/api/delete -d &#39;{
  &quot;model&quot;: &quot;llama3:13b&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<p>Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn&#39;t exist.</p>
<h2>Pull a Model</h2>
<pre><code class=\"language-shell\">POST /api/pull
</code></pre>
<p>Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of the model to pull</li>
<li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.</li>
<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/pull -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p>
<p>The first object is the manifest:</p>
<pre><code class=\"language-json\">{
  &quot;status&quot;: &quot;pulling manifest&quot;
}
</code></pre>
<p>Then there is a series of downloading responses. Until any of the download is completed, the <code>completed</code> key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.</p>
<pre><code class=\"language-json\">{
  &quot;status&quot;: &quot;downloading digestname&quot;,
  &quot;digest&quot;: &quot;digestname&quot;,
  &quot;total&quot;: 2142590208,
  &quot;completed&quot;: 241970
}
</code></pre>
<p>After all the files are downloaded, the final responses are:</p>
<pre><code class=\"language-json\">{
    &quot;status&quot;: &quot;verifying sha256 digest&quot;
}
{
    &quot;status&quot;: &quot;writing manifest&quot;
}
{
    &quot;status&quot;: &quot;removing any unused layers&quot;
}
{
    &quot;status&quot;: &quot;success&quot;
}
</code></pre>
<p>if <code>stream</code> is set to false, then the response is a single JSON object:</p>
<pre><code class=\"language-json\">{
  &quot;status&quot;: &quot;success&quot;
}
</code></pre>
<h2>Push a Model</h2>
<pre><code class=\"language-shell\">POST /api/push
</code></pre>
<p>Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of the model to push in the form of <code>&lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;</code></li>
<li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.</li>
<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/push -d &#39;{
  &quot;model&quot;: &quot;mattw/pygmalion:latest&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p>
<pre><code class=\"language-json\">{ &quot;status&quot;: &quot;retrieving manifest&quot; }
</code></pre>
<p>and then:</p>
<pre><code class=\"language-json\">{
  &quot;status&quot;: &quot;starting upload&quot;,
  &quot;digest&quot;: &quot;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab&quot;,
  &quot;total&quot;: 1928429856
}
</code></pre>
<p>Then there is a series of uploading responses:</p>
<pre><code class=\"language-json\">{
  &quot;status&quot;: &quot;starting upload&quot;,
  &quot;digest&quot;: &quot;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab&quot;,
  &quot;total&quot;: 1928429856
}
</code></pre>
<p>Finally, when the upload is complete:</p>
<pre><code class=\"language-json\">{&quot;status&quot;:&quot;pushing manifest&quot;}
{&quot;status&quot;:&quot;success&quot;}
</code></pre>
<p>If <code>stream</code> is set to <code>false</code>, then the response is a single JSON object:</p>
<pre><code class=\"language-json\">{ &quot;status&quot;: &quot;success&quot; }
</code></pre>
<h2>Generate Embeddings</h2>
<pre><code class=\"language-shell\">POST /api/embed
</code></pre>
<p>Generate embeddings from a model</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of model to generate embeddings from</li>
<li><code>input</code>: text or list of text to generate embeddings for</li>
</ul>
<p>Advanced parameters:</p>
<ul>
<li><code>truncate</code>: truncates the end of each input to fit within context length. Returns error if <code>false</code> and context length is exceeded. Defaults to <code>true</code></li>
<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>
<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/embed -d &#39;{
  &quot;model&quot;: &quot;all-minilm&quot;,
  &quot;input&quot;: &quot;Why is the sky blue?&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;all-minilm&quot;,
  &quot;embeddings&quot;: [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ]],
  &quot;total_duration&quot;: 14143917,
  &quot;load_duration&quot;: 1019500,
  &quot;prompt_eval_count&quot;: 8
}
</code></pre>
<h4>Request (Multiple input)</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/embed -d &#39;{
  &quot;model&quot;: &quot;all-minilm&quot;,
  &quot;input&quot;: [&quot;Why is the sky blue?&quot;, &quot;Why is the grass green?&quot;]
}&#39;
</code></pre>
<h4>Response</h4>
<pre><code class=\"language-json\">{
  &quot;model&quot;: &quot;all-minilm&quot;,
  &quot;embeddings&quot;: [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ],[
    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,
    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481
  ]]
}
</code></pre>
<h2>List Running Models</h2>
<pre><code class=\"language-shell\">GET /api/ps
</code></pre>
<p>List models that are currently loaded into memory.</p>
<h4>Examples</h4>
<h3>Request</h3>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/ps
</code></pre>
<h4>Response</h4>
<p>A single JSON object will be returned.</p>
<pre><code class=\"language-json\">{
  &quot;models&quot;: [
    {
      &quot;name&quot;: &quot;mistral:latest&quot;,
      &quot;model&quot;: &quot;mistral:latest&quot;,
      &quot;size&quot;: 5137025024,
      &quot;digest&quot;: &quot;2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8&quot;,
      &quot;details&quot;: {
        &quot;parent_model&quot;: &quot;&quot;,
        &quot;format&quot;: &quot;gguf&quot;,
        &quot;family&quot;: &quot;llama&quot;,
        &quot;families&quot;: [
          &quot;llama&quot;
        ],
        &quot;parameter_size&quot;: &quot;7.2B&quot;,
        &quot;quantization_level&quot;: &quot;Q4_0&quot;
      },
      &quot;expires_at&quot;: &quot;2024-06-04T14:38:31.83753-07:00&quot;,
      &quot;size_vram&quot;: 5137025024
    }
  ]
}
</code></pre>
<h2>Generate Embedding</h2>
<blockquote>
<p>Note: this endpoint has been superseded by <code>/api/embed</code></p>
</blockquote>
<pre><code class=\"language-shell\">POST /api/embeddings
</code></pre>
<p>Generate embeddings from a model</p>
<h3>Parameters</h3>
<ul>
<li><code>model</code>: name of model to generate embeddings from</li>
<li><code>prompt</code>: text to generate embeddings for</li>
</ul>
<p>Advanced parameters:</p>
<ul>
<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>
<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>
</ul>
<h3>Examples</h3>
<h4>Request</h4>
<pre><code class=\"language-shell\">curl http://localhost:11434/api/embeddings -d &#39;{
  &quot;model&quot;: &quot;all-minilm&quot;,
  &quot;prompt&quot;: &quot;Here is an article about llamas...&quot;
}&#39;
</code></pre>
<h4>Response</h4>
<pre><code class=\"language-json\">{
  &quot;embedding&quot;: [
    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,
    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281
  ]
}
</code></pre>

        <div class="entry-footer">
            <h2>About</h2>
            
        </div>
    </body>
</html>