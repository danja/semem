This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-29T18:32:24.727Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Semem repo

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
docs/
  2024-11-28/
    api-docs.md
    custom-storage.md
    deployment-docs.md
    deployment.md
    memory-docs.md
    monitoring-docs.md
    quickstart.md
    setup-config.md
    storage-docs.md
    testing-docs.md
    troubleshooting.md
    utils-docs.md
  2024-12-29/
    ollama-rest-api.md
  notes.md
spec/
  helpers/
    jasmine_examples/
      SpecHelper.js
    reporter.js
  support/
    jasmine.json
src/
  config.js
  example.js
  in-memory-storage.js
  json-storage.js
  memory-manager.js
  memory-store.js
  remote-storage.js
  storage-base.js
  utils.js
.git
.gitignore
jsdoc.json
LICENSE
package.json
repomix.config.json

================================================================
Repository Files
================================================================

================
File: docs/2024-11-28/api-docs.md
================
# Remote Storage API Documentation

## Base URL
`https://your-api-endpoint/memory`

## Endpoints

### GET /memory
Retrieves all stored memories.

```http
GET /memory
Authorization: Bearer <your-api-key>
```

Response:
```javascript
{
  "shortTermMemory": [
    {
      "id": "uuid",
      "prompt": "string",
      "output": "string",
      "embedding": [number],
      "timestamp": number,
      "accessCount": number,
      "concepts": [string],
      "decayFactor": number
    }
  ],
  "longTermMemory": [/* similar structure */]
}
```

### POST /memory
Stores new memory state.

```http
POST /memory
Authorization: Bearer <your-api-key>
Content-Type: application/json

{
  "shortTermMemory": [...],
  "longTermMemory": [...]
}
```

### Error Responses
- 401: Unauthorized
- 403: Forbidden
- 500: Internal Server Error

================
File: docs/2024-11-28/custom-storage.md
================
# Custom Storage Implementations

## Redis Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import Redis from 'ioredis';

export default class RedisStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.redis = new Redis({
            host: options.host || 'localhost',
            port: options.port || 6379,
            password: options.password,
            keyPrefix: 'semem:'
        });
    }

    async loadHistory() {
        try {
            const shortTerm = await this.redis.get('short_term_memory');
            const longTerm = await this.redis.get('long_term_memory');
            
            return [
                JSON.parse(shortTerm || '[]'),
                JSON.parse(longTerm || '[]')
            ];
        } catch (error) {
            logger.error('Redis load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            const pipeline = this.redis.pipeline();
            
            pipeline.set('short_term_memory', 
                JSON.stringify(memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx].flat()),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                }))));
                
            pipeline.set('long_term_memory', 
                JSON.stringify(memoryStore.longTermMemory));
            
            await pipeline.exec();
        } catch (error) {
            logger.error('Redis save error:', error);
            throw error;
        }
    }
}
```

## MongoDB Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import { MongoClient } from 'mongodb';

export default class MongoStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.url = options.url || 'mongodb://localhost:27017';
        this.dbName = options.dbName || 'semem';
        this.client = null;
        this.db = null;
    }

    async connect() {
        if (!this.client) {
            this.client = await MongoClient.connect(this.url);
            this.db = this.client.db(this.dbName);
        }
    }

    async loadHistory() {
        try {
            await this.connect();
            
            const shortTerm = await this.db.collection('short_term_memory')
                .find({})
                .sort({ timestamp: -1 })
                .toArray();
                
            const longTerm = await this.db.collection('long_term_memory')
                .find({})
                .toArray();
                
            return [shortTerm, longTerm];
        } catch (error) {
            logger.error('MongoDB load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            await this.connect();
            
            // Create session for transaction
            const session = this.client.startSession();
            
            try {
                await session.withTransaction(async () => {
                    // Clear existing memories
                    await this.db.collection('short_term_memory').deleteMany({}, { session });
                    
                    // Insert new short-term memories
                    if (memoryStore.shortTermMemory.length > 0) {
                        await this.db.collection('short_term_memory').insertMany(
                            memoryStore.shortTermMemory.map((item, idx) => ({
                                id: item.id,
                                prompt: item.prompt,
                                output: item.output,
                                embedding: Array.from(memoryStore.embeddings[idx].flat()),
                                timestamp: memoryStore.timestamps[idx],
                                accessCount: memoryStore.accessCounts[idx],
                                concepts: Array.from(memoryStore.conceptsList[idx]),
                                decayFactor: item.decayFactor || 1.0
                            })),
                            { session }
                        );
                    }
                    
                    // Update long-term memories
                    await this.db.collection('long_term_memory').deleteMany({}, { session });
                    if (memoryStore.longTermMemory.length > 0) {
                        await this.db.collection('long_term_memory').insertMany(
                            memoryStore.longTermMemory,
                            { session }
                        );
                    }
                });
            } finally {
                await session.endSession();
            }
        } catch (error) {
            logger.error('MongoDB save error:', error);
            throw error;
        }
    }
}
```

## SQLite Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import sqlite3 from 'sqlite3';
import { open } from 'sqlite';

export default class SQLiteStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.dbPath = options.dbPath || ':memory:';
        this.db = null;
    }

    async init() {
        if (!this.db) {
            this.db = await open({
                filename: this.dbPath,
                driver: sqlite3.Database
            });
            
            await this.createTables();
        }
    }

    async createTables() {
        await this.db.exec(`
            CREATE TABLE IF NOT EXISTS short_term_memory (
                id TEXT PRIMARY KEY,
                prompt TEXT,
                output TEXT,
                embedding BLOB,
                timestamp INTEGER,
                access_count INTEGER,
                concepts TEXT,
                decay_factor REAL
            );
            
            CREATE TABLE IF NOT EXISTS long_term_memory (
                id TEXT PRIMARY KEY,
                prompt TEXT,
                output TEXT,
                timestamp INTEGER,
                concepts TEXT
            );
        `);
    }

    async loadHistory() {
        try {
            await this.init();
            
            const shortTerm = await this.db.all(`
                SELECT * FROM short_term_memory
                ORDER BY timestamp DESC
            `);
            
            const longTerm = await this.db.all(`
                SELECT * FROM long_term_memory
            `);
            
            // Convert stored format back to application format
            return [
                shortTerm.map(row => ({
                    id: row.id,
                    prompt: row.prompt,
                    output: row.output,
                    embedding: new Float32Array(row.embedding),
                    timestamp: row.timestamp,
                    accessCount: row.access_count,
                    concepts: JSON.parse(row.concepts),
                    decayFactor: row.decay_factor
                })),
                longTerm.map(row => ({
                    id: row.id,
                    prompt: row.prompt,
                    output: row.output,
                    timestamp: row.timestamp,
                    concepts: JSON.parse(row.concepts)
                }))
            ];
        } catch (error) {
            logger.error('SQLite load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            await this.init();
            
            await this.db.run('BEGIN TRANSACTION');
            
            try {
                // Clear existing memories
                await this.db.run('DELETE FROM short_term_memory');
                await this.db.run('DELETE FROM long_term_memory');
                
                // Insert short-term memories
                const shortTermStmt = await this.db.prepare(`
                    INSERT INTO short_term_memory VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                `);
                
                for (let idx = 0; idx < memoryStore.shortTermMemory.length; idx++) {
                    const item = memoryStore.shortTermMemory[idx];
                    await shortTermStmt.run(
                        item.id,
                        item.prompt,
                        item.output,
                        Buffer.from(memoryStore.embeddings[idx].buffer),
                        memoryStore.timestamps[idx],
                        memoryStore.accessCounts[idx],
                        JSON.stringify(Array.from(memoryStore.conceptsList[idx])),
                        item.decayFactor || 1.0
                    );
                }
                
                // Insert long-term memories
                const longTermStmt = await this.db.prepare(`
                    INSERT INTO long_term_memory VALUES (?, ?, ?, ?, ?)
                `);
                
                for (const item of memoryStore.longTermMemory) {
                    await longTermStmt.run(
                        item.id,
                        item.prompt,
                        item.output,
                        item.timestamp,
                        JSON.stringify(item.concepts)
                    );
                }
                
                await this.db.run('COMMIT');
            } catch (error) {
                await this.db.run('ROLLBACK');
                throw error;
            }
        } catch (error) {
            logger.error('SQLite save error:', error);
            throw error;
        }
    }
}
```

Q1: Would you like to see a GraphDB storage implementation?
Q2: Should I show an S3/Object Storage implementation?
Q3: Would you like to see a distributed storage implementation?
Q4: Should I add caching layer examples?

================
File: docs/2024-11-28/deployment-docs.md
================
# Deployment Guide

## Prerequisites
- Node.js 18+
- NPM or Yarn
- OpenAI API key (optional)
- Ollama installation (optional)

## Installation
```bash
npm install semem
```

## Environment Setup
```bash
OPENAI_API_KEY=your-key
STORAGE_API_KEY=your-storage-key
STORAGE_ENDPOINT=https://api.example.com
```

## Docker Deployment
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD ["node", "index.js"]
```

## Security Considerations
- API key management
- Rate limiting
- Error handling
- Logging configuration

================
File: docs/2024-11-28/deployment.md
================
# Deployment Guide

## Prerequisites
```bash
# Node.js 18+ required
node --version

# Install dependencies
npm install semem
```

## Environment Setup
```bash
# .env file
OPENAI_API_KEY=your-key
STORAGE_API_KEY=your-storage-key
STORAGE_ENDPOINT=https://api.example.com
```

## Docker Deployment
```dockerfile
FROM node:18-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install

# Copy application files
COPY . .

# Set environment variables
ENV NODE_ENV=production

# Start the application
CMD ["node", "index.js"]
```

## Docker Compose
```yaml
version: '3.8'
services:
  semem:
    build: .
    environment:
      - NODE_ENV=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports:
      - "3000:3000"
    volumes:
      - ./data:/app/data
```

## PM2 Deployment
```bash
# Install PM2
npm install -g pm2

# Start application
pm2 start index.js --name semem

# Enable startup
pm2 startup
pm2 save
```

================
File: docs/2024-11-28/memory-docs.md
================
# Memory System Documentation

## MemoryStore (memoryStore.js)
Core memory management implementation handling storage, retrieval, and processing of memories.

### Key Components
- FAISS index for similarity search
- Graph-based concept relationships
- Clustering mechanism
- Decay/reinforcement system

### Features
- Vector similarity search
- Hierarchical memory clustering
- Spreading activation
- Memory lifecycle management

### Implementation Details
- Uses Float32Array for embeddings
- Implements k-means clustering
- Manages memory transitions
- Handles concept graph updates

## MemoryManager (memoryManager.js)
High-level interface coordinating all memory operations.

### Key Features
- LLM integration (OpenAI/Ollama)
- Embedding generation
- Concept extraction
- Response generation

### Implementation Details
- Model initialization and management
- Embedding standardization
- Context building
- Storage coordination

### Usage Patterns
- Interactive response generation
- Memory persistence
- Concept management
- Configuration handling

================
File: docs/2024-11-28/monitoring-docs.md
================
# Monitoring Documentation

## System Metrics

### Memory Usage Monitoring
```javascript
import { EventEmitter } from 'events';
import { logger } from './utils.js';

class MemoryMonitor extends EventEmitter {
    constructor(memoryManager) {
        super();
        this.memoryManager = memoryManager;
        this.stats = {
            shortTermCount: 0,
            longTermCount: 0,
            embeddingSize: 0,
            lastAccessed: new Date()
        };
    }

    collectMetrics() {
        this.stats = {
            shortTermCount: this.memoryManager.memoryStore.shortTermMemory.length,
            longTermCount: this.memoryManager.memoryStore.longTermMemory.length,
            embeddingSize: this.memoryManager.memoryStore.embeddings.reduce((acc, curr) => 
                acc + curr.byteLength, 0),
            lastAccessed: new Date()
        };
        this.emit('metrics', this.stats);
        logger.debug('Memory metrics:', this.stats);
    }
}
```

### Performance Metrics
```javascript
const performanceMetrics = {
    responseTime: new Map(),
    embeddingTime: new Map(),
    storageOperations: new Map(),
    
    track(operation, duration) {
        const metrics = this.responseTime.get(operation) || {
            count: 0,
            totalTime: 0,
            avgTime: 0
        };
        metrics.count++;
        metrics.totalTime += duration;
        metrics.avgTime = metrics.totalTime / metrics.count;
        this.responseTime.set(operation, metrics);
    }
};
```

## Integration with Monitoring Services

### Prometheus Integration
```javascript
import prometheus from 'prom-client';

const memoryGauge = new prometheus.Gauge({
    name: 'semem_memory_usage_bytes',
    help: 'Memory usage of SeMeM'
});

const responseHistogram = new prometheus.Histogram({
    name: 'semem_response_time_seconds',
    help: 'Response time of memory operations'
});
```

### Health Checks
```javascript
class HealthCheck {
    constructor(memoryManager) {
        this.memoryManager = memoryManager;
    }

    async check() {
        try {
            const status = {
                storage: await this.checkStorage(),
                embeddings: await this.checkEmbeddings(),
                llm: await this.checkLLM(),
                timestamp: new Date()
            };
            return status;
        } catch (error) {
            logger.error('Health check failed:', error);
            throw error;
        }
    }

    async checkStorage() {
        const startTime = Date.now();
        await this.memoryManager.storage.loadHistory();
        return {
            status: 'ok',
            latency: Date.now() - startTime
        };
    }
}
```

## Usage Monitoring

### Rate Limiting
```javascript
class RateLimiter {
    constructor(limit = 100, window = 60000) {
        this.limit = limit;
        this.window = window;
        this.requests = new Map();
    }

    async checkLimit(key) {
        const now = Date.now();
        const requests = this.requests.get(key) || [];
        const windowStart = now - this.window;
        
        // Clean old requests
        const recent = requests.filter(time => time > windowStart);
        this.requests.set(key, recent);
        
        return recent.length < this.limit;
    }
}
```

### Error Tracking
```javascript
class ErrorTracker {
    constructor() {
        this.errors = new Map();
    }

    track(error, context) {
        const key = error.message;
        const entry = this.errors.get(key) || {
            count: 0,
            firstSeen: new Date(),
            lastSeen: new Date(),
            contexts: new Set()
        };
        
        entry.count++;
        entry.lastSeen = new Date();
        entry.contexts.add(JSON.stringify(context));
        
        this.errors.set(key, entry);
        logger.error('Error tracked:', { error, context });
    }
}
```

## Implementation Example
```javascript
import { MemoryManager } from './memoryManager.js';

const manager = new MemoryManager({...});
const monitor = new MemoryMonitor(manager);
const health = new HealthCheck(manager);
const errors = new ErrorTracker();

// Setup periodic monitoring
setInterval(() => {
    monitor.collectMetrics();
    health.check().catch(error => 
        errors.track(error, { component: 'health_check' }));
}, 60000);

// Track performance
monitor.on('metrics', async (stats) => {
    memoryGauge.set(stats.embeddingSize);
    
    try {
        const healthStatus = await health.check();
        logger.info('Health status:', healthStatus);
    } catch (error) {
        errors.track(error, { component: 'monitoring' });
    }
});
```

This monitoring system provides:
1. Memory usage tracking
2. Performance metrics
3. Health checks
4. Error tracking
5. Rate limiting
6. Prometheus integration

Q1: Would you like to see example Grafana dashboards?
Q2: Should I add alerting configuration?
Q3: Would you like to see logging best practices?
Q4: Should I include scaling metrics?

================
File: docs/2024-11-28/quickstart.md
================
# Quick Start Guide

## Basic Setup
```javascript
import { MemoryManager, JSONStorage } from 'semem';

// Initialize with minimal configuration
const manager = new MemoryManager({
    apiKey: process.env.OPENAI_API_KEY,
    storage: new JSONStorage('memory.json')
});

// Simple interaction
async function basicExample() {
    const prompt = "What's the weather like?";
    const response = await manager.generateResponse(prompt);
    console.log(response);
}

// Advanced interaction with memory retrieval
async function advancedExample() {
    const prompt = "Remember our discussion about AI?";
    
    // Get relevant past interactions
    const relevantMemories = await manager.retrieveRelevantInteractions(prompt);
    
    // Generate contextual response
    const response = await manager.generateResponse(prompt, [], relevantMemories);
    
    // Store interaction
    const embedding = await manager.getEmbedding(`${prompt} ${response}`);
    const concepts = await manager.extractConcepts(`${prompt} ${response}`);
    await manager.addInteraction(prompt, response, embedding, concepts);
}
```

## Using with Ollama
```javascript
const manager = new MemoryManager({
    chatModel: 'ollama',
    chatModelName: 'llama2',
    embeddingModel: 'ollama',
    embeddingModelName: 'nomic-embed-text'
});
```

## Using Remote Storage
```javascript
import { RemoteStorage } from 'semem';

const manager = new MemoryManager({
    apiKey: process.env.OPENAI_API_KEY,
    storage: new RemoteStorage({
        endpoint: 'https://api.example.com/memory',
        apiKey: process.env.STORAGE_API_KEY
    })
});
```

================
File: docs/2024-11-28/setup-config.md
================
# Setup and Configuration Guide

## Basic Configuration
```javascript
import { Config, MemoryManager } from 'semem';

const config = new Config({
    storage: {
        type: 'json',
        options: {
            path: 'data/memory.json'
        }
    },
    models: {
        chat: {
            provider: 'openai',
            model: 'gpt-4-turbo-preview'
        },
        embedding: {
            provider: 'openai',
            model: 'text-embedding-3-small'
        }
    },
    memory: {
        dimension: 1536,
        similarityThreshold: 40,
        contextWindow: 3,
        decayRate: 0.0001
    }
});
```

## Storage Configuration

### JSON Storage
```javascript
import { JSONStorage } from 'semem';

const storage = new JSONStorage('data/memory.json');
```

### Remote Storage
```javascript
import { RemoteStorage } from 'semem';

const storage = new RemoteStorage({
    endpoint: 'https://api.example.com/memory',
    apiKey: process.env.STORAGE_API_KEY,
    timeout: 5000,
    retries: 3
});
```

## Performance Tuning
```javascript
const config = new Config({
    memory: {
        // Reduce dimension for faster processing
        dimension: 1024,
        
        // Increase threshold for more precise matching
        similarityThreshold: 50,
        
        // Reduce window size for faster response
        contextWindow: 2,
        
        // Adjust decay rate for memory persistence
        decayRate: 0.0005
    }
});
```

## Error Handling
```javascript
try {
    const manager = new MemoryManager({
        apiKey: process.env.OPENAI_API_KEY,
        storage,
        onError: (error) => {
            console.error('Memory manager error:', error);
            // Implement error handling
        }
    });
} catch (error) {
    console.error('Initialization error:', error);
}
```

## Logging Configuration
```javascript
import { logger } from 'semem';

// Set log level
logger.level = process.env.NODE_ENV === 'production' ? 'info' : 'debug';

// Add custom logging
logger.on('error', (error) => {
    // Custom error handling
});
```

================
File: docs/2024-11-28/storage-docs.md
================
# Storage System Documentation

## BaseStorage (storage.js)
Abstract base class defining the storage interface. All storage implementations must extend this class.

### Methods
- `loadHistory()`: Async method that retrieves stored memory interactions
- `saveMemoryToHistory(memoryStore)`: Async method that persists current memory state

### Key Features
- Abstract interface ensuring consistency across implementations
- Async/await pattern for I/O operations
- Error handling requirements for implementations

## InMemoryStorage (inMemoryStorage.js)
RAM-based storage implementation for development and testing.

### Features
- Fast access and retrieval
- No persistence between restarts
- Ideal for testing and prototyping

### Implementation Details
- Uses JavaScript objects for storage
- Maintains separate short-term and long-term memory arrays
- Handles data structure conversions

## JSONStorage (jsonStorage.js)
File-based storage implementation using JSON format.

### Features
- Persistent storage between application restarts
- Human-readable storage format
- File-based backup capability

### Implementation Details
- Asynchronous file operations
- Automatic file creation if not exists
- Error handling for file operations
- JSON serialization/deserialization

## RemoteStorage (remoteStorage.js)
Network-based storage implementation for distributed systems.

### Features
- RESTful API integration
- Authentication support
- Timeout handling
- Retry logic

### Implementation Details
- HTTP/HTTPS protocols
- Bearer token authentication
- Configurable endpoints
- Network error handling

================
File: docs/2024-11-28/testing-docs.md
================
# Testing Documentation

## Unit Tests
Run with: `npm test`

### Storage Tests
```javascript
import { InMemoryStorage } from '../src/inMemoryStorage.js';

describe('InMemoryStorage', () => {
  it('should store and retrieve memories', async () => {
    const storage = new InMemoryStorage();
    // Test implementation
  });
});
```

### Integration Tests
```javascript
describe('MemoryManager Integration', () => {
  it('should generate and store responses', async () => {
    const manager = new MemoryManager({...});
    // Test implementation
  });
});
```

## Mocking
```javascript
jest.mock('@langchain/openai', () => ({
  ChatOpenAI: jest.fn()
}));
```

================
File: docs/2024-11-28/troubleshooting.md
================
# Troubleshooting Guide

## Common Issues

### Memory Usage
Problem: High memory consumption
```javascript
// Solution: Adjust memory settings
const config = new Config({
  memory: {
    dimension: 1024,  // Lower dimension
    contextWindow: 2  // Reduce context window
  }
});
```

### Storage Errors
Problem: Remote storage timeout
```javascript
// Solution: Configure timeout and retries
const storage = new RemoteStorage({
  timeout: 10000,
  retries: 3
});
```

### Model Errors
Problem: OpenAI API errors
```javascript
// Solution: Implement fallback
try {
  response = await manager.generateResponse(prompt);
} catch {
  response = await fallbackModel.generate(prompt);
}
```

## Debugging
Enable debug logging:
```javascript
logger.level = 'debug';
```

## Performance Optimization
- Use appropriate embedding dimensions
- Implement caching
- Optimize storage patterns

================
File: docs/2024-11-28/utils-docs.md
================
# Utilities Documentation

## Logger (utils.js)
Standardized logging system for application-wide use.

### Methods
- `info(message, ...args)`: Information level logging
- `error(message, ...args)`: Error level logging
- `debug(message, ...args)`: Debug level logging

### Features
- Consistent log formatting
- Multiple log levels
- Console output management

## Vector Operations (utils.js)
Mathematical utilities for vector operations.

### Functions
- `normalize(vector)`: Converts vector to unit length
- `cosineSimilarity(vec1, vec2)`: Calculates vector similarity

### Implementation Details
- Optimized vector calculations
- Numerical stability handling
- Array-based operations

================
File: docs/2024-12-29/ollama-rest-api.md
================
# API

## Endpoints

- [Generate a completion](#generate-a-completion)
- [Generate a chat completion](#generate-a-chat-completion)
- [Create a Model](#create-a-model)
- [List Local Models](#list-local-models)
- [Show Model Information](#show-model-information)
- [Copy a Model](#copy-a-model)
- [Delete a Model](#delete-a-model)
- [Pull a Model](#pull-a-model)
- [Push a Model](#push-a-model)
- [Generate Embeddings](#generate-embeddings)
- [List Running Models](#list-running-models)

## Conventions

### Model names

Model names follow a `model:tag` format, where `model` can have an optional namespace such as `example/model`. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

### Durations

All durations are returned in nanoseconds.

### Streaming responses

Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing `{"stream": false}` for these endpoints.

## Generate a completion

```shell
POST /api/generate
```

Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `prompt`: the prompt to generate a response for
- `suffix`: the text after the model response
- `images`: (optional) a list of base64-encoded images (for multimodal models such as `llava`)

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `system`: system message to (overrides what is defined in the `Modelfile`)
- `template`: the prompt template to use (overrides what is defined in the `Modelfile`)
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `raw`: if `true` no formatting will be applied to the prompt. You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)
- `context` (deprecated): the context parameter returned from a previous request to `/generate`, this can be used to keep a short conversational memory

#### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [structured outputs](#request-structured-outputs) example below.

#### JSON mode

Enable JSON mode by setting the `format` parameter to `json`. This will structure the response as a valid JSON object. See the JSON mode [example](#request-json-mode) below.

> [!IMPORTANT]
> It's important to instruct the model to use JSON in the `prompt`. Otherwise, the model may generate large amounts whitespace.

### Examples

#### Generate request (Streaming)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?"
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "response": "The",
  "done": false
}
```

The final response in the stream also includes additional data about the generation:

- `total_duration`: time spent generating the response
- `load_duration`: time spent in nanoseconds loading the model
- `prompt_eval_count`: number of tokens in the prompt
- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt
- `eval_count`: number of tokens in the response
- `eval_duration`: time in nanoseconds spent generating the response
- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory
- `response`: empty if the response was streamed, if not streamed, this will contain the full response

To calculate how fast the response is generated in tokens per second (token/s), divide `eval_count` / `eval_duration` * `10^9`.

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 10706818083,
  "load_duration": 6338219291,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 130079000,
  "eval_count": 259,
  "eval_duration": 4232710000
}
```

#### Request (No streaming)

##### Request

A response can be received in one reply when streaming is off.

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

##### Response

If `stream` is set to `false`, the response will be a single JSON object:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 5043500667,
  "load_duration": 5025959,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 325953000,
  "eval_count": 290,
  "eval_duration": 4709213000
}
```

#### Request (with suffix)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "codellama:code",
  "prompt": "def compute_gcd(a, b):",
  "suffix": "    return result",
  "options": {
    "temperature": 0
  },
  "stream": false
}'
```

##### Response

```json
{
  "model": "codellama:code",
  "created_at": "2024-07-22T20:47:51.147561Z",
  "response": "\n  if a == 0:\n    return b\n  else:\n    return compute_gcd(b % a, a)\n\ndef compute_lcm(a, b):\n  result = (a * b) / compute_gcd(a, b)\n",
  "done": true,
  "done_reason": "stop",
  "context": [...],
  "total_duration": 1162761250,
  "load_duration": 6683708,
  "prompt_eval_count": 17,
  "prompt_eval_duration": 201222000,
  "eval_count": 63,
  "eval_duration": 953997000
}
```

#### Request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{
  "model": "llama3.1:8b",
  "prompt": "Ollama is 22 years old and is busy saving the world. Respond using JSON",
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  }
}'
```

##### Response

```json
{
  "model": "llama3.1:8b",
  "created_at": "2024-12-06T00:48:09.983619Z",
  "response": "{\n  \"age\": 22,\n  \"available\": true\n}",
  "done": true,
  "done_reason": "stop",
  "context": [1, 2, 3],
  "total_duration": 1075509083,
  "load_duration": 567678166,
  "prompt_eval_count": 28,
  "prompt_eval_duration": 236000000,
  "eval_count": 16,
  "eval_duration": 269000000
}
```

#### Request (JSON mode)

> [!IMPORTANT]
> When `format` is set to `json`, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "What color is the sky at different times of the day? Respond using JSON",
  "format": "json",
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-11-09T21:07:55.186497Z",
  "response": "{\n\"morning\": {\n\"color\": \"blue\"\n},\n\"noon\": {\n\"color\": \"blue-gray\"\n},\n\"afternoon\": {\n\"color\": \"warm gray\"\n},\n\"evening\": {\n\"color\": \"orange\"\n}\n}\n",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4648158584,
  "load_duration": 4071084,
  "prompt_eval_count": 36,
  "prompt_eval_duration": 439038000,
  "eval_count": 180,
  "eval_duration": 4196918000
}
```

The value of `response` will be a string containing JSON similar to:

```json
{
  "morning": {
    "color": "blue"
  },
  "noon": {
    "color": "blue-gray"
  },
  "afternoon": {
    "color": "warm gray"
  },
  "evening": {
    "color": "orange"
  }
}
```

#### Request (with images)

To submit images to multimodal models such as `llava` or `bakllava`, provide a list of base64-encoded `images`:

#### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llava",
  "prompt":"What is in this picture?",
  "stream": false,
  "images": ["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"]
}'
```

#### Response

```
{
  "model": "llava",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": "A happy cartoon character, which is cute and cheerful.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 2938432250,
  "load_duration": 2559292,
  "prompt_eval_count": 1,
  "prompt_eval_duration": 2195557000,
  "eval_count": 44,
  "eval_duration": 736432000
}
```

#### Request (Raw Mode)

In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the `raw` parameter to disable templating. Also note that raw mode will not return a context.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "[INST] why is the sky blue? [/INST]",
  "raw": true,
  "stream": false
}'
```

#### Request (Reproducible outputs)

For reproducible outputs, set `seed` to a number:

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Why is the sky blue?",
  "options": {
    "seed": 123
  }
}'
```

##### Response

```json
{
  "model": "mistral",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": " The sky appears blue because of a phenomenon called Rayleigh scattering.",
  "done": true,
  "total_duration": 8493852375,
  "load_duration": 6589624375,
  "prompt_eval_count": 14,
  "prompt_eval_duration": 119039000,
  "eval_count": 110,
  "eval_duration": 1779061000
}
```

#### Generate request (With options)

If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the `options` parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false,
  "options": {
    "num_keep": 5,
    "seed": 42,
    "num_predict": 100,
    "top_k": 20,
    "top_p": 0.9,
    "min_p": 0.0,
    "typical_p": 0.7,
    "repeat_last_n": 33,
    "temperature": 0.8,
    "repeat_penalty": 1.2,
    "presence_penalty": 1.5,
    "frequency_penalty": 1.0,
    "mirostat": 1,
    "mirostat_tau": 0.8,
    "mirostat_eta": 0.6,
    "penalize_newline": true,
    "stop": ["\n", "user:"],
    "numa": false,
    "num_ctx": 1024,
    "num_batch": 2,
    "num_gpu": 1,
    "main_gpu": 0,
    "low_vram": false,
    "vocab_only": false,
    "use_mmap": true,
    "use_mlock": false,
    "num_thread": 8
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4935886791,
  "load_duration": 534986708,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 107345000,
  "eval_count": 237,
  "eval_duration": 4289432000
}
```

#### Load a model

If an empty prompt is provided, the model will be loaded into memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2"
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-18T19:52:07.071755Z",
  "response": "",
  "done": true
}
```

#### Unload a model

If an empty prompt is provided and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2024-09-12T03:54:03.516566Z",
  "response": "",
  "done": true,
  "done_reason": "unload"
}
```

## Generate a chat completion

```shell
POST /api/chat
```

Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using `"stream": false`. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `messages`: the messages of the chat, this can be used to keep a chat memory
- `tools`: tools for the model to use if supported. Requires `stream` to be set to `false`

The `message` object has the following fields:

- `role`: the role of the message, either `system`, `user`, `assistant`, or `tool`
- `content`: the content of the message
- `images` (optional): a list of images to include in the message (for multimodal models such as `llava`)
- `tool_calls` (optional): a list of tools the model wants to use

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema. 
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](#chat-request-structured-outputs) example below.

### Examples

#### Chat Request (Streaming)

##### Request

Send a chat message with a streaming response.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The",
    "images": null
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "done": true,
  "total_duration": 4883583458,
  "load_duration": 1334875,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 342546000,
  "eval_count": 282,
  "eval_duration": 4535599000
}
```

#### Chat request (No streaming)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ],
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability."}],
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  },
  "options": {
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.1",
  "created_at": "2024-12-06T00:46:58.265747Z",
  "message": { "role": "assistant", "content": "{\"age\": 22, \"available\": false}" },
  "done_reason": "stop",
  "done": true,
  "total_duration": 2254970291,
  "load_duration": 574751416,
  "prompt_eval_count": 34,
  "prompt_eval_duration": 1502000000,
  "eval_count": 12,
  "eval_duration": 175000000
}
```

#### Chat request (With History)

Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The"
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "done": true,
  "total_duration": 8113331500,
  "load_duration": 6396458,
  "prompt_eval_count": 61,
  "prompt_eval_duration": 398801000,
  "eval_count": 468,
  "eval_duration": 7701267000
}
```

#### Chat request (with images)

##### Request

Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llava",
  "messages": [
    {
      "role": "user",
      "content": "what is in this image?",
      "images": ["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"]
    }
  ]
}'
```

##### Response

```json
{
  "model": "llava",
  "created_at": "2023-12-13T22:42:50.203334Z",
  "message": {
    "role": "assistant",
    "content": " The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.",
    "images": null
  },
  "done": true,
  "total_duration": 1668506709,
  "load_duration": 1986209,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 359682000,
  "eval_count": 83,
  "eval_duration": 1303285000
}
```

#### Chat request (Reproducible outputs)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Hello!"
    }
  ],
  "options": {
    "seed": 101,
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (with tools)

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "What is the weather today in Paris?"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The location to get the weather for, e.g. San Francisco, CA"
            },
            "format": {
              "type": "string",
              "description": "The format to return the weather in, e.g. 'celsius' or 'fahrenheit'",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2024-07-22T20:33:28.123648Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_current_weather",
          "arguments": {
            "format": "celsius",
            "location": "Paris, FR"
          }
        }
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 885095291,
  "load_duration": 3753500,
  "prompt_eval_count": 122,
  "prompt_eval_duration": 328493000,
  "eval_count": 33,
  "eval_duration": 552222000
}
```

#### Load a model

If the messages array is empty, the model will be loaded into memory.

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": []
}'
```

##### Response
```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:17:29.110811Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "load",
  "done": true
}
```

#### Unload a model

If the messages array is empty and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [],
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:33:17.547535Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "unload",
  "done": true
}
```

## Create a Model

```shell
POST /api/create
```

Create a model from a [`Modelfile`](./modelfile.md). It is recommended to set `modelfile` to the content of the Modelfile rather than just set `path`. This is a requirement for remote create. Remote model creation must also create any file blobs, fields such as `FROM` and `ADAPTER`, explicitly with the server using [Create a Blob](#create-a-blob) and the value to the path indicated in the response.

### Parameters

- `model`: name of the model to create
- `modelfile` (optional): contents of the Modelfile
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects
- `path` (optional): path to the Modelfile
- `quantize` (optional): quantize a non-quantized (e.g. float16) model

#### Quantization types

| Type | Recommended |
| --- | :-: |
| q2_K | |
| q3_K_L | |
| q3_K_M | |
| q3_K_S | |
| q4_0 | |
| q4_1 | |
| q4_K_M | * |
| q4_K_S | |
| q5_0 | |
| q5_1 | |
| q5_K_M | |
| q5_K_S | |
| q6_K | |
| q8_0 | * |

### Examples

#### Create a new model

Create a new model from a `Modelfile`.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "mario",
  "modelfile": "FROM llama3\nSYSTEM You are mario from Super Mario Bros."
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"reading model metadata"}
{"status":"creating system layer"}
{"status":"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2"}
{"status":"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b"}
{"status":"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d"}
{"status":"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988"}
{"status":"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9"}
{"status":"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42"}
{"status":"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690"}
{"status":"writing manifest"}
{"status":"success"}
```

#### Quantize a model

Quantize a non-quantized model.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "llama3.1:quantized",
  "modelfile": "FROM llama3.1:8b-instruct-fp16",
  "quantize": "q4_K_M"
}'
```

##### Response

A stream of JSON objects is returned:

```
{"status":"quantizing F16 model to Q4_K_M"}
{"status":"creating new layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29"}
{"status":"using existing layer sha256:11ce4ee3e170f6adebac9a991c22e22ab3f8530e154ee669954c4bc73061c258"}
{"status":"using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177"}
{"status":"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb"}
{"status":"creating new layer sha256:455f34728c9b5dd3376378bfb809ee166c145b0b4c1f1a6feca069055066ef9a"}
{"status":"writing manifest"}
{"status":"success"}
```


### Check if a Blob Exists

```shell
HEAD /api/blobs/:digest
```

Ensures that the file blob used for a FROM or ADAPTER field exists on the server. This is checking your Ollama server and not ollama.com.

#### Query Parameters

- `digest`: the SHA256 digest of the blob

#### Examples

##### Request

```shell
curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

##### Response

Return 200 OK if the blob exists, 404 Not Found if it does not.

### Create a Blob

```shell
POST /api/blobs/:digest
```

Create a blob from a file on the server. Returns the server file path.

#### Query Parameters

- `digest`: the expected SHA256 digest of the file

#### Examples

##### Request

```shell
curl -T model.bin -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

##### Response

Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.

## List Local Models

```shell
GET /api/tags
```

List models that are available locally.

### Examples

#### Request

```shell
curl http://localhost:11434/api/tags
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "codellama:13b",
      "modified_at": "2023-11-04T14:56:49.277302595-07:00",
      "size": 7365960935,
      "digest": "9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697",
      "details": {
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "13B",
        "quantization_level": "Q4_0"
      }
    },
    {
      "name": "llama3:latest",
      "modified_at": "2023-12-07T09:32:18.757212583-08:00",
      "size": 3825819519,
      "digest": "fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e",
      "details": {
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

## Show Model Information

```shell
POST /api/show
```

Show information about a model including details, modelfile, template, parameters, license, system prompt.

### Parameters

- `model`: name of the model to show
- `verbose`: (optional) if set to `true`, returns full data for verbose response fields

### Examples

#### Request

```shell
curl http://localhost:11434/api/show -d '{
  "model": "llama3.2"
}'
```

#### Response

```json
{
  "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 4096\nPARAMETER stop \"\u003c/s\u003e\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
  "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"\nstop                           \"<|end_header_id|>\"\nstop                           \"<|eot_id|>\"",
  "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": [
      "llama"
    ],
    "parameter_size": "8.0B",
    "quantization_level": "Q4_0"
  },
  "model_info": {
    "general.architecture": "llama",
    "general.file_type": 2,
    "general.parameter_count": 8030261248,
    "general.quantization_version": 2,
    "llama.attention.head_count": 32,
    "llama.attention.head_count_kv": 8,
    "llama.attention.layer_norm_rms_epsilon": 0.00001,
    "llama.block_count": 32,
    "llama.context_length": 8192,
    "llama.embedding_length": 4096,
    "llama.feed_forward_length": 14336,
    "llama.rope.dimension_count": 128,
    "llama.rope.freq_base": 500000,
    "llama.vocab_size": 128256,
    "tokenizer.ggml.bos_token_id": 128000,
    "tokenizer.ggml.eos_token_id": 128009,
    "tokenizer.ggml.merges": [],            // populates if `verbose=true`
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.pre": "llama-bpe",
    "tokenizer.ggml.token_type": [],        // populates if `verbose=true`
    "tokenizer.ggml.tokens": []             // populates if `verbose=true`
  }
}
```

## Copy a Model

```shell
POST /api/copy
```

Copy a model. Creates a model with another name from an existing model.

### Examples

#### Request

```shell
curl http://localhost:11434/api/copy -d '{
  "source": "llama3.2",
  "destination": "llama3-backup"
}'
```

#### Response

Returns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.

## Delete a Model

```shell
DELETE /api/delete
```

Delete a model and its data.

### Parameters

- `model`: model name to delete

### Examples

#### Request

```shell
curl -X DELETE http://localhost:11434/api/delete -d '{
  "model": "llama3:13b"
}'
```

#### Response

Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.

## Pull a Model

```shell
POST /api/pull
```

Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.

### Parameters

- `model`: name of the model to pull
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/pull -d '{
  "model": "llama3.2"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

The first object is the manifest:

```json
{
  "status": "pulling manifest"
}
```

Then there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.

```json
{
  "status": "downloading digestname",
  "digest": "digestname",
  "total": 2142590208,
  "completed": 241970
}
```

After all the files are downloaded, the final responses are:

```json
{
    "status": "verifying sha256 digest"
}
{
    "status": "writing manifest"
}
{
    "status": "removing any unused layers"
}
{
    "status": "success"
}
```

if `stream` is set to false, then the response is a single JSON object:

```json
{
  "status": "success"
}
```

## Push a Model

```shell
POST /api/push
```

Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.

### Parameters

- `model`: name of the model to push in the form of `<namespace>/<model>:<tag>`
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/push -d '{
  "model": "mattw/pygmalion:latest"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

```json
{ "status": "retrieving manifest" }
```

and then:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Then there is a series of uploading responses:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Finally, when the upload is complete:

```json
{"status":"pushing manifest"}
{"status":"success"}
```

If `stream` is set to `false`, then the response is a single JSON object:

```json
{ "status": "success" }
```

## Generate Embeddings

```shell
POST /api/embed
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `input`: text or list of text to generate embeddings for

Advanced parameters:

- `truncate`: truncates the end of each input to fit within context length. Returns error if `false` and context length is exceeded. Defaults to `true`
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Examples

#### Request

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": "Why is the sky blue?"
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ]],
  "total_duration": 14143917,
  "load_duration": 1019500,
  "prompt_eval_count": 8
}
```

#### Request (Multiple input)

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": ["Why is the sky blue?", "Why is the grass green?"]
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ],[
    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,
    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481
  ]]
}
```

## List Running Models
```shell
GET /api/ps
```

List models that are currently loaded into memory.

#### Examples

### Request

```shell
curl http://localhost:11434/api/ps
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "mistral:latest",
      "model": "mistral:latest",
      "size": 5137025024,
      "digest": "2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "7.2B",
        "quantization_level": "Q4_0"
      },
      "expires_at": "2024-06-04T14:38:31.83753-07:00",
      "size_vram": 5137025024
    }
  ]
}
```

## Generate Embedding

> Note: this endpoint has been superseded by `/api/embed`

```shell
POST /api/embeddings
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `prompt`: text to generate embeddings for

Advanced parameters:

- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Examples

#### Request

```shell
curl http://localhost:11434/api/embeddings -d '{
  "model": "all-minilm",
  "prompt": "Here is an article about llamas..."
}'
```

#### Response

```json
{
  "embedding": [
    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,
    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281
  ]
}
```

================
File: docs/notes.md
================
/home/danny/github-danny/hyperdata/workspaces/hyperdata/articles/semem/plan.md

/home/danny/github-danny/hyperdata/workspaces/danny.ayers.name/entries/2024-11-18_semem.md

/home/danny/github-danny/hyperdata/workspaces/hyperdata/journal/2024-11-18.md

/home/danny/github-danny/hyperdata/workspaces/hyperdata/articles/semem/links.md

- semem is lightweight, componentized trustgraph

[Porting Python Memory Management Project to JavaScript](https://claude.ai/chat/0decba46-fb0b-4c13-a0b6-4fd645cd3113)

#:packer is related

Also look at how Aider does things - it's got a nice code repo format

The #:storm abstraction should be in here somewhere,

/home/danny/github-danny/hyperdata/workspaces/ns/articles/fs-abstraction.md

/home/danny/github-danny/hyperdata/packages/ns/docs/postcraft/content-raw/articles/fs-abstraction.md

================
File: spec/helpers/jasmine_examples/SpecHelper.js
================
beforeEach(function () {
  jasmine.addMatchers({
    toBePlaying: function () {
      return {
        compare: function (actual, expected) {
          const player = actual;

          return {
            pass: player.currentlyPlayingSong === expected && player.isPlaying
          };
        }
      };
    }
  });
});

================
File: spec/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

================
File: spec/support/jasmine.json
================
{
  "spec_dir": "spec",
  "spec_files": [
    "**/*[sS]pec.?(m)js"
  ],
  "helpers": [
    "helpers/**/*.?(m)js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "random": true
  }
}

================
File: src/config.js
================
export default class Config {
    static defaults = {
        storage: {
            type: 'memory',
            options: {
                path: 'interaction_history.json',

                endpoint: 'http://localhost:8080',
                apiKey: '',
                timeout: 5000
            }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'llama2',
                options: {}
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text',
                options: {}
            }
        },
        memory: {
            dimension: 1536,
            similarityThreshold: 40,
            contextWindow: 3,
            decayRate: 0.0001
        }
    };

    constructor(userConfig = {}) {
        this.config = this.mergeConfigs(Config.defaults, userConfig);
    }

    mergeConfigs(defaults, user) {
        const merged = { ...defaults };
        for (const [key, value] of Object.entries(user)) {
            if (value && typeof value === 'object') {
                merged[key] = this.mergeConfigs(defaults[key] || {}, value);
            } else {
                merged[key] = value;
            }
        }
        return merged;
    }

    get(path) {
        return path.split('.').reduce((obj, key) => obj && obj[key], this.config);
    }

    set(path, value) {
        const keys = path.split('.');
        const last = keys.pop();
        const target = keys.reduce((obj, key) => obj[key] = obj[key] || {}, this.config);
        target[last] = value;
    }
}

================
File: src/example.js
================
import MemoryManager from './memoryManager.js';
import JSONStorage from './jsonStorage.js';
import RemoteStorage from './remoteStorage.js';
import Config from './config.js';

async function main() {

    const config = new Config({
        storage: {
            type: 'remote',
            options: {
                endpoint: 'https://api.example.com/memory',
                apiKey: process.env.STORAGE_API_KEY
            }
        },
        models: {
            chat: {
                provider: 'openai',
                model: 'gpt-4-turbo-preview'
            },
            embedding: {
                provider: 'openai',
                model: 'text-embedding-3-small'
            }
        }
    });


    let storage;
    switch (config.get('storage.type')) {
        case 'json':
            storage = new JSONStorage(config.get('storage.options.path'));
            break;
        case 'remote':
            storage = new RemoteStorage(config.get('storage.options'));
            break;
        default:
            storage = new InMemoryStorage();
    }


    const memoryManager = new MemoryManager({
        apiKey: process.env.OPENAI_API_KEY,
        chatModel: config.get('models.chat.provider'),
        chatModelName: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.provider'),
        embeddingModelName: config.get('models.embedding.model'),
        storage
    });


    const prompt = "What's the current state of AI technology?";


    const relevantInteractions = await memoryManager.retrieveRelevantInteractions(prompt);


    const response = await memoryManager.generateResponse(prompt, [], relevantInteractions);
    console.log('Response:', response);


    const embedding = await memoryManager.getEmbedding(`${prompt} ${response}`);
    const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
    await memoryManager.addInteraction(prompt, response, embedding, concepts);
}

main().catch(console.error);

================
File: src/in-memory-storage.js
================
import BaseStorage from './storage.js';
import { logger } from './utils.js';

export default class InMemoryStorage extends BaseStorage {
    constructor() {
        super();
        this.history = {
            shortTermMemory: [],
            longTermMemory: []
        };
    }

    async loadHistory() {
        logger.info('Loading history from in-memory storage');
        return [
            this.history.shortTermMemory || [],
            this.history.longTermMemory || []
        ];
    }

    async saveMemoryToHistory(memoryStore) {
        logger.info('Saving history to in-memory storage');

        this.history = {
            shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                id: item.id,
                prompt: item.prompt,
                output: item.output,
                embedding: Array.from(memoryStore.embeddings[idx].flat()),
                timestamp: memoryStore.timestamps[idx],
                accessCount: memoryStore.accessCounts[idx],
                concepts: Array.from(memoryStore.conceptsList[idx]),
                decayFactor: item.decayFactor || 1.0
            })),
            longTermMemory: [...memoryStore.longTermMemory]
        };

        logger.info(`Saved ${this.history.shortTermMemory.length} short-term and ${this.history.longTermMemory.length} long-term memories`);
    }
}

================
File: src/json-storage.js
================
import { promises as fs } from 'fs';
import BaseStorage from './storage.js';
import { logger } from './utils.js';

export default class JSONStorage extends BaseStorage {
    constructor(filePath = 'interaction_history.json') {
        super();
        this.filePath = filePath;
    }

    async loadHistory() {
        try {
            const exists = await fs.access(this.filePath).then(() => true).catch(() => false);
            if (exists) {
                logger.info('Loading existing interaction history from JSON...');
                const data = await fs.readFile(this.filePath, 'utf8');
                const history = JSON.parse(data);
                return [
                    history.shortTermMemory || [],
                    history.longTermMemory || []
                ];
            }
            logger.info('No existing interaction history found in JSON. Starting fresh.');
            return [[], []];
        } catch (error) {
            logger.error('Error loading history:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            const history = {
                shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx].flat()),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                })),
                longTermMemory: memoryStore.longTermMemory
            };

            await fs.writeFile(this.filePath, JSON.stringify(history, null, 2));
            logger.info(`Saved interaction history to JSON. Short-term: ${history.shortTermMemory.length}, Long-term: ${history.longTermMemory.length}`);
        } catch (error) {
            logger.error('Error saving history:', error);
            throw error;
        }
    }
}

================
File: src/memory-manager.js
================
import { ChatOpenAI } from '@langchain/openai';
import { ChatOllama } from '@langchain/community/chat_models/ollama';
import { OpenAIEmbeddings } from '@langchain/openai';
import ollama from 'ollama';
import { v4 as uuidv4 } from 'uuid';
import MemoryStore from './memoryStore.js';
import InMemoryStorage from './inMemoryStorage.js';
import { logger } from './utils.js';

export default class MemoryManager {
    constructor({
        apiKey,
        chatModel = 'ollama',
        chatModelName = 'llama2',
        embeddingModel = 'ollama',
        embeddingModelName = 'nomic-embed-text',
        storage = null
    }) {
        this.apiKey = apiKey;
        this.chatModelName = chatModelName;
        this.embeddingModelName = embeddingModelName;
        this.dimension = 1536;

        this.initializeChatModel(chatModel, chatModelName);
        this.initializeEmbeddingModel(embeddingModel, embeddingModelName);

        this.memoryStore = new MemoryStore(this.dimension);
        this.storage = storage || new InMemoryStorage();

        this.initialize();
    }

    initializeChatModel(chatModel, modelName) {
        if (chatModel.toLowerCase() === 'openai') {
            this.llm = new ChatOpenAI({
                modelName: modelName,
                apiKey: this.apiKey
            });
        } else if (chatModel.toLowerCase() === 'ollama') {
            this.llm = new ChatOllama({
                model: modelName,
                temperature: 0
            });
        } else {
            throw new Error(`Unsupported chat model: ${chatModel}`);
        }
    }

    async initializeEmbeddingModel(embeddingModel, modelName) {
        if (embeddingModel.toLowerCase() === 'openai') {
            this.embeddings = new OpenAIEmbeddings({
                modelName,
                apiKey: this.apiKey
            });
            this.dimension = modelName === 'text-embedding-3-small' ? 1536 : 1024;
        } else if (embeddingModel.toLowerCase() === 'ollama') {
            this.embeddings = async (text) => {
                const response = await ollama.embeddings({
                    model: modelName,
                    prompt: text
                });
                return response.embedding;
            };
            this.dimension = 1024;
        } else {
            throw new Error(`Unsupported embedding model: ${embeddingModel}`);
        }
    }

    async initialize() {
        const [shortTerm, longTerm] = await this.storage.loadHistory();

        for (const interaction of shortTerm) {
            const embedding = this.standardizeEmbedding(interaction.embedding);
            interaction.embedding = embedding;
            this.memoryStore.addInteraction(interaction);
        }

        this.memoryStore.longTermMemory.push(...longTerm);
        this.memoryStore.clusterInteractions();

        logger.info(`Memory initialized with ${shortTerm.length} short-term and ${longTerm.length} long-term memories`);
    }

    standardizeEmbedding(embedding) {
        const current = embedding.length;
        if (current === this.dimension) return embedding;

        if (current < this.dimension) {
            return [...embedding, ...new Array(this.dimension - current).fill(0)];
        }
        return embedding.slice(0, this.dimension);
    }

    async getEmbedding(text) {
        logger.info('Generating embedding...');
        let embedding;

        try {
            if (typeof this.embeddings === 'function') {
                embedding = await this.embeddings(text);
            } else {
                embedding = await this.embeddings.embedQuery(text);
            }

            return this.standardizeEmbedding(embedding);
        } catch (error) {
            logger.error('Error generating embedding:', error);
            throw error;
        }
    }

    async extractConcepts(text) {
        logger.info('Extracting concepts...');

        const messages = [{
            role: 'system',
            content: 'Extract key concepts from the text. Return only an array of strings.'
        }, {
            role: 'user',
            content: text
        }];

        try {
            const response = await this.llm.call(messages);
            const concepts = JSON.parse(response.content);
            logger.info('Extracted concepts:', concepts);
            return concepts;
        } catch (error) {
            logger.error('Error extracting concepts:', error);
            return [];
        }
    }

    async addInteraction(prompt, output, embedding, concepts) {
        const interaction = {
            id: uuidv4(),
            prompt,
            output,
            embedding,
            timestamp: Date.now(),
            accessCount: 1,
            concepts,
            decayFactor: 1.0
        };

        this.memoryStore.addInteraction(interaction);
        await this.storage.saveMemoryToHistory(this.memoryStore);
    }

    async retrieveRelevantInteractions(query, similarityThreshold = 40, excludeLastN = 0) {
        const queryEmbedding = await this.getEmbedding(query);
        const queryConcepts = await this.extractConcepts(query);
        return this.memoryStore.retrieve(queryEmbedding, queryConcepts, similarityThreshold, excludeLastN);
    }

    async generateResponse(prompt, lastInteractions = [], retrievals = [], contextWindow = 3) {
        const context = this.buildContext(lastInteractions, retrievals, contextWindow);

        const messages = [{
            role: 'system',
            content: "You're a helpful assistant with memory of past interactions."
        }, {
            role: 'user',
            content: `${context}\nCurrent prompt: ${prompt}`
        }];

        try {
            const response = await this.llm.call(messages);
            return response.content.trim();
        } catch (error) {
            logger.error('Error generating response:', error);
            throw error;
        }
    }

    buildContext(lastInteractions, retrievals, contextWindow) {

================
File: src/memory-store.js
================
import faiss from 'faiss';
import { createRequire } from 'module';
import { kmeans } from 'ml-kmeans';
import { logger, vectorOps } from './utils.js';

const require = createRequire(import.meta.url);
const { Graph } = require('graphology');

export default class MemoryStore {
    constructor(dimension = 1536) {
        this.dimension = dimension;
        this.index = new faiss.IndexFlatL2(dimension);
        this.shortTermMemory = [];
        this.longTermMemory = [];
        this.embeddings = [];
        this.timestamps = [];
        this.accessCounts = [];
        this.conceptsList = [];
        this.graph = new Graph();
        this.semanticMemory = new Map();
        this.clusterLabels = [];
    }

    addInteraction(interaction) {
        const { id, prompt, output, embedding, timestamp = Date.now(),
                accessCount = 1, concepts = [], decayFactor = 1.0 } = interaction;

        logger.info(`Adding interaction: '${prompt}'`);

        this.shortTermMemory.push({
            id, prompt, output, timestamp, accessCount, decayFactor
        });

        this.embeddings.push(new Float32Array(embedding.flat()));
        this.index.add(new Float32Array(embedding.flat()));
        this.timestamps.push(timestamp);
        this.accessCounts.push(accessCount);
        this.conceptsList.push(new Set(concepts));

        this.updateGraph(new Set(concepts));
        this.clusterInteractions();
    }

    updateGraph(concepts) {
        for (const concept of concepts) {
            if (!this.graph.hasNode(concept)) {
                this.graph.addNode(concept);
            }
        }

        for (const concept1 of concepts) {
            for (const concept2 of concepts) {
                if (concept1 !== concept2) {
                    const edgeKey = `${concept1}--${concept2}`;
                    if (this.graph.hasEdge(edgeKey)) {
                        const weight = this.graph.getEdgeAttribute(edgeKey, 'weight');
                        this.graph.setEdgeAttribute(edgeKey, 'weight', weight + 1);
                    } else {
                        this.graph.addEdge(concept1, concept2, { weight: 1 });
                    }
                }
            }
        }
    }

    classifyMemory() {
        this.shortTermMemory.forEach((interaction, idx) => {
            if (this.accessCounts[idx] > 10 &&
                !this.longTermMemory.some(ltm => ltm.id === interaction.id)) {
                this.longTermMemory.push(interaction);
                logger.info(`Moved interaction ${interaction.id} to long-term memory`);
            }
        });
    }

    async retrieve(queryEmbedding, queryConcepts, similarityThreshold = 40, excludeLastN = 0) {
        if (this.shortTermMemory.length === 0) {
            logger.info('No interactions available');
            return [];
        }

        logger.info('Retrieving relevant interactions...');
        const relevantInteractions = [];
        const currentTime = Date.now();
        const decayRate = 0.0001;
        const relevantIndices = new Set();

        const normalizedQuery = vectorOps.normalize(queryEmbedding.flat());
        const normalizedEmbeddings = this.embeddings.map(e => vectorOps.normalize(Array.from(e)));

        for (let idx = 0; idx < this.shortTermMemory.length - excludeLastN; idx++) {
            const similarity = vectorOps.cosineSimilarity(normalizedQuery, normalizedEmbeddings[idx]) * 100;
            const timeDiff = (currentTime - this.timestamps[idx]) / 1000;
            const decayFactor = this.shortTermMemory[idx].decayFactor * Math.exp(-decayRate * timeDiff);
            const reinforcementFactor = Math.log1p(this.accessCounts[idx]);
            const adjustedSimilarity = similarity * decayFactor * reinforcementFactor;

            if (adjustedSimilarity >= similarityThreshold) {
                relevantIndices.add(idx);
                this.accessCounts[idx]++;
                this.timestamps[idx] = currentTime;
                this.shortTermMemory[idx].decayFactor *= 1.1;

                relevantInteractions.push({
                    similarity: adjustedSimilarity,
                    interaction: this.shortTermMemory[idx],
                    concepts: this.conceptsList[idx]
                });
            }
        }


        this.shortTermMemory.forEach((item, idx) => {
            if (!relevantIndices.has(idx)) {
                item.decayFactor *= 0.9;
            }
        });

        const activatedConcepts = await this.spreadingActivation(queryConcepts);


        return this.combineResults(relevantInteractions, activatedConcepts, normalizedQuery);
    }

    async spreadingActivation(queryConcepts) {
        const activatedNodes = new Map();
        const initialActivation = 1.0;
        const decayFactor = 0.5;

        queryConcepts.forEach(concept => {
            activatedNodes.set(concept, initialActivation);
        });


        for (let step = 0; step < 2; step++) {
            const newActivations = new Map();

            for (const [node, activation] of activatedNodes) {
                if (this.graph.hasNode(node)) {
                    this.graph.forEachNeighbor(node, (neighbor, attributes) => {
                        if (!activatedNodes.has(neighbor)) {
                            const weight = attributes.weight;
                            const newActivation = activation * decayFactor * weight;
                            newActivations.set(neighbor,
                                (newActivations.get(neighbor) || 0) + newActivation);
                        }
                    });
                }
            }

            newActivations.forEach((value, key) => {
                activatedNodes.set(key, value);
            });
        }

        return Object.fromEntries(activatedNodes);
    }

    clusterInteractions() {
        if (this.embeddings.length < 2) return;

        const embeddingsMatrix = this.embeddings.map(e => Array.from(e));
        const numClusters = Math.min(10, this.embeddings.length);

        const { clusters } = kmeans(embeddingsMatrix, numClusters);
        this.clusterLabels = clusters;

        this.semanticMemory.clear();
        clusters.forEach((label, idx) => {
            if (!this.semanticMemory.has(label)) {
                this.semanticMemory.set(label, []);
            }
            this.semanticMemory.get(label).push({
                embedding: this.embeddings[idx],
                interaction: this.shortTermMemory[idx]
            });
        });
    }

    combineResults(relevantInteractions, activatedConcepts, normalizedQuery) {
        const combined = relevantInteractions.map(({ similarity, interaction, concepts }) => {
            const activationScore = Array.from(concepts)
                .reduce((sum, c) => sum + (activatedConcepts[c] || 0), 0);
            return {
                ...interaction,
                totalScore: similarity + activationScore
            };
        });

        combined.sort((a, b) => b.totalScore - a.totalScore);


        const semanticResults = this.retrieveFromSemanticMemory(normalizedQuery);
        return [...combined, ...semanticResults];
    }

    retrieveFromSemanticMemory(normalizedQuery) {
        if (this.semanticMemory.size === 0) return [];


        let bestCluster = -1;
        let bestSimilarity = -1;

        this.semanticMemory.forEach((items, label) => {
            const centroid = this.calculateCentroid(items.map(i => i.embedding));
            const similarity = vectorOps.cosineSimilarity(normalizedQuery, centroid);

            if (similarity > bestSimilarity) {
                bestSimilarity = similarity;
                bestCluster = label;
            }
        });

        if (bestCluster === -1) return [];


        return this.semanticMemory.get(bestCluster)
            .map(({ embedding, interaction }) => ({
                ...interaction,
                similarity: vectorOps.cosineSimilarity(normalizedQuery,
                    vectorOps.normalize(Array.from(embedding)))
            }))
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, 5);
    }

    calculateCentroid(embeddings) {
        const sum = embeddings.reduce((acc, curr) => {
            const arr = Array.from(curr);
            return acc.map((val, idx) => val + arr[idx]);
        }, new Array(this.dimension).fill(0));

        return sum.map(val => val / embeddings.length);
    }
}

================
File: src/remote-storage.js
================
import BaseStorage from './storage.js';
import { logger } from './utils.js';

export default class RemoteStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.endpoint = options.endpoint || 'http://localhost:8080';
        this.apiKey = options.apiKey;
        this.timeout = options.timeout || 5000;
    }

    async loadHistory() {
        try {
            const response = await fetch(`${this.endpoint}/memory`, {
                method: 'GET',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json'
                },
                timeout: this.timeout
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            return [
                data.shortTermMemory || [],
                data.longTermMemory || []
            ];
        } catch (error) {
            logger.error('Error loading remote history:', error);
            throw error;
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            const history = {
                shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx].flat()),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                })),
                longTermMemory: memoryStore.longTermMemory
            };

            const response = await fetch(`${this.endpoint}/memory`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(history),
                timeout: this.timeout
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            logger.info(`Saved memory to remote storage. Short-term: ${history.shortTermMemory.length}, Long-term: ${history.longTermMemory.length}`);
        } catch (error) {
            logger.error('Error saving to remote storage:', error);
            throw error;
        }
    }
}

================
File: src/storage-base.js
================
export default class BaseStorage {
    async loadHistory() {
        throw new Error('Method loadHistory() must be implemented');
    }

    async saveMemoryToHistory(memoryStore) {
        throw new Error('Method saveMemoryToHistory() must be implemented');
    }
}

================
File: src/utils.js
================
export const logger = {
    info: (...args) => console.log('[INFO]', ...args),
    error: (...args) => console.error('[ERROR]', ...args),
    debug: (...args) => console.debug('[DEBUG]', ...args)
};


export const vectorOps = {
    normalize: (vector) => {
        const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
        return vector.map(val => val / magnitude);
    },

    cosineSimilarity: (vec1, vec2) => {
        const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
        const mag1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));
        const mag2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));
        return dotProduct / (mag1 * mag2);
    }
};

================
File: .git
================
gitdir: ../../.git/modules/packages/semem

================
File: .gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: jsdoc.json
================
{
    "source": {
        "include": [
            "src"
        ],
        "exclude": [
            "node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "./docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: package.json
================
{
  "name": "semem",
  "version": "1.0.0",
  "description": "Semantic Memory",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "test": "jasmine",
    "rp": "repomix -c repomix.config.json . "
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/danja/semem.git"
  },
  "keywords": [
    "semantic",
    "memory",
    "llm",
    "rdf",
    "sparql"
  ],
  "author": "Danny Ayers",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/danja/semem/issues"
  },
  "homepage": "https://github.com/danja/semem#readme",
  "devDependencies": {
    "jasmine": "^5.4.0",
    "jsdoc": "^4.0.4"
  },
  "dependencies": {
    "@langchain/core": "^0.3.19",
    "@langchain/openai": "^0.3.14",
    "faiss": "^1.0.0",
    "ollama": "^0.5.10"
  }
}

================
File: repomix.config.json
================
{
    "output": {
        "filePath": "./repomix-semem.txt",
        "headerText": "Semem repo",
        "removeComments": true
    },
    "include": [
        "**/*"
    ],
    "ignore": {
        "useDefaultPatterns": false,
        "customPatterns": [
            ".env",
            "**/_*",
            "node_modules",
            "*.log",
            "**/*repomix*.txt",
            "**/*.html",
            "**/data/*",
            "**/*copy.js",
            "**/conversations.json"
        ]
    }
}
