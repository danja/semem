This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-13T19:39:38.327Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
- Code comments have been removed.

Additional Info:
----------------
User Provided Header:
-----------------------
Semem repo

================================================================
Directory Structure
================================================================
_old-src/
  example.js
  memory-manager.js
  remote-storage.js
docs/
  2024-11-28/
    api-docs.md
    custom-storage.md
    deployment-docs.md
    deployment.md
    memory-docs.md
    monitoring-docs.md
    quickstart.md
    setup-config.md
    storage-docs.md
    testing-docs.md
    troubleshooting.md
    utils-docs.md
  2024-12-29/
    ollama-rest-api.md
  2025-01-01/
    fix-sparql-prompt.md
  2025-01-13/
    handover-api.ttl
    handover-api1.md
  artifacts_2024-12-29/
    context-manager.js
    context-window.js
    memory-manager.js
    ollama-api.js
    ollama-example.js
    prompt-templates.js
    semem-docs.md
    semem-rdf.txt
    test-suite.js
  description_2024-12-30/
    semem-architecture.mermaid
    semem-description.md
    semem-next-steps.md
  description_2025-01-01/
    architecture.md
    capabilities.md
    concept-system.md
    config-guide.md
    custom-storage.js
    memory-dynamics.md
    memory-flow.mermaid
    retrieval-algorithm.md
    sparql-details.md
    system-overview.mermaid
    troubleshooting.md
    usage-example.js
  description_latest/
    2025-01-13_api-req.md
    architecture.md
    capabilities.md
    concept-system.md
    config-guide.md
    custom-storage.js
    handover-api.ttl
    handover-api1.md
    memory-dynamics.md
    memory-flow.mermaid
    retrieval-algorithm.md
    sparql-details.md
    system-overview.mermaid
    troubleshooting.md
    usage-example.js
  prompts/
    2025-01-13_api-docs.md
    2025-01-13_api.md
  handover_2024-12-30.md
  handover_2024-12-30.ttl
  notes.md
  sparql-prompt_2024-12-30.md
misc/
  scripts/
    ollama-embedding-test.sh
    sparql-auth-test.sh
    sparql-upload-test.sh
src/
  api/
    cli/
      about.md
      CLIHandler.js
    common/
      APIRegistry.js
      BaseAPI.js
      CustomValidators.js
      RDFParser.js
      RDFValidator.js
      types.d.ts.ts
    features/
      ActiveHandler.js
      PassiveHandler.js
      SelfieHandler.js
    http/
      client/
        SememClient.js
      server/
        HTTPServer.js
        openapi-schema.js
    repl/
      REPLHandler.js
    about.md
    APILogger.js
    MetricsCollector.js
  connectors/
    OllamaConnector.js
  stores/
    BaseStore.js
    CachedSPARQLStore.js
    InMemoryStore.js
    JSONStore.js
    MemoryStore.js
    SPARQLStore.js
  utils/
    SPARQLHelpers.js
  Config.js
  ContextManager.js
  ContextWindowManager.js
  MemoryManager.js
  OllamaExample.js
  PromptTemplates.js
  SPARQLExample.js
  Utils.js
tests/
  helpers/
    jasmine_examples/
      SpecHelper.js
    reporter.js
  integration/
    llms/
      Ollama.spec.js
    sparql/
      sparql-advanced-backup-spec.js
      sparql-basic-backup-spec.js
      sparql-federation-spec.js
      sparql-store-integration-spec.js
  mocks/
    Ollama.js
  support/
    jasmine.json
  unit/
    cached-sparql-store-spec.js
    ContextWindowManager.spec.js
    MemoryManager.spec.js
    sparql-endpoint-spec.js
    sparql-store-spec.js
.git
.gitignore
about.md
jasmine.json
jsdoc.json
LICENSE
package.json
repomix.config.json

================================================================
Files
================================================================

================
File: _old-src/example.js
================
import MemoryManager from './memoryManager.js';
import JSONStorage from './jsonStorage.js';
import RemoteStorage from './remoteStorage.js';
import Config from './config.js';

async function main() {

    const config = new Config({
        storage: {
            type: 'remote',
            options: {
                endpoint: 'https://api.example.com/memory',
                apiKey: process.env.STORAGE_API_KEY
            }
        },
        models: {
            chat: {
                provider: 'openai',
                model: 'gpt-4-turbo-preview'
            },
            embedding: {
                provider: 'openai',
                model: 'text-embedding-3-small'
            }
        }
    });


    let storage;
    switch (config.get('storage.type')) {
        case 'json':
            storage = new JSONStorage(config.get('storage.options.path'));
            break;
        case 'remote':
            storage = new RemoteStorage(config.get('storage.options'));
            break;
        default:
            storage = new InMemoryStorage();
    }


    const memoryManager = new MemoryManager({
        apiKey: process.env.OPENAI_API_KEY,
        chatModel: config.get('models.chat.provider'),
        chatModelName: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.provider'),
        embeddingModelName: config.get('models.embedding.model'),
        storage
    });


    const prompt = "What's the current state of AI technology?";


    const relevantInteractions = await memoryManager.retrieveRelevantInteractions(prompt);


    const response = await memoryManager.generateResponse(prompt, [], relevantInteractions);
    console.log('Response:', response);


    const embedding = await memoryManager.getEmbedding(`${prompt} ${response}`);
    const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
    await memoryManager.addInteraction(prompt, response, embedding, concepts);
}

main().catch(console.error);

================
File: _old-src/memory-manager.js
================
import { ChatOpenAI } from '@langchain/openai';
import { ChatOllama } from '@langchain/community/chat_models/ollama';
import { OpenAIEmbeddings } from '@langchain/openai';
import ollama from 'ollama';
import { v4 as uuidv4 } from 'uuid';
import MemoryStore from './memoryStore.js';
import InMemoryStorage from './inMemoryStorage.js';
import { logger } from './utils.js';

export default class MemoryManager {
    constructor({
        apiKey,
        chatModel = 'ollama',
        chatModelName = 'llama2',
        embeddingModel = 'ollama',
        embeddingModelName = 'nomic-embed-text',
        storage = null
    }) {
        this.apiKey = apiKey;
        this.chatModelName = chatModelName;
        this.embeddingModelName = embeddingModelName;
        this.dimension = 1536;

        this.initializeChatModel(chatModel, chatModelName);
        this.initializeEmbeddingModel(embeddingModel, embeddingModelName);

        this.memoryStore = new MemoryStore(this.dimension);
        this.storage = storage || new InMemoryStorage();

        this.initialize();
    }

    initializeChatModel(chatModel, modelName) {
        if (chatModel.toLowerCase() === 'openai') {
            this.llm = new ChatOpenAI({
                modelName: modelName,
                apiKey: this.apiKey
            });
        } else if (chatModel.toLowerCase() === 'ollama') {
            this.llm = new ChatOllama({
                model: modelName,
                temperature: 0
            });
        } else {
            throw new Error(`Unsupported chat model: ${chatModel}`);
        }
    }

    async initializeEmbeddingModel(embeddingModel, modelName) {
        if (embeddingModel.toLowerCase() === 'openai') {
            this.embeddings = new OpenAIEmbeddings({
                modelName,
                apiKey: this.apiKey
            });
            this.dimension = modelName === 'text-embedding-3-small' ? 1536 : 1024;
        } else if (embeddingModel.toLowerCase() === 'ollama') {
            this.embeddings = async (text) => {
                const response = await ollama.embeddings({
                    model: modelName,
                    prompt: text
                });
                return response.embedding;
            };
            this.dimension = 1024;
        } else {
            throw new Error(`Unsupported embedding model: ${embeddingModel}`);
        }
    }

    async initialize() {
        const [shortTerm, longTerm] = await this.storage.loadHistory();

        for (const interaction of shortTerm) {
            const embedding = this.standardizeEmbedding(interaction.embedding);
            interaction.embedding = embedding;
            this.memoryStore.addInteraction(interaction);
        }

        this.memoryStore.longTermMemory.push(...longTerm);
        this.memoryStore.clusterInteractions();

        logger.info(`Memory initialized with ${shortTerm.length} short-term and ${longTerm.length} long-term memories`);
    }

    standardizeEmbedding(embedding) {
        const current = embedding.length;
        if (current === this.dimension) return embedding;

        if (current < this.dimension) {
            return [...embedding, ...new Array(this.dimension - current).fill(0)];
        }
        return embedding.slice(0, this.dimension);
    }

    async getEmbedding(text) {
        logger.info('Generating embedding...');
        let embedding;

        try {
            if (typeof this.embeddings === 'function') {
                embedding = await this.embeddings(text);
            } else {
                embedding = await this.embeddings.embedQuery(text);
            }

            return this.standardizeEmbedding(embedding);
        } catch (error) {
            logger.error('Error generating embedding:', error);
            throw error;
        }
    }

    async extractConcepts(text) {
        logger.info('Extracting concepts...');

        const messages = [{
            role: 'system',
            content: 'Extract key concepts from the text. Return only an array of strings.'
        }, {
            role: 'user',
            content: text
        }];

        try {
            const response = await this.llm.call(messages);
            const concepts = JSON.parse(response.content);
            logger.info('Extracted concepts:', concepts);
            return concepts;
        } catch (error) {
            logger.error('Error extracting concepts:', error);
            return [];
        }
    }

    async addInteraction(prompt, output, embedding, concepts) {
        const interaction = {
            id: uuidv4(),
            prompt,
            output,
            embedding,
            timestamp: Date.now(),
            accessCount: 1,
            concepts,
            decayFactor: 1.0
        };

        this.memoryStore.addInteraction(interaction);
        await this.storage.saveMemoryToHistory(this.memoryStore);
    }

    async retrieveRelevantInteractions(query, similarityThreshold = 40, excludeLastN = 0) {
        const queryEmbedding = await this.getEmbedding(query);
        const queryConcepts = await this.extractConcepts(query);
        return this.memoryStore.retrieve(queryEmbedding, queryConcepts, similarityThreshold, excludeLastN);
    }

    async generateResponse(prompt, lastInteractions = [], retrievals = [], contextWindow = 3) {
        const context = this.buildContext(lastInteractions, retrievals, contextWindow);

        const messages = [{
            role: 'system',
            content: "You're a helpful assistant with memory of past interactions."
        }, {
            role: 'user',
            content: `${context}\nCurrent prompt: ${prompt}`
        }];

        try {
            const response = await this.llm.call(messages);
            return response.content.trim();
        } catch (error) {
            logger.error('Error generating response:', error);
            throw error;
        }
    }

    buildContext(lastInteractions, retrievals, contextWindow) {

================
File: _old-src/remote-storage.js
================
import BaseStorage from './storage.js';
import { logger } from './utils.js';

export default class RemoteStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.endpoint = options.endpoint || 'http://localhost:8080';
        this.apiKey = options.apiKey;
        this.timeout = options.timeout || 5000;
    }

    async loadHistory() {
        try {
            const response = await fetch(`${this.endpoint}/memory`, {
                method: 'GET',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json'
                },
                timeout: this.timeout
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            return [
                data.shortTermMemory || [],
                data.longTermMemory || []
            ];
        } catch (error) {
            logger.error('Error loading remote history:', error);
            throw error;
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            const history = {
                shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx].flat()),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                })),
                longTermMemory: memoryStore.longTermMemory
            };

            const response = await fetch(`${this.endpoint}/memory`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(history),
                timeout: this.timeout
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            logger.info(`Saved memory to remote storage. Short-term: ${history.shortTermMemory.length}, Long-term: ${history.longTermMemory.length}`);
        } catch (error) {
            logger.error('Error saving to remote storage:', error);
            throw error;
        }
    }
}

================
File: docs/2024-11-28/api-docs.md
================
# Remote Storage API Documentation

## Base URL
`https://your-api-endpoint/memory`

## Endpoints

### GET /memory
Retrieves all stored memories.

```http
GET /memory
Authorization: Bearer <your-api-key>
```

Response:
```javascript
{
  "shortTermMemory": [
    {
      "id": "uuid",
      "prompt": "string",
      "output": "string",
      "embedding": [number],
      "timestamp": number,
      "accessCount": number,
      "concepts": [string],
      "decayFactor": number
    }
  ],
  "longTermMemory": [/* similar structure */]
}
```

### POST /memory
Stores new memory state.

```http
POST /memory
Authorization: Bearer <your-api-key>
Content-Type: application/json

{
  "shortTermMemory": [...],
  "longTermMemory": [...]
}
```

### Error Responses
- 401: Unauthorized
- 403: Forbidden
- 500: Internal Server Error

================
File: docs/2024-11-28/custom-storage.md
================
# Custom Storage Implementations

## Redis Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import Redis from 'ioredis';

export default class RedisStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.redis = new Redis({
            host: options.host || 'localhost',
            port: options.port || 6379,
            password: options.password,
            keyPrefix: 'semem:'
        });
    }

    async loadHistory() {
        try {
            const shortTerm = await this.redis.get('short_term_memory');
            const longTerm = await this.redis.get('long_term_memory');
            
            return [
                JSON.parse(shortTerm || '[]'),
                JSON.parse(longTerm || '[]')
            ];
        } catch (error) {
            logger.error('Redis load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            const pipeline = this.redis.pipeline();
            
            pipeline.set('short_term_memory', 
                JSON.stringify(memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx].flat()),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                }))));
                
            pipeline.set('long_term_memory', 
                JSON.stringify(memoryStore.longTermMemory));
            
            await pipeline.exec();
        } catch (error) {
            logger.error('Redis save error:', error);
            throw error;
        }
    }
}
```

## MongoDB Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import { MongoClient } from 'mongodb';

export default class MongoStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.url = options.url || 'mongodb://localhost:27017';
        this.dbName = options.dbName || 'semem';
        this.client = null;
        this.db = null;
    }

    async connect() {
        if (!this.client) {
            this.client = await MongoClient.connect(this.url);
            this.db = this.client.db(this.dbName);
        }
    }

    async loadHistory() {
        try {
            await this.connect();
            
            const shortTerm = await this.db.collection('short_term_memory')
                .find({})
                .sort({ timestamp: -1 })
                .toArray();
                
            const longTerm = await this.db.collection('long_term_memory')
                .find({})
                .toArray();
                
            return [shortTerm, longTerm];
        } catch (error) {
            logger.error('MongoDB load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            await this.connect();
            
            // Create session for transaction
            const session = this.client.startSession();
            
            try {
                await session.withTransaction(async () => {
                    // Clear existing memories
                    await this.db.collection('short_term_memory').deleteMany({}, { session });
                    
                    // Insert new short-term memories
                    if (memoryStore.shortTermMemory.length > 0) {
                        await this.db.collection('short_term_memory').insertMany(
                            memoryStore.shortTermMemory.map((item, idx) => ({
                                id: item.id,
                                prompt: item.prompt,
                                output: item.output,
                                embedding: Array.from(memoryStore.embeddings[idx].flat()),
                                timestamp: memoryStore.timestamps[idx],
                                accessCount: memoryStore.accessCounts[idx],
                                concepts: Array.from(memoryStore.conceptsList[idx]),
                                decayFactor: item.decayFactor || 1.0
                            })),
                            { session }
                        );
                    }
                    
                    // Update long-term memories
                    await this.db.collection('long_term_memory').deleteMany({}, { session });
                    if (memoryStore.longTermMemory.length > 0) {
                        await this.db.collection('long_term_memory').insertMany(
                            memoryStore.longTermMemory,
                            { session }
                        );
                    }
                });
            } finally {
                await session.endSession();
            }
        } catch (error) {
            logger.error('MongoDB save error:', error);
            throw error;
        }
    }
}
```

## SQLite Storage Implementation
```javascript
import { BaseStorage } from 'semem';
import sqlite3 from 'sqlite3';
import { open } from 'sqlite';

export default class SQLiteStorage extends BaseStorage {
    constructor(options = {}) {
        super();
        this.dbPath = options.dbPath || ':memory:';
        this.db = null;
    }

    async init() {
        if (!this.db) {
            this.db = await open({
                filename: this.dbPath,
                driver: sqlite3.Database
            });
            
            await this.createTables();
        }
    }

    async createTables() {
        await this.db.exec(`
            CREATE TABLE IF NOT EXISTS short_term_memory (
                id TEXT PRIMARY KEY,
                prompt TEXT,
                output TEXT,
                embedding BLOB,
                timestamp INTEGER,
                access_count INTEGER,
                concepts TEXT,
                decay_factor REAL
            );
            
            CREATE TABLE IF NOT EXISTS long_term_memory (
                id TEXT PRIMARY KEY,
                prompt TEXT,
                output TEXT,
                timestamp INTEGER,
                concepts TEXT
            );
        `);
    }

    async loadHistory() {
        try {
            await this.init();
            
            const shortTerm = await this.db.all(`
                SELECT * FROM short_term_memory
                ORDER BY timestamp DESC
            `);
            
            const longTerm = await this.db.all(`
                SELECT * FROM long_term_memory
            `);
            
            // Convert stored format back to application format
            return [
                shortTerm.map(row => ({
                    id: row.id,
                    prompt: row.prompt,
                    output: row.output,
                    embedding: new Float32Array(row.embedding),
                    timestamp: row.timestamp,
                    accessCount: row.access_count,
                    concepts: JSON.parse(row.concepts),
                    decayFactor: row.decay_factor
                })),
                longTerm.map(row => ({
                    id: row.id,
                    prompt: row.prompt,
                    output: row.output,
                    timestamp: row.timestamp,
                    concepts: JSON.parse(row.concepts)
                }))
            ];
        } catch (error) {
            logger.error('SQLite load error:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            await this.init();
            
            await this.db.run('BEGIN TRANSACTION');
            
            try {
                // Clear existing memories
                await this.db.run('DELETE FROM short_term_memory');
                await this.db.run('DELETE FROM long_term_memory');
                
                // Insert short-term memories
                const shortTermStmt = await this.db.prepare(`
                    INSERT INTO short_term_memory VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                `);
                
                for (let idx = 0; idx < memoryStore.shortTermMemory.length; idx++) {
                    const item = memoryStore.shortTermMemory[idx];
                    await shortTermStmt.run(
                        item.id,
                        item.prompt,
                        item.output,
                        Buffer.from(memoryStore.embeddings[idx].buffer),
                        memoryStore.timestamps[idx],
                        memoryStore.accessCounts[idx],
                        JSON.stringify(Array.from(memoryStore.conceptsList[idx])),
                        item.decayFactor || 1.0
                    );
                }
                
                // Insert long-term memories
                const longTermStmt = await this.db.prepare(`
                    INSERT INTO long_term_memory VALUES (?, ?, ?, ?, ?)
                `);
                
                for (const item of memoryStore.longTermMemory) {
                    await longTermStmt.run(
                        item.id,
                        item.prompt,
                        item.output,
                        item.timestamp,
                        JSON.stringify(item.concepts)
                    );
                }
                
                await this.db.run('COMMIT');
            } catch (error) {
                await this.db.run('ROLLBACK');
                throw error;
            }
        } catch (error) {
            logger.error('SQLite save error:', error);
            throw error;
        }
    }
}
```

Q1: Would you like to see a GraphDB storage implementation?
Q2: Should I show an S3/Object Storage implementation?
Q3: Would you like to see a distributed storage implementation?
Q4: Should I add caching layer examples?

================
File: docs/2024-11-28/deployment-docs.md
================
# Deployment Guide

## Prerequisites
- Node.js 18+
- NPM or Yarn
- OpenAI API key (optional)
- Ollama installation (optional)

## Installation
```bash
npm install semem
```

## Environment Setup
```bash
OPENAI_API_KEY=your-key
STORAGE_API_KEY=your-storage-key
STORAGE_ENDPOINT=https://api.example.com
```

## Docker Deployment
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD ["node", "index.js"]
```

## Security Considerations
- API key management
- Rate limiting
- Error handling
- Logging configuration

================
File: docs/2024-11-28/deployment.md
================
# Deployment Guide

## Prerequisites
```bash
# Node.js 18+ required
node --version

# Install dependencies
npm install semem
```

## Environment Setup
```bash
# .env file
OPENAI_API_KEY=your-key
STORAGE_API_KEY=your-storage-key
STORAGE_ENDPOINT=https://api.example.com
```

## Docker Deployment
```dockerfile
FROM node:18-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install

# Copy application files
COPY . .

# Set environment variables
ENV NODE_ENV=production

# Start the application
CMD ["node", "index.js"]
```

## Docker Compose
```yaml
version: '3.8'
services:
  semem:
    build: .
    environment:
      - NODE_ENV=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports:
      - "3000:3000"
    volumes:
      - ./data:/app/data
```

## PM2 Deployment
```bash
# Install PM2
npm install -g pm2

# Start application
pm2 start index.js --name semem

# Enable startup
pm2 startup
pm2 save
```

================
File: docs/2024-11-28/memory-docs.md
================
# Memory System Documentation

## MemoryStore (memoryStore.js)
Core memory management implementation handling storage, retrieval, and processing of memories.

### Key Components
- FAISS index for similarity search
- Graph-based concept relationships
- Clustering mechanism
- Decay/reinforcement system

### Features
- Vector similarity search
- Hierarchical memory clustering
- Spreading activation
- Memory lifecycle management

### Implementation Details
- Uses Float32Array for embeddings
- Implements k-means clustering
- Manages memory transitions
- Handles concept graph updates

## MemoryManager (memoryManager.js)
High-level interface coordinating all memory operations.

### Key Features
- LLM integration (OpenAI/Ollama)
- Embedding generation
- Concept extraction
- Response generation

### Implementation Details
- Model initialization and management
- Embedding standardization
- Context building
- Storage coordination

### Usage Patterns
- Interactive response generation
- Memory persistence
- Concept management
- Configuration handling

================
File: docs/2024-11-28/monitoring-docs.md
================
# Monitoring Documentation

## System Metrics

### Memory Usage Monitoring
```javascript
import { EventEmitter } from 'events';
import { logger } from './utils.js';

class MemoryMonitor extends EventEmitter {
    constructor(memoryManager) {
        super();
        this.memoryManager = memoryManager;
        this.stats = {
            shortTermCount: 0,
            longTermCount: 0,
            embeddingSize: 0,
            lastAccessed: new Date()
        };
    }

    collectMetrics() {
        this.stats = {
            shortTermCount: this.memoryManager.memoryStore.shortTermMemory.length,
            longTermCount: this.memoryManager.memoryStore.longTermMemory.length,
            embeddingSize: this.memoryManager.memoryStore.embeddings.reduce((acc, curr) => 
                acc + curr.byteLength, 0),
            lastAccessed: new Date()
        };
        this.emit('metrics', this.stats);
        logger.debug('Memory metrics:', this.stats);
    }
}
```

### Performance Metrics
```javascript
const performanceMetrics = {
    responseTime: new Map(),
    embeddingTime: new Map(),
    storageOperations: new Map(),
    
    track(operation, duration) {
        const metrics = this.responseTime.get(operation) || {
            count: 0,
            totalTime: 0,
            avgTime: 0
        };
        metrics.count++;
        metrics.totalTime += duration;
        metrics.avgTime = metrics.totalTime / metrics.count;
        this.responseTime.set(operation, metrics);
    }
};
```

## Integration with Monitoring Services

### Prometheus Integration
```javascript
import prometheus from 'prom-client';

const memoryGauge = new prometheus.Gauge({
    name: 'semem_memory_usage_bytes',
    help: 'Memory usage of SeMeM'
});

const responseHistogram = new prometheus.Histogram({
    name: 'semem_response_time_seconds',
    help: 'Response time of memory operations'
});
```

### Health Checks
```javascript
class HealthCheck {
    constructor(memoryManager) {
        this.memoryManager = memoryManager;
    }

    async check() {
        try {
            const status = {
                storage: await this.checkStorage(),
                embeddings: await this.checkEmbeddings(),
                llm: await this.checkLLM(),
                timestamp: new Date()
            };
            return status;
        } catch (error) {
            logger.error('Health check failed:', error);
            throw error;
        }
    }

    async checkStorage() {
        const startTime = Date.now();
        await this.memoryManager.storage.loadHistory();
        return {
            status: 'ok',
            latency: Date.now() - startTime
        };
    }
}
```

## Usage Monitoring

### Rate Limiting
```javascript
class RateLimiter {
    constructor(limit = 100, window = 60000) {
        this.limit = limit;
        this.window = window;
        this.requests = new Map();
    }

    async checkLimit(key) {
        const now = Date.now();
        const requests = this.requests.get(key) || [];
        const windowStart = now - this.window;
        
        // Clean old requests
        const recent = requests.filter(time => time > windowStart);
        this.requests.set(key, recent);
        
        return recent.length < this.limit;
    }
}
```

### Error Tracking
```javascript
class ErrorTracker {
    constructor() {
        this.errors = new Map();
    }

    track(error, context) {
        const key = error.message;
        const entry = this.errors.get(key) || {
            count: 0,
            firstSeen: new Date(),
            lastSeen: new Date(),
            contexts: new Set()
        };
        
        entry.count++;
        entry.lastSeen = new Date();
        entry.contexts.add(JSON.stringify(context));
        
        this.errors.set(key, entry);
        logger.error('Error tracked:', { error, context });
    }
}
```

## Implementation Example
```javascript
import { MemoryManager } from './memoryManager.js';

const manager = new MemoryManager({...});
const monitor = new MemoryMonitor(manager);
const health = new HealthCheck(manager);
const errors = new ErrorTracker();

// Setup periodic monitoring
setInterval(() => {
    monitor.collectMetrics();
    health.check().catch(error => 
        errors.track(error, { component: 'health_check' }));
}, 60000);

// Track performance
monitor.on('metrics', async (stats) => {
    memoryGauge.set(stats.embeddingSize);
    
    try {
        const healthStatus = await health.check();
        logger.info('Health status:', healthStatus);
    } catch (error) {
        errors.track(error, { component: 'monitoring' });
    }
});
```

This monitoring system provides:
1. Memory usage tracking
2. Performance metrics
3. Health checks
4. Error tracking
5. Rate limiting
6. Prometheus integration

Q1: Would you like to see example Grafana dashboards?
Q2: Should I add alerting configuration?
Q3: Would you like to see logging best practices?
Q4: Should I include scaling metrics?

================
File: docs/2024-11-28/quickstart.md
================
# Quick Start Guide

## Basic Setup
```javascript
import { MemoryManager, JSONStorage } from 'semem';

// Initialize with minimal configuration
const manager = new MemoryManager({
    apiKey: process.env.OPENAI_API_KEY,
    storage: new JSONStorage('memory.json')
});

// Simple interaction
async function basicExample() {
    const prompt = "What's the weather like?";
    const response = await manager.generateResponse(prompt);
    console.log(response);
}

// Advanced interaction with memory retrieval
async function advancedExample() {
    const prompt = "Remember our discussion about AI?";
    
    // Get relevant past interactions
    const relevantMemories = await manager.retrieveRelevantInteractions(prompt);
    
    // Generate contextual response
    const response = await manager.generateResponse(prompt, [], relevantMemories);
    
    // Store interaction
    const embedding = await manager.getEmbedding(`${prompt} ${response}`);
    const concepts = await manager.extractConcepts(`${prompt} ${response}`);
    await manager.addInteraction(prompt, response, embedding, concepts);
}
```

## Using with Ollama
```javascript
const manager = new MemoryManager({
    chatModel: 'ollama',
    chatModelName: 'llama2',
    embeddingModel: 'ollama',
    embeddingModelName: 'nomic-embed-text'
});
```

## Using Remote Storage
```javascript
import { RemoteStorage } from 'semem';

const manager = new MemoryManager({
    apiKey: process.env.OPENAI_API_KEY,
    storage: new RemoteStorage({
        endpoint: 'https://api.example.com/memory',
        apiKey: process.env.STORAGE_API_KEY
    })
});
```

================
File: docs/2024-11-28/setup-config.md
================
# Setup and Configuration Guide

## Basic Configuration
```javascript
import { Config, MemoryManager } from 'semem';

const config = new Config({
    storage: {
        type: 'json',
        options: {
            path: 'data/memory.json'
        }
    },
    models: {
        chat: {
            provider: 'openai',
            model: 'gpt-4-turbo-preview'
        },
        embedding: {
            provider: 'openai',
            model: 'text-embedding-3-small'
        }
    },
    memory: {
        dimension: 1536,
        similarityThreshold: 40,
        contextWindow: 3,
        decayRate: 0.0001
    }
});
```

## Storage Configuration

### JSON Storage
```javascript
import { JSONStorage } from 'semem';

const storage = new JSONStorage('data/memory.json');
```

### Remote Storage
```javascript
import { RemoteStorage } from 'semem';

const storage = new RemoteStorage({
    endpoint: 'https://api.example.com/memory',
    apiKey: process.env.STORAGE_API_KEY,
    timeout: 5000,
    retries: 3
});
```

## Performance Tuning
```javascript
const config = new Config({
    memory: {
        // Reduce dimension for faster processing
        dimension: 1024,
        
        // Increase threshold for more precise matching
        similarityThreshold: 50,
        
        // Reduce window size for faster response
        contextWindow: 2,
        
        // Adjust decay rate for memory persistence
        decayRate: 0.0005
    }
});
```

## Error Handling
```javascript
try {
    const manager = new MemoryManager({
        apiKey: process.env.OPENAI_API_KEY,
        storage,
        onError: (error) => {
            console.error('Memory manager error:', error);
            // Implement error handling
        }
    });
} catch (error) {
    console.error('Initialization error:', error);
}
```

## Logging Configuration
```javascript
import { logger } from 'semem';

// Set log level
logger.level = process.env.NODE_ENV === 'production' ? 'info' : 'debug';

// Add custom logging
logger.on('error', (error) => {
    // Custom error handling
});
```

================
File: docs/2024-11-28/storage-docs.md
================
# Storage System Documentation

## BaseStorage (storage.js)
Abstract base class defining the storage interface. All storage implementations must extend this class.

### Methods
- `loadHistory()`: Async method that retrieves stored memory interactions
- `saveMemoryToHistory(memoryStore)`: Async method that persists current memory state

### Key Features
- Abstract interface ensuring consistency across implementations
- Async/await pattern for I/O operations
- Error handling requirements for implementations

## InMemoryStorage (inMemoryStorage.js)
RAM-based storage implementation for development and testing.

### Features
- Fast access and retrieval
- No persistence between restarts
- Ideal for testing and prototyping

### Implementation Details
- Uses JavaScript objects for storage
- Maintains separate short-term and long-term memory arrays
- Handles data structure conversions

## JSONStorage (jsonStorage.js)
File-based storage implementation using JSON format.

### Features
- Persistent storage between application restarts
- Human-readable storage format
- File-based backup capability

### Implementation Details
- Asynchronous file operations
- Automatic file creation if not exists
- Error handling for file operations
- JSON serialization/deserialization

## RemoteStorage (remoteStorage.js)
Network-based storage implementation for distributed systems.

### Features
- RESTful API integration
- Authentication support
- Timeout handling
- Retry logic

### Implementation Details
- HTTP/HTTPS protocols
- Bearer token authentication
- Configurable endpoints
- Network error handling

================
File: docs/2024-11-28/testing-docs.md
================
# Testing Documentation

## Unit Tests
Run with: `npm test`

### Storage Tests
```javascript
import { InMemoryStorage } from '../src/inMemoryStorage.js';

describe('InMemoryStorage', () => {
  it('should store and retrieve memories', async () => {
    const storage = new InMemoryStorage();
    // Test implementation
  });
});
```

### Integration Tests
```javascript
describe('MemoryManager Integration', () => {
  it('should generate and store responses', async () => {
    const manager = new MemoryManager({...});
    // Test implementation
  });
});
```

## Mocking
```javascript
jest.mock('@langchain/openai', () => ({
  ChatOpenAI: jest.fn()
}));
```

================
File: docs/2024-11-28/troubleshooting.md
================
# Troubleshooting Guide

## Common Issues

### Memory Usage
Problem: High memory consumption
```javascript
// Solution: Adjust memory settings
const config = new Config({
  memory: {
    dimension: 1024,  // Lower dimension
    contextWindow: 2  // Reduce context window
  }
});
```

### Storage Errors
Problem: Remote storage timeout
```javascript
// Solution: Configure timeout and retries
const storage = new RemoteStorage({
  timeout: 10000,
  retries: 3
});
```

### Model Errors
Problem: OpenAI API errors
```javascript
// Solution: Implement fallback
try {
  response = await manager.generateResponse(prompt);
} catch {
  response = await fallbackModel.generate(prompt);
}
```

## Debugging
Enable debug logging:
```javascript
logger.level = 'debug';
```

## Performance Optimization
- Use appropriate embedding dimensions
- Implement caching
- Optimize storage patterns

================
File: docs/2024-11-28/utils-docs.md
================
# Utilities Documentation

## Logger (utils.js)
Standardized logging system for application-wide use.

### Methods
- `info(message, ...args)`: Information level logging
- `error(message, ...args)`: Error level logging
- `debug(message, ...args)`: Debug level logging

### Features
- Consistent log formatting
- Multiple log levels
- Console output management

## Vector Operations (utils.js)
Mathematical utilities for vector operations.

### Functions
- `normalize(vector)`: Converts vector to unit length
- `cosineSimilarity(vec1, vec2)`: Calculates vector similarity

### Implementation Details
- Optimized vector calculations
- Numerical stability handling
- Array-based operations

================
File: docs/2024-12-29/ollama-rest-api.md
================
# API

## Endpoints

- [Generate a completion](#generate-a-completion)
- [Generate a chat completion](#generate-a-chat-completion)
- [Create a Model](#create-a-model)
- [List Local Models](#list-local-models)
- [Show Model Information](#show-model-information)
- [Copy a Model](#copy-a-model)
- [Delete a Model](#delete-a-model)
- [Pull a Model](#pull-a-model)
- [Push a Model](#push-a-model)
- [Generate Embeddings](#generate-embeddings)
- [List Running Models](#list-running-models)

## Conventions

### Model names

Model names follow a `model:tag` format, where `model` can have an optional namespace such as `example/model`. Some examples are `orca-mini:3b-q4_1` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

### Durations

All durations are returned in nanoseconds.

### Streaming responses

Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing `{"stream": false}` for these endpoints.

## Generate a completion

```shell
POST /api/generate
```

Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `prompt`: the prompt to generate a response for
- `suffix`: the text after the model response
- `images`: (optional) a list of base64-encoded images (for multimodal models such as `llava`)

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `system`: system message to (overrides what is defined in the `Modelfile`)
- `template`: the prompt template to use (overrides what is defined in the `Modelfile`)
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `raw`: if `true` no formatting will be applied to the prompt. You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)
- `context` (deprecated): the context parameter returned from a previous request to `/generate`, this can be used to keep a short conversational memory

#### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [structured outputs](#request-structured-outputs) example below.

#### JSON mode

Enable JSON mode by setting the `format` parameter to `json`. This will structure the response as a valid JSON object. See the JSON mode [example](#request-json-mode) below.

> [!IMPORTANT]
> It's important to instruct the model to use JSON in the `prompt`. Otherwise, the model may generate large amounts whitespace.

### Examples

#### Generate request (Streaming)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?"
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "response": "The",
  "done": false
}
```

The final response in the stream also includes additional data about the generation:

- `total_duration`: time spent generating the response
- `load_duration`: time spent in nanoseconds loading the model
- `prompt_eval_count`: number of tokens in the prompt
- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt
- `eval_count`: number of tokens in the response
- `eval_duration`: time in nanoseconds spent generating the response
- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory
- `response`: empty if the response was streamed, if not streamed, this will contain the full response

To calculate how fast the response is generated in tokens per second (token/s), divide `eval_count` / `eval_duration` * `10^9`.

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 10706818083,
  "load_duration": 6338219291,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 130079000,
  "eval_count": 259,
  "eval_duration": 4232710000
}
```

#### Request (No streaming)

##### Request

A response can be received in one reply when streaming is off.

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

##### Response

If `stream` is set to `false`, the response will be a single JSON object:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 5043500667,
  "load_duration": 5025959,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 325953000,
  "eval_count": 290,
  "eval_duration": 4709213000
}
```

#### Request (with suffix)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "codellama:code",
  "prompt": "def compute_gcd(a, b):",
  "suffix": "    return result",
  "options": {
    "temperature": 0
  },
  "stream": false
}'
```

##### Response

```json
{
  "model": "codellama:code",
  "created_at": "2024-07-22T20:47:51.147561Z",
  "response": "\n  if a == 0:\n    return b\n  else:\n    return compute_gcd(b % a, a)\n\ndef compute_lcm(a, b):\n  result = (a * b) / compute_gcd(a, b)\n",
  "done": true,
  "done_reason": "stop",
  "context": [...],
  "total_duration": 1162761250,
  "load_duration": 6683708,
  "prompt_eval_count": 17,
  "prompt_eval_duration": 201222000,
  "eval_count": 63,
  "eval_duration": 953997000
}
```

#### Request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{
  "model": "llama3.1:8b",
  "prompt": "Ollama is 22 years old and is busy saving the world. Respond using JSON",
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  }
}'
```

##### Response

```json
{
  "model": "llama3.1:8b",
  "created_at": "2024-12-06T00:48:09.983619Z",
  "response": "{\n  \"age\": 22,\n  \"available\": true\n}",
  "done": true,
  "done_reason": "stop",
  "context": [1, 2, 3],
  "total_duration": 1075509083,
  "load_duration": 567678166,
  "prompt_eval_count": 28,
  "prompt_eval_duration": 236000000,
  "eval_count": 16,
  "eval_duration": 269000000
}
```

#### Request (JSON mode)

> [!IMPORTANT]
> When `format` is set to `json`, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "What color is the sky at different times of the day? Respond using JSON",
  "format": "json",
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-11-09T21:07:55.186497Z",
  "response": "{\n\"morning\": {\n\"color\": \"blue\"\n},\n\"noon\": {\n\"color\": \"blue-gray\"\n},\n\"afternoon\": {\n\"color\": \"warm gray\"\n},\n\"evening\": {\n\"color\": \"orange\"\n}\n}\n",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4648158584,
  "load_duration": 4071084,
  "prompt_eval_count": 36,
  "prompt_eval_duration": 439038000,
  "eval_count": 180,
  "eval_duration": 4196918000
}
```

The value of `response` will be a string containing JSON similar to:

```json
{
  "morning": {
    "color": "blue"
  },
  "noon": {
    "color": "blue-gray"
  },
  "afternoon": {
    "color": "warm gray"
  },
  "evening": {
    "color": "orange"
  }
}
```

#### Request (with images)

To submit images to multimodal models such as `llava` or `bakllava`, provide a list of base64-encoded `images`:

#### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llava",
  "prompt":"What is in this picture?",
  "stream": false,
  "images": ["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"]
}'
```

#### Response

```
{
  "model": "llava",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": "A happy cartoon character, which is cute and cheerful.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 2938432250,
  "load_duration": 2559292,
  "prompt_eval_count": 1,
  "prompt_eval_duration": 2195557000,
  "eval_count": 44,
  "eval_duration": 736432000
}
```

#### Request (Raw Mode)

In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the `raw` parameter to disable templating. Also note that raw mode will not return a context.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "[INST] why is the sky blue? [/INST]",
  "raw": true,
  "stream": false
}'
```

#### Request (Reproducible outputs)

For reproducible outputs, set `seed` to a number:

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Why is the sky blue?",
  "options": {
    "seed": 123
  }
}'
```

##### Response

```json
{
  "model": "mistral",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": " The sky appears blue because of a phenomenon called Rayleigh scattering.",
  "done": true,
  "total_duration": 8493852375,
  "load_duration": 6589624375,
  "prompt_eval_count": 14,
  "prompt_eval_duration": 119039000,
  "eval_count": 110,
  "eval_duration": 1779061000
}
```

#### Generate request (With options)

If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the `options` parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false,
  "options": {
    "num_keep": 5,
    "seed": 42,
    "num_predict": 100,
    "top_k": 20,
    "top_p": 0.9,
    "min_p": 0.0,
    "typical_p": 0.7,
    "repeat_last_n": 33,
    "temperature": 0.8,
    "repeat_penalty": 1.2,
    "presence_penalty": 1.5,
    "frequency_penalty": 1.0,
    "mirostat": 1,
    "mirostat_tau": 0.8,
    "mirostat_eta": 0.6,
    "penalize_newline": true,
    "stop": ["\n", "user:"],
    "numa": false,
    "num_ctx": 1024,
    "num_batch": 2,
    "num_gpu": 1,
    "main_gpu": 0,
    "low_vram": false,
    "vocab_only": false,
    "use_mmap": true,
    "use_mlock": false,
    "num_thread": 8
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4935886791,
  "load_duration": 534986708,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 107345000,
  "eval_count": 237,
  "eval_duration": 4289432000
}
```

#### Load a model

If an empty prompt is provided, the model will be loaded into memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2"
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-18T19:52:07.071755Z",
  "response": "",
  "done": true
}
```

#### Unload a model

If an empty prompt is provided and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2024-09-12T03:54:03.516566Z",
  "response": "",
  "done": true,
  "done_reason": "unload"
}
```

## Generate a chat completion

```shell
POST /api/chat
```

Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using `"stream": false`. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `messages`: the messages of the chat, this can be used to keep a chat memory
- `tools`: tools for the model to use if supported. Requires `stream` to be set to `false`

The `message` object has the following fields:

- `role`: the role of the message, either `system`, `user`, `assistant`, or `tool`
- `content`: the content of the message
- `images` (optional): a list of images to include in the message (for multimodal models such as `llava`)
- `tool_calls` (optional): a list of tools the model wants to use

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema. 
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](#chat-request-structured-outputs) example below.

### Examples

#### Chat Request (Streaming)

##### Request

Send a chat message with a streaming response.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The",
    "images": null
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "done": true,
  "total_duration": 4883583458,
  "load_duration": 1334875,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 342546000,
  "eval_count": 282,
  "eval_duration": 4535599000
}
```

#### Chat request (No streaming)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ],
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability."}],
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  },
  "options": {
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.1",
  "created_at": "2024-12-06T00:46:58.265747Z",
  "message": { "role": "assistant", "content": "{\"age\": 22, \"available\": false}" },
  "done_reason": "stop",
  "done": true,
  "total_duration": 2254970291,
  "load_duration": 574751416,
  "prompt_eval_count": 34,
  "prompt_eval_duration": 1502000000,
  "eval_count": 12,
  "eval_duration": 175000000
}
```

#### Chat request (With History)

Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The"
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "done": true,
  "total_duration": 8113331500,
  "load_duration": 6396458,
  "prompt_eval_count": 61,
  "prompt_eval_duration": 398801000,
  "eval_count": 468,
  "eval_duration": 7701267000
}
```

#### Chat request (with images)

##### Request

Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llava",
  "messages": [
    {
      "role": "user",
      "content": "what is in this image?",
      "images": ["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"]
    }
  ]
}'
```

##### Response

```json
{
  "model": "llava",
  "created_at": "2023-12-13T22:42:50.203334Z",
  "message": {
    "role": "assistant",
    "content": " The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.",
    "images": null
  },
  "done": true,
  "total_duration": 1668506709,
  "load_duration": 1986209,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 359682000,
  "eval_count": 83,
  "eval_duration": 1303285000
}
```

#### Chat request (Reproducible outputs)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Hello!"
    }
  ],
  "options": {
    "seed": 101,
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (with tools)

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "What is the weather today in Paris?"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The location to get the weather for, e.g. San Francisco, CA"
            },
            "format": {
              "type": "string",
              "description": "The format to return the weather in, e.g. 'celsius' or 'fahrenheit'",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2024-07-22T20:33:28.123648Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_current_weather",
          "arguments": {
            "format": "celsius",
            "location": "Paris, FR"
          }
        }
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 885095291,
  "load_duration": 3753500,
  "prompt_eval_count": 122,
  "prompt_eval_duration": 328493000,
  "eval_count": 33,
  "eval_duration": 552222000
}
```

#### Load a model

If the messages array is empty, the model will be loaded into memory.

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": []
}'
```

##### Response
```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:17:29.110811Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "load",
  "done": true
}
```

#### Unload a model

If the messages array is empty and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [],
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:33:17.547535Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "unload",
  "done": true
}
```

## Create a Model

```shell
POST /api/create
```

Create a model from a [`Modelfile`](./modelfile.md). It is recommended to set `modelfile` to the content of the Modelfile rather than just set `path`. This is a requirement for remote create. Remote model creation must also create any file blobs, fields such as `FROM` and `ADAPTER`, explicitly with the server using [Create a Blob](#create-a-blob) and the value to the path indicated in the response.

### Parameters

- `model`: name of the model to create
- `modelfile` (optional): contents of the Modelfile
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects
- `path` (optional): path to the Modelfile
- `quantize` (optional): quantize a non-quantized (e.g. float16) model

#### Quantization types

| Type | Recommended |
| --- | :-: |
| q2_K | |
| q3_K_L | |
| q3_K_M | |
| q3_K_S | |
| q4_0 | |
| q4_1 | |
| q4_K_M | * |
| q4_K_S | |
| q5_0 | |
| q5_1 | |
| q5_K_M | |
| q5_K_S | |
| q6_K | |
| q8_0 | * |

### Examples

#### Create a new model

Create a new model from a `Modelfile`.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "mario",
  "modelfile": "FROM llama3\nSYSTEM You are mario from Super Mario Bros."
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"reading model metadata"}
{"status":"creating system layer"}
{"status":"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2"}
{"status":"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b"}
{"status":"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d"}
{"status":"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988"}
{"status":"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9"}
{"status":"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42"}
{"status":"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690"}
{"status":"writing manifest"}
{"status":"success"}
```

#### Quantize a model

Quantize a non-quantized model.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "llama3.1:quantized",
  "modelfile": "FROM llama3.1:8b-instruct-fp16",
  "quantize": "q4_K_M"
}'
```

##### Response

A stream of JSON objects is returned:

```
{"status":"quantizing F16 model to Q4_K_M"}
{"status":"creating new layer sha256:667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29"}
{"status":"using existing layer sha256:11ce4ee3e170f6adebac9a991c22e22ab3f8530e154ee669954c4bc73061c258"}
{"status":"using existing layer sha256:0ba8f0e314b4264dfd19df045cde9d4c394a52474bf92ed6a3de22a4ca31a177"}
{"status":"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb"}
{"status":"creating new layer sha256:455f34728c9b5dd3376378bfb809ee166c145b0b4c1f1a6feca069055066ef9a"}
{"status":"writing manifest"}
{"status":"success"}
```


### Check if a Blob Exists

```shell
HEAD /api/blobs/:digest
```

Ensures that the file blob used for a FROM or ADAPTER field exists on the server. This is checking your Ollama server and not ollama.com.

#### Query Parameters

- `digest`: the SHA256 digest of the blob

#### Examples

##### Request

```shell
curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

##### Response

Return 200 OK if the blob exists, 404 Not Found if it does not.

### Create a Blob

```shell
POST /api/blobs/:digest
```

Create a blob from a file on the server. Returns the server file path.

#### Query Parameters

- `digest`: the expected SHA256 digest of the file

#### Examples

##### Request

```shell
curl -T model.bin -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

##### Response

Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.

## List Local Models

```shell
GET /api/tags
```

List models that are available locally.

### Examples

#### Request

```shell
curl http://localhost:11434/api/tags
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "codellama:13b",
      "modified_at": "2023-11-04T14:56:49.277302595-07:00",
      "size": 7365960935,
      "digest": "9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697",
      "details": {
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "13B",
        "quantization_level": "Q4_0"
      }
    },
    {
      "name": "llama3:latest",
      "modified_at": "2023-12-07T09:32:18.757212583-08:00",
      "size": 3825819519,
      "digest": "fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e",
      "details": {
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

## Show Model Information

```shell
POST /api/show
```

Show information about a model including details, modelfile, template, parameters, license, system prompt.

### Parameters

- `model`: name of the model to show
- `verbose`: (optional) if set to `true`, returns full data for verbose response fields

### Examples

#### Request

```shell
curl http://localhost:11434/api/show -d '{
  "model": "llama3.2"
}'
```

#### Response

```json
{
  "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 4096\nPARAMETER stop \"\u003c/s\u003e\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
  "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"\nstop                           \"<|end_header_id|>\"\nstop                           \"<|eot_id|>\"",
  "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": [
      "llama"
    ],
    "parameter_size": "8.0B",
    "quantization_level": "Q4_0"
  },
  "model_info": {
    "general.architecture": "llama",
    "general.file_type": 2,
    "general.parameter_count": 8030261248,
    "general.quantization_version": 2,
    "llama.attention.head_count": 32,
    "llama.attention.head_count_kv": 8,
    "llama.attention.layer_norm_rms_epsilon": 0.00001,
    "llama.block_count": 32,
    "llama.context_length": 8192,
    "llama.embedding_length": 4096,
    "llama.feed_forward_length": 14336,
    "llama.rope.dimension_count": 128,
    "llama.rope.freq_base": 500000,
    "llama.vocab_size": 128256,
    "tokenizer.ggml.bos_token_id": 128000,
    "tokenizer.ggml.eos_token_id": 128009,
    "tokenizer.ggml.merges": [],            // populates if `verbose=true`
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.pre": "llama-bpe",
    "tokenizer.ggml.token_type": [],        // populates if `verbose=true`
    "tokenizer.ggml.tokens": []             // populates if `verbose=true`
  }
}
```

## Copy a Model

```shell
POST /api/copy
```

Copy a model. Creates a model with another name from an existing model.

### Examples

#### Request

```shell
curl http://localhost:11434/api/copy -d '{
  "source": "llama3.2",
  "destination": "llama3-backup"
}'
```

#### Response

Returns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.

## Delete a Model

```shell
DELETE /api/delete
```

Delete a model and its data.

### Parameters

- `model`: model name to delete

### Examples

#### Request

```shell
curl -X DELETE http://localhost:11434/api/delete -d '{
  "model": "llama3:13b"
}'
```

#### Response

Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.

## Pull a Model

```shell
POST /api/pull
```

Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.

### Parameters

- `model`: name of the model to pull
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/pull -d '{
  "model": "llama3.2"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

The first object is the manifest:

```json
{
  "status": "pulling manifest"
}
```

Then there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.

```json
{
  "status": "downloading digestname",
  "digest": "digestname",
  "total": 2142590208,
  "completed": 241970
}
```

After all the files are downloaded, the final responses are:

```json
{
    "status": "verifying sha256 digest"
}
{
    "status": "writing manifest"
}
{
    "status": "removing any unused layers"
}
{
    "status": "success"
}
```

if `stream` is set to false, then the response is a single JSON object:

```json
{
  "status": "success"
}
```

## Push a Model

```shell
POST /api/push
```

Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.

### Parameters

- `model`: name of the model to push in the form of `<namespace>/<model>:<tag>`
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/push -d '{
  "model": "mattw/pygmalion:latest"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

```json
{ "status": "retrieving manifest" }
```

and then:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Then there is a series of uploading responses:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Finally, when the upload is complete:

```json
{"status":"pushing manifest"}
{"status":"success"}
```

If `stream` is set to `false`, then the response is a single JSON object:

```json
{ "status": "success" }
```

## Generate Embeddings

```shell
POST /api/embed
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `input`: text or list of text to generate embeddings for

Advanced parameters:

- `truncate`: truncates the end of each input to fit within context length. Returns error if `false` and context length is exceeded. Defaults to `true`
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Examples

#### Request

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": "Why is the sky blue?"
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ]],
  "total_duration": 14143917,
  "load_duration": 1019500,
  "prompt_eval_count": 8
}
```

#### Request (Multiple input)

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": ["Why is the sky blue?", "Why is the grass green?"]
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ],[
    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,
    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481
  ]]
}
```

## List Running Models
```shell
GET /api/ps
```

List models that are currently loaded into memory.

#### Examples

### Request

```shell
curl http://localhost:11434/api/ps
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "mistral:latest",
      "model": "mistral:latest",
      "size": 5137025024,
      "digest": "2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "7.2B",
        "quantization_level": "Q4_0"
      },
      "expires_at": "2024-06-04T14:38:31.83753-07:00",
      "size_vram": 5137025024
    }
  ]
}
```

## Generate Embedding

> Note: this endpoint has been superseded by `/api/embed`

```shell
POST /api/embeddings
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `prompt`: text to generate embeddings for

Advanced parameters:

- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Examples

#### Request

```shell
curl http://localhost:11434/api/embeddings -d '{
  "model": "all-minilm",
  "prompt": "Here is an article about llamas..."
}'
```

#### Response

```json
{
  "embedding": [
    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,
    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281
  ]
}
```

================
File: docs/2025-01-01/fix-sparql-prompt.md
================
There is an issue in the code around SPARQL UPDATE calls, this error:

```
[INFO] Adding interaction: 'What's the current state of AI technology?'
[ERROR] SPARQL update error: Error: SPARQL update failed: 500
    at SPARQLStore._executeSparqlUpdate (file:///home/danny/github-danny/hyperdata/packages/semem/src/stores/SPARQLStore.js:55:23)
...
```

Away from the JS, these scripts work fine :

```
misc/scripts/sparql-auth-test.sh
misc/scripts/sparql-upload-test.sh
misc/scripts/ollama-embedding-test.sh
```

The following test (with helper) works fine as well :

```
spec/unit/sparql-endpoint-spec.js
src/utils/SPARQLHelpers.js
```

================
File: docs/2025-01-13/handover-api.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix doap: <http://usefulinc.com/ns/doap#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix prj: <http://example.org/project/> .

# Project Description
prj:SememAPI a doap:Project ;
    dc:title "Semem API Implementation"@en ;
    dc:description "API layer for Semantic Memory System with multiple access modes"@en ;
    dc:created "2025-01-13"^^xsd:date ;
    doap:programming-language "JavaScript" ;
    doap:repository [
        a doap:GitRepository ;
        doap:location <https://github.com/organization/semem> 
    ] .

# Core Components
prj:CoreComponents a prj:ComponentSet ;
    rdfs:label "Core API Components"@en ;
    prj:includes 
        prj:BaseAPI,
        prj:APIRegistry,
        prj:RDFParser,
        prj:RDFValidator .

prj:BaseAPI a prj:Component ;
    rdfs:label "Base API"@en ;
    dc:description "Abstract base interface for all API implementations"@en ;
    prj:sourceFile "src/api/common/BaseAPI.js" ;
    prj:features (
        "Lifecycle management"
        "Event emission"
        "Error handling"
        "Metric collection"
    ) .

prj:APIRegistry a prj:Component ;
    rdfs:label "API Registry"@en ;
    dc:description "Central registry for API service discovery"@en ;
    prj:sourceFile "src/api/common/APIRegistry.js" ;
    prj:features (
        "Service registration"
        "Instance management"
        "Configuration"
        "Metrics aggregation"
    ) .

# Access Modes
prj:AccessModes a prj:ComponentSet ;
    rdfs:label "Access Modes"@en ;
    prj:includes 
        prj:CLIHandler,
        prj:REPLHandler,
        prj:HTTPServer,
        prj:WebForms,
        prj:RDFParser .

prj:CLIHandler a prj:Component ;
    rdfs:label "CLI Handler"@en ;
    dc:description "Command line interface implementation"@en ;
    prj:sourceFile "src/api/cli/CLIHandler.js" ;
    prj:dependencies (
        "yargs"
        "chalk"
    ) ;
    prj:features (
        "Command parsing"
        "Colorized output"
        "History management"
    ) .

# Feature Sets
prj:FeatureSets a prj:ComponentSet ;
    rdfs:label "Feature Sets"@en ;
    prj:includes 
        prj:SelfieHandler,
        prj:PassiveHandler,
        prj:ActiveHandler .

prj:SelfieHandler a prj:Component ;
    rdfs:label "Selfie Handler"@en ;
    dc:description "System monitoring and metrics"@en ;
    prj:sourceFile "src/api/features/SelfieHandler.js" ;
    prj:features (
        "Metric collection"
        "Performance monitoring"
        "Error tracking"
        "OpenTelemetry integration"
    ) .

# Data Validation
prj:Validation a prj:ComponentSet ;
    rdfs:label "Data Validation"@en ;
    prj:includes 
        prj:RDFValidator,
        prj:CustomValidators .

prj:RDFValidator a prj:Component ;
    rdfs:label "RDF Validator"@en ;
    dc:description "RDF schema and SHACL validation"@en ;
    prj:sourceFile "src/api/common/RDFValidator.js" ;
    prj:features (
        "Schema validation"
        "SHACL support"
        "Error reporting"
        "Shape management"
    ) .

# Development Notes
prj:DevelopmentNotes a prj:Documentation ;
    rdfs:label "Development Notes"@en ;
    prj:shortTerm (
        "Complete WebSocket implementation"
        "Add visualization components"
        "Enhance RDF validation"
        "Improve error handling"
        "Add test coverage"
    ) ;
    prj:mediumTerm (
        "Add federation support"
        "Implement caching improvements"
        "Enhance monitoring"
        "Add backup systems"
    ) ;
    prj:longTerm (
        "Add graph visualization"
        "Implement distributed storage"
        "Add machine learning features"
    ) .

# Critical Notes
prj:CriticalNotes a prj:Documentation ;
    rdfs:label "Critical Implementation Notes"@en ;
    prj:notes (
        "Always use transactions for storage"
        "Monitor API rate limits"
        "Keep secret management secure"
        "Regular metric collection"
        "Proper error handling"
    ) .

# Dependencies
prj:Dependencies a prj:Requirements ;
    rdfs:label "Project Dependencies"@en ;
    prj:runtime (
        "Node.js 18+"
        "Express"
        "yargs"
        "chalk"
        "dotenv"
        "loglevel"
    ) ;
    prj:development (
        "Jasmine"
        "nodemon"
        "eslint"
    ) .

================
File: docs/2025-01-13/handover-api1.md
================
# Semem API Implementation Handover

## Overview
Implementation of an API layer for Semem (Semantic Memory) system with multiple access modes and comprehensive monitoring. Follows modular architecture with clear separation of concerns.

## Core Components

### Base API Layer
- `BaseAPI`: Abstract interface for API implementations
- `APIRegistry`: Central service discovery and management
- Event emission for monitoring
- Lifecycle management (initialize/shutdown)

### Access Modes
1. **Command Line Interface (CLI)**
   - Entry point: `src/api/cli/run.js`
   - Uses yargs for command parsing
   - Colorized output with chalk
   - Command history support

2. **REPL Environment**
   - Interactive shell with chat/RDF modes
   - Command completion
   - Help system
   - History management

3. **HTTP REST API**
   - Express-based server
   - OpenAPI documentation
   - Rate limiting
   - Compression and security middleware
   - CORS support

4. **Web Forms**
   - Static HTML/CSS/JS interface
   - No framework dependencies
   - Real-time API integration
   - Responsive design

5. **RDF DSL**
   - Custom semantic query language
   - SPARQL generation
   - Prefix management
   - Transaction support

## Feature Sets

### Selfie (Monitoring)
- Metric collection and aggregation
- OpenTelemetry integration
- Error tracking and reporting
- Storage metrics
- API performance monitoring

### Passive (Storage)
- SPARQL endpoint integration
- Caching layer
- Transaction support
- Batch operations
- Query federation

### Active (End-User)
- Chat interface
- Semantic search
- Memory retrieval
- Concept mapping
- Context management

## Data Validation
- RDF schema validation
- SHACL constraint support
- Custom validation functions
- Shape management
- Error reporting

## Configuration
- Environment-based config
- Secure secret management
- Override support
- Runtime reconfiguration

## Dependencies
- Node.js 18+
- Express for HTTP
- yargs for CLI
- chalk for terminal output
- dotenv for secrets
- loglevel for logging

## Testing
- Unit tests with Jasmine
- Integration tests for endpoints
- SPARQL testing utilities
- Mock data generators
- Performance testing

## Security
- API key authentication
- Rate limiting
- Input validation
- CORS configuration
- Error sanitization

## Future Development

### Short Term
1. Complete WebSocket implementation
2. Add visualization components
3. Enhance RDF validation
4. Improve error handling
5. Add more test coverage

### Medium Term
1. Add federation support
2. Implement caching improvements
3. Enhance monitoring
4. Add backup systems
5. Improve documentation

### Long Term
1. Add graph visualization
2. Implement distributed storage
3. Add machine learning features
4. Create management interface
5. Add workflow automation

## Critical Notes
1. Always use transactions for storage operations
2. Monitor API rate limits
3. Keep secret management secure
4. Regular metric collection
5. Proper error handling

## Support
- Source: src/api/
- Tests: spec/
- Documentation: docs/
- Issues: GitHub repository

================
File: docs/artifacts_2024-12-29/context-manager.js
================
import { logger } from './utils.js';

export default class ContextManager {
    constructor(options = {}) {
        this.maxTokens = options.maxTokens || 8192;
        this.maxTimeWindow = options.maxTimeWindow || 24 * 60 * 60 * 1000;
        this.relevanceThreshold = options.relevanceThreshold || 0.7;
        this.maxContextSize = options.maxContextSize || 5;
        this.contextBuffer = [];

        this.windowManager = new ContextWindowManager({
            maxWindowSize: this.maxTokens,
            minWindowSize: Math.floor(this.maxTokens / 4),
            overlapRatio: options.overlapRatio || 0.1
        });
    }

    addToContext(interaction, similarity = 1.0) {
        this.contextBuffer.push({
            ...interaction,
            similarity,
            addedAt: Date.now()
        });


        if (this.contextBuffer.length > this.maxContextSize * 2) {
            this.pruneContext();
        }
    }

    pruneContext() {
        const now = Date.now();
        this.contextBuffer = this.contextBuffer
            .filter(item => {
                const age = now - item.addedAt;
                return age < this.maxTimeWindow && item.similarity >= this.relevanceThreshold;
            })
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, this.maxContextSize);
    }

    summarizeContext(interactions) {

        const groupedInteractions = {};

        for (const interaction of interactions) {
            const mainConcept = interaction.concepts?.[0] || 'general';
            if (!groupedInteractions[mainConcept]) {
                groupedInteractions[mainConcept] = [];
            }
            groupedInteractions[mainConcept].push(interaction);
        }


        const summaries = [];
        for (const [concept, group] of Object.entries(groupedInteractions)) {
            if (group.length === 1) {
                summaries.push(this.formatSingleInteraction(group[0]));
            } else {
                summaries.push(this.formatGroupSummary(concept, group));
            }
        }

        return summaries.join('\n\n');
    }

    formatSingleInteraction(interaction) {
        return `Q: ${interaction.prompt}\nA: ${interaction.output}`;
    }

    formatGroupSummary(concept, interactions) {
        const summary = `Topic: ${concept}\n` +
            interactions
                .slice(0, 3)
                .map(i => `- ${i.prompt} → ${i.output.substring(0, 50)}...`)
                .join('\n');
        return summary;
    }

    buildContext(currentPrompt, retrievals = [], recentInteractions = [], options = {}) {
        this.pruneContext();


        retrievals.forEach(retrieval => {
            this.addToContext(retrieval.interaction, retrieval.similarity);
        });


        recentInteractions.forEach(interaction => {
            this.addToContext(interaction, 0.9);
        });

        const contextParts = [];


        if (options.systemContext) {
            contextParts.push(`System Context: ${options.systemContext}`);
        }


        const historicalContext = this.summarizeContext(
            this.contextBuffer.slice(0, this.maxContextSize)
        );

        if (historicalContext) {
            contextParts.push('Relevant Context:', historicalContext);
        }

        const fullContext = contextParts.join('\n\n');


        if (this.windowManager.estimateTokens(fullContext) > this.maxTokens) {
            const windows = this.windowManager.processContext(fullContext);
            return this.windowManager.mergeOverlappingContent(windows);
        }

        return fullContext;
    }
}

================
File: docs/artifacts_2024-12-29/context-window.js
================
import { logger } from './utils.js';

export default class ContextWindowManager {
    constructor(options = {}) {
        this.minWindowSize = options.minWindowSize || 1024;
        this.maxWindowSize = options.maxWindowSize || 8192;
        this.overlapRatio = options.overlapRatio || 0.1;
        this.avgTokenLength = options.avgTokenLength || 4;
    }


    estimateTokens(text) {
        return Math.ceil(text.length / this.avgTokenLength);
    }


    calculateWindowSize(input) {
        const estimatedTokens = this.estimateTokens(input);


        let windowSize = Math.min(
            this.maxWindowSize,
            Math.max(
                this.minWindowSize,
                estimatedTokens * 1.2
            )
        );

        logger.debug(`Calculated window size: ${windowSize} for input length: ${input.length}`);
        return windowSize;
    }


    createWindows(text, windowSize) {
        const windows = [];
        const overlapSize = Math.floor(windowSize * this.overlapRatio);
        const stride = windowSize - overlapSize;

        let position = 0;
        while (position < text.length) {
            const window = {
                text: text.slice(position, position + windowSize),
                start: position,
                end: Math.min(position + windowSize, text.length)
            };

            windows.push(window);
            position += stride;

            if (position + windowSize >= text.length) {

                if (position < text.length) {
                    windows.push({
                        text: text.slice(position),
                        start: position,
                        end: text.length
                    });
                }
                break;
            }
        }

        return windows;
    }


    mergeOverlappingContent(windows) {
        if (windows.length === 0) return '';
        if (windows.length === 1) return windows[0].text;

        let merged = windows[0].text;
        for (let i = 1; i < windows.length; i++) {
            const overlap = this._findBestOverlap(
                merged.slice(-this.maxWindowSize),
                windows[i].text
            );
            merged += windows[i].text.slice(overlap);
        }

        return merged;
    }


    _findBestOverlap(end, start, minOverlap = 10) {

        for (let overlap = Math.min(end.length, start.length); overlap >= minOverlap; overlap--) {
            const endSlice = end.slice(-overlap);
            const startSlice = start.slice(0, overlap);

            if (endSlice === startSlice) {
                return overlap;
            }
        }

        return 0;
    }


    processContext(context, options = {}) {
        const windowSize = this.calculateWindowSize(context);
        const windows = this.createWindows(context, windowSize);

        logger.debug(`Created ${windows.length} windows with size ${windowSize}`);


        if (options.includeMetadata) {
            return windows.map(window => ({
                ...window,
                tokenEstimate: this.estimateTokens(window.text)
            }));
        }

        return windows;
    }
}

================
File: docs/artifacts_2024-12-29/memory-manager.js
================
import { v4 as uuidv4 } from 'uuid';
import MemoryStore from './memoryStore.js';
import InMemoryStorage from './inMemoryStorage.js';
import ContextManager from './contextManager.js';
import PromptTemplates from './promptTemplates.js';
import { logger } from './utils.js';

export default class MemoryManager {
    constructor({
        llmProvider,
        chatModel = 'llama2',
        embeddingModel = 'nomic-embed-text',
        storage = null,
        dimension = 1536,
        contextOptions = {
            maxTokens: embeddingModel === 'nomic-embed-text' ? 8192 : 4096
        }
    }) {
        this.llmProvider = llmProvider;
        this.chatModel = chatModel;
        this.embeddingModel = embeddingModel;
        this.dimension = dimension;


        this.memoryStore = new MemoryStore(this.dimension);
        this.storage = storage || new InMemoryStorage();
        this.contextManager = new ContextManager(contextOptions);

        this.initialize();
    }

    async initialize() {
        const [shortTerm, longTerm] = await this.storage.loadHistory();

        for (const interaction of shortTerm) {
            const embedding = this.standardizeEmbedding(interaction.embedding);
            interaction.embedding = embedding;
            this.memoryStore.addInteraction(interaction);
        }

        this.memoryStore.longTermMemory.push(...longTerm);
        this.memoryStore.clusterInteractions();

        logger.info(`Memory initialized with ${shortTerm.length} short-term and ${longTerm.length} long-term memories`);
    }

    standardizeEmbedding(embedding) {
        const current = embedding.length;
        if (current === this.dimension) return embedding;

        if (current < this.dimension) {
            return [...embedding, ...new Array(this.dimension - current).fill(0)];
        }
        return embedding.slice(0, this.dimension);
    }

    async getEmbedding(text) {
        logger.info('Generating embedding...');
        try {
            const embedding = await this.llmProvider.generateEmbedding(
                this.embeddingModel,
                text
            );
            return this.standardizeEmbedding(embedding);
        } catch (error) {
            logger.error('Error generating embedding:', error);
            throw error;
        }
    }

    async extractConcepts(text) {
        logger.info('Extracting concepts...');
        try {
            const prompt = PromptTemplates.formatConceptPrompt(this.chatModel, text);
            const response = await this.llmProvider.generateCompletion(
                this.chatModel,
                prompt,
                { temperature: 0.2 }
            );

            const match = response.match(/\[.*\]/);
            if (match) {
                const concepts = JSON.parse(match[0]);
                logger.info('Extracted concepts:', concepts);
                return concepts;
            }

            logger.info('No concepts extracted, returning empty array');
            return [];
        } catch (error) {
            logger.error('Error extracting concepts:', error);
            return [];
        }
    }

    async addInteraction(prompt, output, embedding, concepts) {
        const interaction = {
            id: uuidv4(),
            prompt,
            output,
            embedding,
            timestamp: Date.now(),
            accessCount: 1,
            concepts,
            decayFactor: 1.0
        };

        this.memoryStore.addInteraction(interaction);
        await this.storage.saveMemoryToHistory(this.memoryStore);
    }

    async retrieveRelevantInteractions(query, similarityThreshold = 40, excludeLastN = 0) {
        const queryEmbedding = await this.getEmbedding(query);
        const queryConcepts = await this.extractConcepts(query);
        return this.memoryStore.retrieve(queryEmbedding, queryConcepts, similarityThreshold, excludeLastN);
    }

    async generateResponse(prompt, lastInteractions = [], retrievals = [], contextWindow = 3) {
        const context = this.contextManager.buildContext(
            prompt,
            retrievals,
            lastInteractions,
            { systemContext: "You're a helpful assistant with memory of past interactions." }
        );

        try {
            const messages = PromptTemplates.formatChatPrompt(
                this.chatModel,
                "You're a helpful assistant with memory of past interactions.",
                context,
                prompt
            );

            const response = await this.llmProvider.generateChat(
                this.chatModel,
                messages,
                { temperature: 0.7 }
            );

            return response.trim();
        } catch (error) {
            logger.error('Error generating response:', error);
            throw error;
        }
    }
}

================
File: docs/artifacts_2024-12-29/ollama-api.js
================
import fetch from 'node-fetch';

export default class OllamaAPI {
    constructor(baseUrl = 'http://localhost:11434') {
        this.baseUrl = baseUrl;
    }

    async generateEmbedding(model, input) {
        const response = await fetch(`${this.baseUrl}/api/embeddings`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                prompt: input,
                options: {
                    num_ctx: 8192
                }
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.embedding;
    }

    async generateChat(model, messages, options = {}) {
        const response = await fetch(`${this.baseUrl}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                messages,
                stream: false,
                options
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.message.content;
    }

    async generateCompletion(model, prompt, options = {}) {
        const response = await fetch(`${this.baseUrl}/api/generate`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                prompt,
                stream: false,
                options
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.response;
    }
}

================
File: docs/artifacts_2024-12-29/ollama-example.js
================
import MemoryManager from './memoryManager.js';
import JSONStorage from './jsonStorage.js';
import Config from './config.js';
import OllamaAPI from './ollama-api.js';

async function main() {
    const config = new Config({
        storage: {
            type: 'json',
            options: {
                path: 'memory.json'
            }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'llama2'
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text'
            }
        }
    });

    const storage = new JSONStorage(config.get('storage.options.path'));
    const ollama = new OllamaAPI();

    const memoryManager = new MemoryManager({
        llmProvider: ollama,
        chatModel: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.model'),
        storage
    });

    const prompt = "What's the current state of AI technology?";

    const relevantInteractions = await memoryManager.retrieveRelevantInteractions(prompt);
    const response = await memoryManager.generateResponse(prompt, [], relevantInteractions);
    console.log('Response:', response);

    const embedding = await memoryManager.getEmbedding(`${prompt} ${response}`);
    const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
    await memoryManager.addInteraction(prompt, response, embedding, concepts);
}

main().catch(console.error);

================
File: docs/artifacts_2024-12-29/prompt-templates.js
================
export default class PromptTemplates {
    static templates = {
        'llama2': {
            chat: (system, context, query) => {
                const messages = [{
                    role: 'system',
                    content: system
                }];

                if (context) {
                    messages.push({
                        role: 'user',
                        content: context
                    });
                    messages.push({
                        role: 'assistant',
                        content: 'I understand the context provided. How can I help with your query?'
                    });
                }

                messages.push({
                    role: 'user',
                    content: query
                });

                return messages;
            },
            completion: (context, query) => {
                return `[INST] ${context ? `Context:\n${context}\n\n` : ''}Query: ${query} [/INST]`;
            },
            extractConcepts: (text) => {
                return `[INST] Extract key concepts from the following text and return them as a JSON array of strings only. Example: ["concept1", "concept2"]. Text: "${text}" [/INST]`;
            }
        },

        'mistral': {
            chat: (system, context, query) => {
                const messages = [{
                    role: 'system',
                    content: system
                }];

                if (context) {
                    messages.push({
                        role: 'user',
                        content: `Previous Context:\n${context}`
                    });
                    messages.push({
                        role: 'assistant',
                        content: 'Context received. What would you like to know?'
                    });
                }

                messages.push({
                    role: 'user',
                    content: query
                });

                return messages;
            },
            completion: (context, query) => {
                return `<s>[INST] ${context ? `${context}\n\n` : ''}${query} [/INST]`;
            },
            extractConcepts: (text) => {
                return `<s>[INST] Extract and return only a JSON array of key concepts from: "${text}" [/INST]`;
            }
        }
    };

    static getTemplateForModel(modelName) {
        // Handle model name variants
        const baseModel = modelName.split(':')[0].toLowerCase();
        const modelFamily = baseModel.replace(/[\d.]/g, ''); // Remove version numbers
        return this.templates[modelFamily] || this.templates['llama2'];
    }

    static formatChatPrompt(modelName, system, context, query) {
        const template = this.getTemplateForModel(modelName);
        return template.chat(system, context, query);
    }

    static formatCompletionPrompt(modelName, context, query) {
        const template = this.getTemplateForModel(modelName);
        return template.completion(context, query);
    }

    static formatConceptPrompt(modelName, text) {
        const template = this.getTemplateForModel(modelName);
        return template.extractConcepts(text);
    }

    static registerTemplate(modelName, template) {
        if (!template.chat || !template.completion || !template.extractConcepts) {
            throw new Error('Template must implement chat, completion, and extractConcepts methods');
        }
        this.templates[modelName.toLowerCase()] = template;
    }
}

================
File: docs/artifacts_2024-12-29/semem-docs.md
================
# Semem Documentation

## Overview
Semem is a semantic memory management system for LLMs, providing context-aware interactions using vector embeddings and concept graphs.

## Installation
```bash
npm install semem
```

## Basic Usage
```javascript
import { MemoryManager, JSONStorage } from 'semem';
import OllamaAPI from './ollama-api.js';

const ollama = new OllamaAPI();
const storage = new JSONStorage('memory.json');

const manager = new MemoryManager({
    llmProvider: ollama,
    chatModel: 'llama2',
    embeddingModel: 'nomic-embed-text',
    storage
});
```

## Components

### OllamaAPI
Wrapper for Ollama's REST API endpoints:
- Chat generation
- Embeddings generation (supports 8192 context window)
- Completions

### MemoryManager
Core component handling:
- Interaction storage
- Context management
- Response generation
- Concept extraction

### ContextWindowManager
Handles large context processing:
- Adaptive window sizing
- Overlap management
- Context merging

## Configuration

### Default Configuration
```javascript
const config = new Config({
    storage: {
        type: 'json',
        options: { 
            path: 'memory.json' 
        }
    },
    models: {
        chat: {
            provider: 'ollama',
            model: 'llama2'
        },
        embedding: {
            provider: 'ollama',
            model: 'nomic-embed-text'
        }
    },
    memory: {
        dimension: 1536,
        similarityThreshold: 40,
        contextWindow: 3,
        decayRate: 0.0001
    }
});
```

### Context Options
```javascript
const contextOptions = {
    maxTokens: 8192,  // For nomic-embed-text
    maxTimeWindow: 24 * 60 * 60 * 1000, // 24 hours
    overlapRatio: 0.1, // 10% window overlap
    minWindowSize: 1024
};
```

## Testing

### Unit Tests
Located in `spec/unit/`:
```javascript
// spec/unit/memoryManager.spec.js
describe('MemoryManager', () => {
    let manager;
    
    beforeEach(() => {
        manager = new MemoryManager({...});
    });

    it('should generate embeddings', async () => {
        const embedding = await manager.getEmbedding('test text');
        expect(embedding.length).toBe(1536);
    });
});
```

### Integration Tests
Located in `spec/integration/`:
```javascript
// spec/integration/ollama.spec.js
describe('OllamaAPI Integration', () => {
    let api;
    
    beforeEach(() => {
        api = new OllamaAPI();
    });

    it('should generate embeddings', async () => {
        const embedding = await api.generateEmbedding(
            'nomic-embed-text',
            'Test text'
        );
        expect(embedding.length).toBe(1536);
    });
});
```

### Running Tests
```bash
npm test                 # Run all tests
npm test:unit           # Run unit tests only
npm test:integration    # Run integration tests only
```

## Error Handling

### API Errors
```javascript
try {
    const response = await manager.generateResponse(prompt);
} catch (error) {
    if (error.name === 'OllamaAPIError') {
        // Handle API-specific errors
    } else if (error.name === 'StorageError') {
        // Handle storage-related errors
    }
}
```

### Context Handling
```javascript
// Automatically handles context window limits
const response = await manager.generateResponse(prompt, [], retrievals);
```

## Storage Options

### JSON Storage
```javascript
import { JSONStorage } from 'semem';
const storage = new JSONStorage('memory.json');
```

### Remote Storage
```javascript
import { RemoteStorage } from 'semem';
const storage = new RemoteStorage({
    endpoint: 'https://api.example.com/memory',
    apiKey: 'your-api-key'
});
```

### Custom Storage
```javascript
import { BaseStorage } from 'semem';

class CustomStorage extends BaseStorage {
    async loadHistory() {
        // Implementation
    }
    
    async saveMemoryToHistory(memoryStore) {
        // Implementation
    }
}
```

## Next Steps

1. Test Implementation
   - Create test fixtures
   - Add unit tests for core components
   - Add integration tests
   - Add performance tests

2. Additional Features
   - Token counting implementation
   - Custom tokenization support
   - Priority-based window selection
   - Performance optimizations

3. Storage Extensions
   - Redis implementation
   - MongoDB implementation
   - GraphDB support

4. Documentation
   - API reference
   - Performance guidelines
   - Deployment guide
   - Security considerations

================
File: docs/artifacts_2024-12-29/semem-rdf.txt
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix sem: <http://example.org/semem#> .
@prefix doap: <http://usefulinc.com/ns/doap#> .

# Project Description
sem:Semem a doap:Project ;
    doap:name "Semem" ;
    doap:description "Semantic Memory Management System for LLMs" ;
    doap:programming-language "JavaScript" ;
    doap:homepage <https://github.com/danja/semem> ;
    doap:license <http://opensource.org/licenses/MIT> .

# Core Classes
sem:MemoryManager a rdfs:Class ;
    rdfs:label "Memory Manager"@en ;
    rdfs:comment "Core component managing memory operations and LLM interactions"@en ;
    rdfs:subClassOf sem:Component .

sem:OllamaAPI a rdfs:Class ;
    rdfs:label "Ollama API Wrapper"@en ;
    rdfs:comment "REST API wrapper for Ollama LLM service"@en ;
    rdfs:subClassOf sem:Component .

sem:ContextWindowManager a rdfs:Class ;
    rdfs:label "Context Window Manager"@en ;
    rdfs:comment "Manages context windows and overlap for large texts"@en ;
    rdfs:subClassOf sem:Component .

# Properties
sem:hasEmbeddingDimension a rdf:Property ;
    rdfs:domain sem:MemoryManager ;
    rdfs:range xsd:integer ;
    rdfs:comment "Dimension of embedding vectors"@en .

sem:hasSimilarityThreshold a rdf:Property ;
    rdfs:domain sem:MemoryManager ;
    rdfs:range xsd:float ;
    rdfs:comment "Threshold for memory similarity matching"@en .

sem:hasContext a rdf:Property ;
    rdfs:domain sem:Memory ;
    rdfs:range xsd:string ;
    rdfs:comment "Associated context for a memory"@en .

# Memory Types
sem:Memory a rdfs:Class ;
    rdfs:label "Memory"@en .

sem:ShortTermMemory a rdfs:Class ;
    rdfs:subClassOf sem:Memory ;
    rdfs:label "Short-term Memory"@en .

sem:LongTermMemory a rdfs:Class ;
    rdfs:subClassOf sem:Memory ;
    rdfs:label "Long-term Memory"@en .

# Testing Components
sem:TestSuite a rdfs:Class ;
    rdfs:label "Test Suite"@en .

sem:UnitTest a rdfs:Class ;
    rdfs:subClassOf sem:TestSuite ;
    rdfs:label "Unit Test"@en .

sem:IntegrationTest a rdfs:Class ;
    rdfs:subClassOf sem:TestSuite ;
    rdfs:label "Integration Test"@en .

# Test Cases
sem:MemoryManagerTest a sem:UnitTest ;
    rdfs:label "Memory Manager Test"@en ;
    sem:testsClass sem:MemoryManager .

sem:OllamaAPITest a sem:IntegrationTest ;
    rdfs:label "Ollama API Test"@en ;
    sem:testsClass sem:OllamaAPI .

sem:ContextWindowTest a sem:UnitTest ;
    rdfs:label "Context Window Test"@en ;
    sem:testsClass sem:ContextWindowManager .

================
File: docs/artifacts_2024-12-29/test-suite.js
================
import MemoryManager from '../../src/memoryManager.js';
import { MockOllamaAPI } from '../mocks/ollama.js';
import InMemoryStorage from '../../src/inMemoryStorage.js';

describe('MemoryManager', () => {
    let manager;
    let mockOllama;

    beforeEach(() => {
        mockOllama = new MockOllamaAPI();
        manager = new MemoryManager({
            llmProvider: mockOllama,
            chatModel: 'llama2',
            embeddingModel: 'nomic-embed-text',
            storage: new InMemoryStorage()
        });
    });

    it('should generate embeddings', async () => {
        const embedding = await manager.getEmbedding('test text');
        expect(embedding.length).toBe(1536);
        expect(Array.isArray(embedding)).toBe(true);
    });

    it('should extract concepts', async () => {
        const concepts = await manager.extractConcepts('AI and machine learning');
        expect(Array.isArray(concepts)).toBe(true);
        expect(concepts.length).toBeGreaterThan(0);
    });

    it('should add and retrieve interactions', async () => {
        const prompt = 'test prompt';
        const response = 'test response';
        const embedding = new Array(1536).fill(0);
        const concepts = ['test'];

        await manager.addInteraction(prompt, response, embedding, concepts);
        const retrievals = await manager.retrieveRelevantInteractions(prompt);

        expect(retrievals.length).toBeGreaterThan(0);
        expect(retrievals[0].interaction.prompt).toBe(prompt);
    });
});


import ContextWindowManager from '../../src/contextWindow.js';

describe('ContextWindowManager', () => {
    let windowManager;

    beforeEach(() => {
        windowManager = new ContextWindowManager({
            maxWindowSize: 1000,
            minWindowSize: 250,
            overlapRatio: 0.1
        });
    });

    it('should calculate correct window size', () => {
        const size = windowManager.calculateWindowSize('x'.repeat(1000));
        expect(size).toBeLessThanOrEqual(1000);
        expect(size).toBeGreaterThanOrEqual(250);
    });

    it('should create overlapping windows', () => {
        const text = 'x'.repeat(2000);
        const windows = windowManager.createWindows(text, 1000);
        expect(windows.length).toBeGreaterThan(1);
        expect(windows[0].text.length).toBeLessThanOrEqual(1000);
    });

    it('should merge overlapping content', () => {
        const windows = [
            { text: 'Hello world' },
            { text: 'world and universe' }
        ];
        const merged = windowManager.mergeOverlappingContent(windows);
        expect(merged).toBe('Hello world and universe');
    });
});


import OllamaAPI from './ollama-api.js';

describe('OllamaAPI Integration', () => {
    let api;

    beforeEach(() => {
        api = new OllamaAPI('http://localhost:11434');
    });

    it('should generate chat response', async () => {
        const messages = [{
            role: 'user',
            content: 'Hello, how are you?'
        }];

        const response = await api.generateChat('llama2', messages);
        expect(typeof response).toBe('string');
        expect(response.length).toBeGreaterThan(0);
    });

    it('should generate embeddings', async () => {
        const embedding = await api.generateEmbedding(
            'nomic-embed-text',
            'Test text for embedding'
        );

        expect(Array.isArray(embedding)).toBe(true);
        expect(embedding.length).toBe(1536);
    });

    it('should handle API errors gracefully', async () => {
        try {
            await api.generateChat('nonexistent-model', []);
            fail('Should have thrown an error');
        } catch (error) {
            expect(error.message).toContain('Ollama API error');
        }
    });
});


export class MockOllamaAPI {
    async generateEmbedding(model, input) {
        return new Array(1536).fill(0).map(() => Math.random());
    }

    async generateChat(model, messages) {
        return 'Mock response';
    }

    async generateCompletion(model, prompt) {
        return 'Mock completion';
    }
}

================
File: docs/description_2024-12-30/semem-architecture.mermaid
================
flowchart TB
    subgraph Client
        MA[Memory Access Layer]
        Config[Config Manager]
    end

    subgraph Core
        MM[Memory Manager]
        CM[Context Manager]
        CW[Context Window Manager]
        PT[Prompt Templates]
    end

    subgraph Storage
        IS[InMemory Store]
        JS[JSON Store]
        SS[SPARQL Store]
        CS[Cached SPARQL Store]
    end

    subgraph External
        OC[Ollama Connector]
        VE[Vector Embeddings]
        DB[(SPARQL Endpoint)]
    end

    MA --> MM
    Config --> MM
    MM --> CM
    MM --> CW
    MM --> PT
    MM --> |Store Selection| Storage
    IS --> MA
    JS --> MA
    SS --> MA
    CS --> SS
    SS --> DB
    MM --> OC
    OC --> VE

================
File: docs/description_2024-12-30/semem-description.md
================
# Current Semem Capabilities

Semem is a semantic memory system that currently provides:

1. **Memory Management**
   - Vector embeddings generation via Ollama (nomic-embed-text model)
   - Concept extraction from text
   - Similarity-based memory retrieval
   - Short-term and long-term memory segregation

2. **Storage Options**
   - In-memory storage
   - JSON file persistence
   - SPARQL endpoint integration
   - Caching layer for SPARQL operations

3. **LLM Integration**
   - Ollama API support for chat and embeddings
   - Configurable model selection
   - Context window management
   - Prompt templating system

4. **Data Structures**
   - Interaction storage with embeddings
   - Concept mapping
   - Decay factors for memory relevance
   - Transaction support for SPARQL operations

The system can currently:
- Generate embeddings for text input
- Store and retrieve memories with vector similarity
- Handle conversation context
- Persist data across multiple storage backends
- Integrate with SPARQL endpoints for semantic data storage

Key limitations:
- SPARQL integration needs authentication fixes
- Vector similarity search needs Faiss implementation completion
- Limited to basic interaction patterns
- No visualization components yet

================
File: docs/description_2024-12-30/semem-next-steps.md
================
# Recommended Next Steps for Semem

## Immediate Priorities
1. Fix SPARQL authentication and update operations
2. Complete Faiss integration for vector similarity search
3. Add proper error handling and recovery mechanisms
4. Implement comprehensive test coverage

## Short-term Enhancements
1. Add visualization components using Cytoscape.js
2. Implement UMAP for dimensionality reduction
3. Develop RDF-Ext integration for graph visualization
4. Create proper documentation and usage examples

## Medium-term Features
1. Implement integration with external knowledge bases (e.g., Wikidata)
2. Add support for other LLM providers
3. Develop graph-based reasoning capabilities
4. Create a RESTful API interface

## Long-term Goals
1. Implement agent communication protocols
2. Add distributed storage capabilities
3. Develop advanced reasoning with EYE integration
4. Create web-based management interface

================
File: docs/description_2025-01-01/architecture.md
================
# Semem Architecture

## Core Components

### Memory Manager
The central component that orchestrates all memory operations. It handles:
- Interaction storage and retrieval
- Embedding generation and caching
- Concept extraction
- Memory classification

### Storage Layer
Implements a pluggable storage architecture with multiple backends:
- BaseStore: Abstract interface for storage implementations
- InMemoryStore: RAM-based storage for testing
- JSONStore: File-based persistent storage
- SPARQLStore: Semantic triple store integration
- CachedSPARQLStore: Performance-optimized SPARQL storage

### Context Management
Manages conversation context through:
- Window size calculation
- Content overlap handling
- Token counting
- Context pruning

### LLM Integration
Provides abstracted access to language models:
- OllamaConnector: Integration with local Ollama models
- Configurable model selection
- Prompt template management
- Embedding generation

### Memory Processing
Sophisticated memory handling through:
- Vector similarity search
- Semantic clustering
- Concept graph maintenance
- Decay and reinforcement mechanisms

## Data Flow
1. New interactions are processed for embedding generation
2. Concepts are extracted using LLM
3. Memory is stored with metadata
4. Retrieval combines embedding similarity and concept matching
5. Context is managed for optimal interaction

================
File: docs/description_2025-01-01/capabilities.md
================
# Semem Capabilities Overview

Semem is a semantic memory system designed for AI applications that provides persistent, queryable storage of conversations and interactions. It combines embedding-based similarity search with semantic understanding.

## Core Features

### Memory Management
- Short-term and long-term memory storage
- Automatic memory classification and decay
- Concept extraction from interactions
- Semantic clustering of related memories
- Context window management for large conversations

### AI Integration
- Supports multiple LLM providers (Ollama, OpenAI)
- Embedding generation for semantic search
- Configurable models for chat and embeddings
- Prompt template management for different models

### Storage Options
- In-memory storage for testing/development
- JSON file-based persistent storage
- SPARQL-based semantic triple store
- Cached SPARQL store with automatic cleanup

### Advanced Features
- Transaction support with rollback capability 
- Backup and recovery mechanisms
- Federation across multiple SPARQL endpoints
- Memory clustering and concept relationships
- Automatic decay and reinforcement of memories

## Configuration
The system is highly configurable, supporting:
- Custom storage backends
- Multiple LLM providers
- Adjustable memory parameters
- SPARQL endpoint configuration
- Context window sizes

================
File: docs/description_2025-01-01/concept-system.md
================
# Concept System Architecture

The concept system in Semem builds a semantic network of related ideas extracted from interactions. This network enhances memory retrieval by understanding conceptual relationships.

## Concept Extraction
The system uses the LLM to extract key concepts through carefully crafted prompts that:
1. Identify main topics and themes
2. Extract entities and relationships
3. Recognize abstract concepts
4. Maintain consistency across extractions

For example, from a weather-related interaction, it might extract:
- weather conditions
- temperature
- location
- time period
- weather patterns

## Graph Building
The system maintains a weighted graph where:
- Nodes represent concepts
- Edges represent co-occurrence relationships
- Edge weights indicate relationship strength
- Node centrality reflects concept importance

Each time concepts are extracted:
1. New concepts become nodes
2. Co-occurring concepts get connected
3. Existing relationships are strengthened
4. Graph metrics are updated

## Spreading Activation
During memory retrieval, the system uses spreading activation to:
1. Start from query concepts
2. Activate connected concepts
3. Decay activation with distance
4. Combine with embedding similarity

This creates a rich semantic network that improves memory retrieval accuracy.

================
File: docs/description_2025-01-01/config-guide.md
================
# Semem Configuration Guide

## Basic Configuration
The configuration system uses a hierarchical structure with sensible defaults that can be overridden.

### Storage Configuration
```javascript
{
    storage: {
        type: 'json',  // 'json', 'memory', or 'sparql'
        options: {
            path: 'memory.json',  // For JSON storage
            // OR for SPARQL:
            graphName: 'http://example.org/memory',
            endpoint: 'http://localhost:4030'
        }
    }
}
```

### Model Configuration
```javascript
{
    models: {
        chat: {
            provider: 'ollama',  // 'ollama' or 'openai'
            model: 'llama2',
            options: {
                temperature: 0.7
            }
        },
        embedding: {
            provider: 'ollama',
            model: 'nomic-embed-text',
            options: {
                dimension: 1536
            }
        }
    }
}
```

### Memory Parameters
```javascript
{
    memory: {
        dimension: 1536,
        similarityThreshold: 40,
        contextWindow: 3,
        decayRate: 0.0001
    }
}
```

### SPARQL Endpoint Configuration
```javascript
{
    sparqlEndpoints: [{
        label: "main",
        user: "admin",
        password: "admin123",
        urlBase: "http://localhost:4030",
        query: "/query",
        update: "/update"
    }]
}
```

## Advanced Options
- Cache configuration for SPARQL store
- Transaction handling settings
- Context window parameters
- Backup and recovery settings

================
File: docs/description_2025-01-01/custom-storage.js
================
import BaseStore from './BaseStore.js';
import { logger } from '../Utils.js';

export default class CustomStore extends BaseStore {
    constructor(options = {}) {
        super();

        this.options = options;
        this.connected = false;
        this.inTransaction = false;
    }


    async loadHistory() {
        try {

            const shortTerm = await this.loadShortTermMemories();
            const longTerm = await this.loadLongTermMemories();


            return [shortTerm, longTerm];
        } catch (error) {
            logger.error('Error loading history:', error);
            throw error;
        }
    }


    async saveMemoryToHistory(memoryStore) {
        try {

            await this.beginTransaction();


            await this.saveMemories(
                memoryStore.shortTermMemory,
                'short-term'
            );


            await this.saveMemories(
                memoryStore.longTermMemory,
                'long-term'
            );


            await this.commitTransaction();
        } catch (error) {

            await this.rollbackTransaction();
            throw error;
        }
    }


    async beginTransaction() {
        if (this.inTransaction) {
            throw new Error('Transaction already in progress');
        }
        this.inTransaction = true;

    }

    async commitTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        this.inTransaction = false;
    }

    async rollbackTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        this.inTransaction = false;
    }


    async verify() {
        try {

            return true;
        } catch {
            return false;
        }
    }


    async close() {
        if (this.inTransaction) {
            await this.rollbackTransaction();
        }

    }
}

================
File: docs/description_2025-01-01/memory-dynamics.md
================
# Memory Dynamics in Semem

The memory system in Semem mimics human memory by implementing both decay and reinforcement mechanisms. This creates a dynamic system where frequently accessed, relevant memories remain readily available while less useful ones gradually fade.

## Decay Mechanism
Memories in Semem decay over time following an exponential decay function:

decayFactor = baseDecay * Math.exp(-decayRate * timeDiff)

Where:
- baseDecay starts at 1.0 for new memories
- decayRate is configurable (default 0.0001)
- timeDiff is the time since last access in seconds

This creates a natural forgetting curve where older memories become progressively less influential in retrieval unless reinforced.

## Reinforcement System
Every time a memory is accessed during retrieval:
1. Its accessCount increments
2. The timestamp updates to current time
3. The decayFactor increases by 10% (multiplied by 1.1)
4. A reinforcement boost is calculated as log(accessCount + 1)

This creates a rich-get-richer dynamic where useful memories become more likely to be retrieved again.

## Memory Classification
Memories that exceed an access threshold (default 10 accesses) get promoted to long-term memory. This creates two tiers:
- Short-term: Recent or infrequently accessed memories
- Long-term: Frequently accessed, well-established memories

The system maintains balance through regular cleanup cycles that assess and adjust memory status based on these dynamics.

================
File: docs/description_2025-01-01/memory-flow.mermaid
================
sequenceDiagram
    participant U as User
    participant MM as MemoryManager
    participant LLM as LLM Provider
    participant S as Storage

    U->>MM: New Interaction
    MM->>LLM: Generate Embedding
    MM->>LLM: Extract Concepts
    
    MM->>MM: Process Memory
    Note over MM: Classify Memory<br/>Update Concepts Graph<br/>Calculate Decay
    
    MM->>S: Store Memory
    
    U->>MM: Query Memory
    MM->>S: Retrieve Similar
    MM->>MM: Apply Context
    MM->>U: Return Response

================
File: docs/description_2025-01-01/retrieval-algorithm.md
================
# Memory Retrieval Algorithm

The retrieval system uses a sophisticated multi-stage approach:

1. **Vector Similarity**
   - Generates embedding for query
   - Performs cosine similarity comparison
   - Applies decay factor based on time
   - Considers access count reinforcement

2. **Concept Matching**
   - Extracts concepts from query
   - Activates related concepts in graph
   - Uses spreading activation for concept relationships
   - Combines with vector similarity scores

3. **Semantic Clustering**
   - Groups related memories
   - Maintains cluster centroids
   - Updates clusters dynamically
   - Provides fallback recommendations

4. **Context Building**
   - Selects most relevant memories
   - Manages context window size
   - Handles content overlap
   - Builds coherent context for LLM

The final relevance score is calculated as:
```
relevance = (similarity * decay * reinforcement) + conceptScore
```

Where:
- similarity: cosine similarity between embeddings
- decay: exponential decay based on time
- reinforcement: logarithmic function of access count
- conceptScore: spreading activation score from concept graph

================
File: docs/description_2025-01-01/sparql-details.md
================
# SPARQL Integration in Semem

The SPARQL integration in Semem provides a sophisticated semantic storage layer that enables rich querying and knowledge graph capabilities. The system uses a carefully designed RDF schema to represent memories and their relationships.

## Core Schema
Memories are stored using a custom vocabulary:
```turtle
@prefix mcp: <http://purl.org/stuff/mcp/>

mcp:Interaction 
    a rdfs:Class ;
    rdfs:label "Memory Interaction" .

mcp:embedding
    a rdf:Property ;
    rdfs:domain mcp:Interaction ;
    rdfs:range xsd:string .
```

## Transaction Management
The SPARQLStore implements ACID transactions through:
1. Automatic backup creation before transactions
2. Graph-level locking for concurrent access
3. Rollback capability using backup graphs
4. Transaction isolation through separate graph contexts

## Caching Layer
The CachedSPARQLStore extends functionality with:
1. In-memory query result caching
2. Automatic cache invalidation on updates
3. Time-based cache expiration
4. Size-limited LRU caching strategy

## Federation Support
The system supports federated queries across multiple endpoints, enabling:
1. Distributed memory storage
2. Cross-graph concept relationships
3. Metadata management in separate graphs
4. Scalable memory organization

================
File: docs/description_2025-01-01/system-overview.mermaid
================
graph TB
    User[User/Application] --> MM[Memory Manager]
    
    subgraph Core["Core Components"]
        MM --> CM[Context Manager]
        MM --> Vector[Vector Store]
        MM --> Concepts[Concept Extractor]
    end
    
    subgraph Storage["Storage Layer"]
        MM --> InMem[In-Memory Store]
        MM --> JSON[JSON Store]
        MM --> SPARQL[SPARQL Store]
    end
    
    subgraph AI["AI Integration"]
        MM --> Ollama[Ollama]
        MM --> OpenAI[OpenAI]
        Ollama --> Embed[Embedding Models]
        Ollama --> Chat[Chat Models]
        OpenAI --> Embed
        OpenAI --> Chat
    end

================
File: docs/description_2025-01-01/troubleshooting.md
================
# Troubleshooting Guide

## Common Issues and Solutions

### Embedding Generation
- Error: "Embedding dimension mismatch"
  - Verify model configuration
  - Check embedding model availability
  - Ensure consistent dimensions across storage

### Storage Issues
- SPARQL Connection Failures
  - Verify endpoint configuration
  - Check authentication credentials
  - Confirm graph exists and permissions

### Memory Management
- High Memory Usage
  - Adjust cache size settings
  - Enable automatic cleanup
  - Use appropriate storage backend

### Performance
- Slow Retrieval
  - Enable caching for SPARQL
  - Optimize similarity threshold
  - Adjust context window size

### Integration
- LLM Provider Issues
  - Verify Ollama/OpenAI setup
  - Check API credentials
  - Confirm model availability

## Debugging Steps
1. Enable debug logging
2. Check configuration
3. Verify storage health
4. Test LLM connectivity
5. Validate embeddings
6. Monitor memory usage

================
File: docs/description_2025-01-01/usage-example.js
================
import MemoryManager from './src/MemoryManager.js';
import JSONStore from './src/stores/JSONStore.js';
import OllamaConnector from './src/connectors/OllamaConnector.js';
import Config from './src/Config.js';

async function main() {

    const config = new Config({
        storage: {
            type: 'json',
            options: { path: 'memory.json' }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'llama2'
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text'
            }
        }
    });


    const storage = new JSONStore(config.get('storage.options.path'));
    const llmProvider = new OllamaConnector();


    const memoryManager = new MemoryManager({
        llmProvider,
        chatModel: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.model'),
        storage
    });

    try {

        const prompt = "What's the weather like today?";


        const relevantMemories = await memoryManager.retrieveRelevantInteractions(prompt);


        const response = await memoryManager.generateResponse(
            prompt,
            [],
            relevantMemories
        );


        const embedding = await memoryManager.generateEmbedding(`${prompt} ${response}`);
        const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
        await memoryManager.addInteraction(prompt, response, embedding, concepts);

        console.log('Response:', response);
    } catch (error) {
        console.error('Error:', error);
    } finally {

        await memoryManager.dispose();
    }
}

main().catch(console.error);

================
File: docs/description_latest/2025-01-13_api-req.md
================
# Provisional API Requirements

## JS API

Wherever possible access to functionality for the APIs will be through shared interfaces, library-style. These will in turn delegate operations to modules found in `src/connectors`, `src/stores` and `src/utils`. Versatility and ease of extension will be enabled by separation of concerns and modularity. Interface/class hierarchies and dependency injection along with other design patterns should be used as appropriate.
The loglevel lib will be used for logging.

#### Implementation Notes

Shared interfaces will be implemented by modules in the `src/api/common` dir.
Default configuration will be in `src/Config.js`. Values may be overriden via the API.
Any secrets will be managed using the dotenv lib.
Typescript type descriptions (for VSCode support) should be included.

## Access Modes

Semem will have five access modes :

1. Command-line interface
2. REPL
3. HTTP JSON
4. HTTP Forms
5. RDF Turtle DSL

### 1. Command-line interface

The CLI will operate in the style of common Unix-style command line tools. It will use the chalk lib to colorize outputs for improved clarity.

#### Implementation Notes

The entry point will be `src/api/cli/run.js`. This will use the yargs lib to parse commands and nothing more, delegating elsewhere for functionality.

### 2. REPL

This will operate like eg. Python's command line interactive mode. It should be user-friendly with help available.

#### Implementation Notes

The REPL will have different modes of operation, for example human language chat and explicit RDF statements.

### 3. HTTP JSON

The API functionality will be accessible through a HTTP interface using standard REST techniques.

#### Implementation Notes

This should include an OpenAPI schema.
The express lib should be used for HTTP services (HTTPS support will be the responsibility of any target host). Server code will appear under `src/api/http/server`

### 4. HTTP Forms

Interactions will be available through simple statically-served HTML, CSS and vanilla JS forms.

#### Implementation Notes

Server code will appear under `src/api/http/server`, client code under `src/api/http/client`

### 5. RDF Turtle DSL

This is not an immediate requirement but consideration should be given to how it might be supported.

## Interaction Feature Sets

To give some intuition as to the nature of interactions, they will be grouped as follows :

1. Selfie - self-analysis, monitoring and testing
2. Passive -
3. Active

### 1. Selfie (self-analysis, monitoring and testing)

- storage metrics
- performance metrics
- error reporting

#### Implementation Notes

Only a very minimal implementation is needed at this point, but it should be created in such a way to support OpenTelemetry specifications. (If relevant, note that during development of Semem, Jasmine and chai are used for unit & integration tests). The automatic posting of reports to a SPARQL store is requirement for a future phase.

### 2. Passive (functionality of units in isolation)

This should support things like posting data to, and querying the SPARQL store as well as direct chat with LLMs and semantic search of stored SPARQL data using embeddings.

### 3. Active (functionality of units in concert)

This will be the typical end-user style of interaction.

---

Remember to generate complete artifacts corresponding to files containing full source code and give their name and path. Use 'ThisCase.js' style file & ES class naming.

================
File: docs/description_latest/architecture.md
================
# Semem Architecture

## Core Components

### Memory Manager
The central component that orchestrates all memory operations. It handles:
- Interaction storage and retrieval
- Embedding generation and caching
- Concept extraction
- Memory classification

### Storage Layer
Implements a pluggable storage architecture with multiple backends:
- BaseStore: Abstract interface for storage implementations
- InMemoryStore: RAM-based storage for testing
- JSONStore: File-based persistent storage
- SPARQLStore: Semantic triple store integration
- CachedSPARQLStore: Performance-optimized SPARQL storage

### Context Management
Manages conversation context through:
- Window size calculation
- Content overlap handling
- Token counting
- Context pruning

### LLM Integration
Provides abstracted access to language models:
- OllamaConnector: Integration with local Ollama models
- Configurable model selection
- Prompt template management
- Embedding generation

### Memory Processing
Sophisticated memory handling through:
- Vector similarity search
- Semantic clustering
- Concept graph maintenance
- Decay and reinforcement mechanisms

## Data Flow
1. New interactions are processed for embedding generation
2. Concepts are extracted using LLM
3. Memory is stored with metadata
4. Retrieval combines embedding similarity and concept matching
5. Context is managed for optimal interaction

================
File: docs/description_latest/capabilities.md
================
# Semem Capabilities Overview

Semem is a semantic memory system designed for AI applications that provides persistent, queryable storage of conversations and interactions. It combines embedding-based similarity search with semantic understanding.

## Core Features

### Memory Management
- Short-term and long-term memory storage
- Automatic memory classification and decay
- Concept extraction from interactions
- Semantic clustering of related memories
- Context window management for large conversations

### AI Integration
- Supports multiple LLM providers (Ollama, OpenAI)
- Embedding generation for semantic search
- Configurable models for chat and embeddings
- Prompt template management for different models

### Storage Options
- In-memory storage for testing/development
- JSON file-based persistent storage
- SPARQL-based semantic triple store
- Cached SPARQL store with automatic cleanup

### Advanced Features
- Transaction support with rollback capability 
- Backup and recovery mechanisms
- Federation across multiple SPARQL endpoints
- Memory clustering and concept relationships
- Automatic decay and reinforcement of memories

## Configuration
The system is highly configurable, supporting:
- Custom storage backends
- Multiple LLM providers
- Adjustable memory parameters
- SPARQL endpoint configuration
- Context window sizes

================
File: docs/description_latest/concept-system.md
================
# Concept System Architecture

The concept system in Semem builds a semantic network of related ideas extracted from interactions. This network enhances memory retrieval by understanding conceptual relationships.

## Concept Extraction
The system uses the LLM to extract key concepts through carefully crafted prompts that:
1. Identify main topics and themes
2. Extract entities and relationships
3. Recognize abstract concepts
4. Maintain consistency across extractions

For example, from a weather-related interaction, it might extract:
- weather conditions
- temperature
- location
- time period
- weather patterns

## Graph Building
The system maintains a weighted graph where:
- Nodes represent concepts
- Edges represent co-occurrence relationships
- Edge weights indicate relationship strength
- Node centrality reflects concept importance

Each time concepts are extracted:
1. New concepts become nodes
2. Co-occurring concepts get connected
3. Existing relationships are strengthened
4. Graph metrics are updated

## Spreading Activation
During memory retrieval, the system uses spreading activation to:
1. Start from query concepts
2. Activate connected concepts
3. Decay activation with distance
4. Combine with embedding similarity

This creates a rich semantic network that improves memory retrieval accuracy.

================
File: docs/description_latest/config-guide.md
================
# Semem Configuration Guide

## Basic Configuration
The configuration system uses a hierarchical structure with sensible defaults that can be overridden.

### Storage Configuration
```javascript
{
    storage: {
        type: 'json',  // 'json', 'memory', or 'sparql'
        options: {
            path: 'memory.json',  // For JSON storage
            // OR for SPARQL:
            graphName: 'http://example.org/memory',
            endpoint: 'http://localhost:4030'
        }
    }
}
```

### Model Configuration
```javascript
{
    models: {
        chat: {
            provider: 'ollama',  // 'ollama' or 'openai'
            model: 'llama2',
            options: {
                temperature: 0.7
            }
        },
        embedding: {
            provider: 'ollama',
            model: 'nomic-embed-text',
            options: {
                dimension: 1536
            }
        }
    }
}
```

### Memory Parameters
```javascript
{
    memory: {
        dimension: 1536,
        similarityThreshold: 40,
        contextWindow: 3,
        decayRate: 0.0001
    }
}
```

### SPARQL Endpoint Configuration
```javascript
{
    sparqlEndpoints: [{
        label: "main",
        user: "admin",
        password: "admin123",
        urlBase: "http://localhost:4030",
        query: "/query",
        update: "/update"
    }]
}
```

## Advanced Options
- Cache configuration for SPARQL store
- Transaction handling settings
- Context window parameters
- Backup and recovery settings

================
File: docs/description_latest/custom-storage.js
================
import BaseStore from './BaseStore.js';
import { logger } from '../Utils.js';

export default class CustomStore extends BaseStore {
    constructor(options = {}) {
        super();

        this.options = options;
        this.connected = false;
        this.inTransaction = false;
    }


    async loadHistory() {
        try {

            const shortTerm = await this.loadShortTermMemories();
            const longTerm = await this.loadLongTermMemories();


            return [shortTerm, longTerm];
        } catch (error) {
            logger.error('Error loading history:', error);
            throw error;
        }
    }


    async saveMemoryToHistory(memoryStore) {
        try {

            await this.beginTransaction();


            await this.saveMemories(
                memoryStore.shortTermMemory,
                'short-term'
            );


            await this.saveMemories(
                memoryStore.longTermMemory,
                'long-term'
            );


            await this.commitTransaction();
        } catch (error) {

            await this.rollbackTransaction();
            throw error;
        }
    }


    async beginTransaction() {
        if (this.inTransaction) {
            throw new Error('Transaction already in progress');
        }
        this.inTransaction = true;

    }

    async commitTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        this.inTransaction = false;
    }

    async rollbackTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        this.inTransaction = false;
    }


    async verify() {
        try {

            return true;
        } catch {
            return false;
        }
    }


    async close() {
        if (this.inTransaction) {
            await this.rollbackTransaction();
        }

    }
}

================
File: docs/description_latest/handover-api.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix doap: <http://usefulinc.com/ns/doap#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix prj: <http://example.org/project/> .

# Project Description
prj:SememAPI a doap:Project ;
    dc:title "Semem API Implementation"@en ;
    dc:description "API layer for Semantic Memory System with multiple access modes"@en ;
    dc:created "2025-01-13"^^xsd:date ;
    doap:programming-language "JavaScript" ;
    doap:repository [
        a doap:GitRepository ;
        doap:location <https://github.com/organization/semem> 
    ] .

# Core Components
prj:CoreComponents a prj:ComponentSet ;
    rdfs:label "Core API Components"@en ;
    prj:includes 
        prj:BaseAPI,
        prj:APIRegistry,
        prj:RDFParser,
        prj:RDFValidator .

prj:BaseAPI a prj:Component ;
    rdfs:label "Base API"@en ;
    dc:description "Abstract base interface for all API implementations"@en ;
    prj:sourceFile "src/api/common/BaseAPI.js" ;
    prj:features (
        "Lifecycle management"
        "Event emission"
        "Error handling"
        "Metric collection"
    ) .

prj:APIRegistry a prj:Component ;
    rdfs:label "API Registry"@en ;
    dc:description "Central registry for API service discovery"@en ;
    prj:sourceFile "src/api/common/APIRegistry.js" ;
    prj:features (
        "Service registration"
        "Instance management"
        "Configuration"
        "Metrics aggregation"
    ) .

# Access Modes
prj:AccessModes a prj:ComponentSet ;
    rdfs:label "Access Modes"@en ;
    prj:includes 
        prj:CLIHandler,
        prj:REPLHandler,
        prj:HTTPServer,
        prj:WebForms,
        prj:RDFParser .

prj:CLIHandler a prj:Component ;
    rdfs:label "CLI Handler"@en ;
    dc:description "Command line interface implementation"@en ;
    prj:sourceFile "src/api/cli/CLIHandler.js" ;
    prj:dependencies (
        "yargs"
        "chalk"
    ) ;
    prj:features (
        "Command parsing"
        "Colorized output"
        "History management"
    ) .

# Feature Sets
prj:FeatureSets a prj:ComponentSet ;
    rdfs:label "Feature Sets"@en ;
    prj:includes 
        prj:SelfieHandler,
        prj:PassiveHandler,
        prj:ActiveHandler .

prj:SelfieHandler a prj:Component ;
    rdfs:label "Selfie Handler"@en ;
    dc:description "System monitoring and metrics"@en ;
    prj:sourceFile "src/api/features/SelfieHandler.js" ;
    prj:features (
        "Metric collection"
        "Performance monitoring"
        "Error tracking"
        "OpenTelemetry integration"
    ) .

# Data Validation
prj:Validation a prj:ComponentSet ;
    rdfs:label "Data Validation"@en ;
    prj:includes 
        prj:RDFValidator,
        prj:CustomValidators .

prj:RDFValidator a prj:Component ;
    rdfs:label "RDF Validator"@en ;
    dc:description "RDF schema and SHACL validation"@en ;
    prj:sourceFile "src/api/common/RDFValidator.js" ;
    prj:features (
        "Schema validation"
        "SHACL support"
        "Error reporting"
        "Shape management"
    ) .

# Development Notes
prj:DevelopmentNotes a prj:Documentation ;
    rdfs:label "Development Notes"@en ;
    prj:shortTerm (
        "Complete WebSocket implementation"
        "Add visualization components"
        "Enhance RDF validation"
        "Improve error handling"
        "Add test coverage"
    ) ;
    prj:mediumTerm (
        "Add federation support"
        "Implement caching improvements"
        "Enhance monitoring"
        "Add backup systems"
    ) ;
    prj:longTerm (
        "Add graph visualization"
        "Implement distributed storage"
        "Add machine learning features"
    ) .

# Critical Notes
prj:CriticalNotes a prj:Documentation ;
    rdfs:label "Critical Implementation Notes"@en ;
    prj:notes (
        "Always use transactions for storage"
        "Monitor API rate limits"
        "Keep secret management secure"
        "Regular metric collection"
        "Proper error handling"
    ) .

# Dependencies
prj:Dependencies a prj:Requirements ;
    rdfs:label "Project Dependencies"@en ;
    prj:runtime (
        "Node.js 18+"
        "Express"
        "yargs"
        "chalk"
        "dotenv"
        "loglevel"
    ) ;
    prj:development (
        "Jasmine"
        "nodemon"
        "eslint"
    ) .

================
File: docs/description_latest/handover-api1.md
================
# Semem API Implementation Handover

## Overview
Implementation of an API layer for Semem (Semantic Memory) system with multiple access modes and comprehensive monitoring. Follows modular architecture with clear separation of concerns.

## Core Components

### Base API Layer
- `BaseAPI`: Abstract interface for API implementations
- `APIRegistry`: Central service discovery and management
- Event emission for monitoring
- Lifecycle management (initialize/shutdown)

### Access Modes
1. **Command Line Interface (CLI)**
   - Entry point: `src/api/cli/run.js`
   - Uses yargs for command parsing
   - Colorized output with chalk
   - Command history support

2. **REPL Environment**
   - Interactive shell with chat/RDF modes
   - Command completion
   - Help system
   - History management

3. **HTTP REST API**
   - Express-based server
   - OpenAPI documentation
   - Rate limiting
   - Compression and security middleware
   - CORS support

4. **Web Forms**
   - Static HTML/CSS/JS interface
   - No framework dependencies
   - Real-time API integration
   - Responsive design

5. **RDF DSL**
   - Custom semantic query language
   - SPARQL generation
   - Prefix management
   - Transaction support

## Feature Sets

### Selfie (Monitoring)
- Metric collection and aggregation
- OpenTelemetry integration
- Error tracking and reporting
- Storage metrics
- API performance monitoring

### Passive (Storage)
- SPARQL endpoint integration
- Caching layer
- Transaction support
- Batch operations
- Query federation

### Active (End-User)
- Chat interface
- Semantic search
- Memory retrieval
- Concept mapping
- Context management

## Data Validation
- RDF schema validation
- SHACL constraint support
- Custom validation functions
- Shape management
- Error reporting

## Configuration
- Environment-based config
- Secure secret management
- Override support
- Runtime reconfiguration

## Dependencies
- Node.js 18+
- Express for HTTP
- yargs for CLI
- chalk for terminal output
- dotenv for secrets
- loglevel for logging

## Testing
- Unit tests with Jasmine
- Integration tests for endpoints
- SPARQL testing utilities
- Mock data generators
- Performance testing

## Security
- API key authentication
- Rate limiting
- Input validation
- CORS configuration
- Error sanitization

## Future Development

### Short Term
1. Complete WebSocket implementation
2. Add visualization components
3. Enhance RDF validation
4. Improve error handling
5. Add more test coverage

### Medium Term
1. Add federation support
2. Implement caching improvements
3. Enhance monitoring
4. Add backup systems
5. Improve documentation

### Long Term
1. Add graph visualization
2. Implement distributed storage
3. Add machine learning features
4. Create management interface
5. Add workflow automation

## Critical Notes
1. Always use transactions for storage operations
2. Monitor API rate limits
3. Keep secret management secure
4. Regular metric collection
5. Proper error handling

## Support
- Source: src/api/
- Tests: spec/
- Documentation: docs/
- Issues: GitHub repository

================
File: docs/description_latest/memory-dynamics.md
================
# Memory Dynamics in Semem

The memory system in Semem mimics human memory by implementing both decay and reinforcement mechanisms. This creates a dynamic system where frequently accessed, relevant memories remain readily available while less useful ones gradually fade.

## Decay Mechanism
Memories in Semem decay over time following an exponential decay function:

decayFactor = baseDecay * Math.exp(-decayRate * timeDiff)

Where:
- baseDecay starts at 1.0 for new memories
- decayRate is configurable (default 0.0001)
- timeDiff is the time since last access in seconds

This creates a natural forgetting curve where older memories become progressively less influential in retrieval unless reinforced.

## Reinforcement System
Every time a memory is accessed during retrieval:
1. Its accessCount increments
2. The timestamp updates to current time
3. The decayFactor increases by 10% (multiplied by 1.1)
4. A reinforcement boost is calculated as log(accessCount + 1)

This creates a rich-get-richer dynamic where useful memories become more likely to be retrieved again.

## Memory Classification
Memories that exceed an access threshold (default 10 accesses) get promoted to long-term memory. This creates two tiers:
- Short-term: Recent or infrequently accessed memories
- Long-term: Frequently accessed, well-established memories

The system maintains balance through regular cleanup cycles that assess and adjust memory status based on these dynamics.

================
File: docs/description_latest/memory-flow.mermaid
================
sequenceDiagram
    participant U as User
    participant MM as MemoryManager
    participant LLM as LLM Provider
    participant S as Storage

    U->>MM: New Interaction
    MM->>LLM: Generate Embedding
    MM->>LLM: Extract Concepts
    
    MM->>MM: Process Memory
    Note over MM: Classify Memory<br/>Update Concepts Graph<br/>Calculate Decay
    
    MM->>S: Store Memory
    
    U->>MM: Query Memory
    MM->>S: Retrieve Similar
    MM->>MM: Apply Context
    MM->>U: Return Response

================
File: docs/description_latest/retrieval-algorithm.md
================
# Memory Retrieval Algorithm

The retrieval system uses a sophisticated multi-stage approach:

1. **Vector Similarity**
   - Generates embedding for query
   - Performs cosine similarity comparison
   - Applies decay factor based on time
   - Considers access count reinforcement

2. **Concept Matching**
   - Extracts concepts from query
   - Activates related concepts in graph
   - Uses spreading activation for concept relationships
   - Combines with vector similarity scores

3. **Semantic Clustering**
   - Groups related memories
   - Maintains cluster centroids
   - Updates clusters dynamically
   - Provides fallback recommendations

4. **Context Building**
   - Selects most relevant memories
   - Manages context window size
   - Handles content overlap
   - Builds coherent context for LLM

The final relevance score is calculated as:
```
relevance = (similarity * decay * reinforcement) + conceptScore
```

Where:
- similarity: cosine similarity between embeddings
- decay: exponential decay based on time
- reinforcement: logarithmic function of access count
- conceptScore: spreading activation score from concept graph

================
File: docs/description_latest/sparql-details.md
================
# SPARQL Integration in Semem

The SPARQL integration in Semem provides a sophisticated semantic storage layer that enables rich querying and knowledge graph capabilities. The system uses a carefully designed RDF schema to represent memories and their relationships.

## Core Schema
Memories are stored using a custom vocabulary:
```turtle
@prefix mcp: <http://purl.org/stuff/mcp/>

mcp:Interaction 
    a rdfs:Class ;
    rdfs:label "Memory Interaction" .

mcp:embedding
    a rdf:Property ;
    rdfs:domain mcp:Interaction ;
    rdfs:range xsd:string .
```

## Transaction Management
The SPARQLStore implements ACID transactions through:
1. Automatic backup creation before transactions
2. Graph-level locking for concurrent access
3. Rollback capability using backup graphs
4. Transaction isolation through separate graph contexts

## Caching Layer
The CachedSPARQLStore extends functionality with:
1. In-memory query result caching
2. Automatic cache invalidation on updates
3. Time-based cache expiration
4. Size-limited LRU caching strategy

## Federation Support
The system supports federated queries across multiple endpoints, enabling:
1. Distributed memory storage
2. Cross-graph concept relationships
3. Metadata management in separate graphs
4. Scalable memory organization

================
File: docs/description_latest/system-overview.mermaid
================
graph TB
    User[User/Application] --> MM[Memory Manager]
    
    subgraph Core["Core Components"]
        MM --> CM[Context Manager]
        MM --> Vector[Vector Store]
        MM --> Concepts[Concept Extractor]
    end
    
    subgraph Storage["Storage Layer"]
        MM --> InMem[In-Memory Store]
        MM --> JSON[JSON Store]
        MM --> SPARQL[SPARQL Store]
    end
    
    subgraph AI["AI Integration"]
        MM --> Ollama[Ollama]
        MM --> OpenAI[OpenAI]
        Ollama --> Embed[Embedding Models]
        Ollama --> Chat[Chat Models]
        OpenAI --> Embed
        OpenAI --> Chat
    end

================
File: docs/description_latest/troubleshooting.md
================
# Troubleshooting Guide

## Common Issues and Solutions

### Embedding Generation
- Error: "Embedding dimension mismatch"
  - Verify model configuration
  - Check embedding model availability
  - Ensure consistent dimensions across storage

### Storage Issues
- SPARQL Connection Failures
  - Verify endpoint configuration
  - Check authentication credentials
  - Confirm graph exists and permissions

### Memory Management
- High Memory Usage
  - Adjust cache size settings
  - Enable automatic cleanup
  - Use appropriate storage backend

### Performance
- Slow Retrieval
  - Enable caching for SPARQL
  - Optimize similarity threshold
  - Adjust context window size

### Integration
- LLM Provider Issues
  - Verify Ollama/OpenAI setup
  - Check API credentials
  - Confirm model availability

## Debugging Steps
1. Enable debug logging
2. Check configuration
3. Verify storage health
4. Test LLM connectivity
5. Validate embeddings
6. Monitor memory usage

================
File: docs/description_latest/usage-example.js
================
import MemoryManager from './src/MemoryManager.js';
import JSONStore from './src/stores/JSONStore.js';
import OllamaConnector from './src/connectors/OllamaConnector.js';
import Config from './src/Config.js';

async function main() {

    const config = new Config({
        storage: {
            type: 'json',
            options: { path: 'memory.json' }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'llama2'
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text'
            }
        }
    });


    const storage = new JSONStore(config.get('storage.options.path'));
    const llmProvider = new OllamaConnector();


    const memoryManager = new MemoryManager({
        llmProvider,
        chatModel: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.model'),
        storage
    });

    try {

        const prompt = "What's the weather like today?";


        const relevantMemories = await memoryManager.retrieveRelevantInteractions(prompt);


        const response = await memoryManager.generateResponse(
            prompt,
            [],
            relevantMemories
        );


        const embedding = await memoryManager.generateEmbedding(`${prompt} ${response}`);
        const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
        await memoryManager.addInteraction(prompt, response, embedding, concepts);

        console.log('Response:', response);
    } catch (error) {
        console.error('Error:', error);
    } finally {

        await memoryManager.dispose();
    }
}

main().catch(console.error);

================
File: docs/prompts/2025-01-13_api-docs.md
================
Please create documentation for the API as described in `docs/description_latest/2025-01-13_api-req.md` and implemented in the files under `src/api`. For each of the five access modes there should be an individual artifact in markdown including at least an overview of features, a quick start with usage examples and a comprehensive list of features, with code examples as appropriate. A further artifact, an index page should give a short description of the five access modes.

================
File: docs/prompts/2025-01-13_api.md
================
Please review the requirements below and compare with the code in your project knowledge. Please implement the missing pieces.

Our project today is to create a set of APIs for Semem. Below are the provisional requirements for code. Tests and documentation will be covered later. The initial aim is to get draft code in place in a well-organised structure with all the necessary ES modules, classes and methods.
First please read the requirements and refer to the documentation and current source code in project knowledge to get an idea of the current functionality and think hard about what will required. Then create a list of modules that will be needed, and we will step through creating these as artifacts one-by-one.

## JS API

Wherever possible access to functionality for the APIs will be through shared interfaces, library-style. These will in turn delegate operations to modules found in `src/connectors`, `src/stores` and `src/utils`. Versatility and ease of extension will be enabled by separation of concerns and modularity. Interface/class hierarchies and dependency injection along with other design patterns should be used as appropriate.
The loglevel lib will be used for logging.

#### Implementation Notes

Shared interfaces will be implemented by modules in the `src/api/common` dir.
Default configuration will be in `src/Config.js`. Values may be overriden via the API.
Any secrets will be managed using the dotenv lib.
Typescript type descriptions (for VSCode support) should be included.

## Access Modes

Semem will have five access modes :

1. Command-line interface
2. REPL
3. HTTP JSON
4. HTTP Forms
5. RDF Turtle DSL

### 1. Command-line interface

The CLI will operate in the style of common Unix-style command line tools. It will use the chalk lib to colorize outputs for improved clarity.

#### Implementation Notes

The entry point will be `src/api/cli/run.js`. This will use the yargs lib to parse commands and nothing more, delegating elsewhere for functionality.

### 2. REPL

This will operate like eg. Python's command line interactive mode. It should be user-friendly with help available.

#### Implementation Notes

The REPL will have different modes of operation, for example human language chat and explicit RDF statements.

### 3. HTTP JSON

The API functionality will be accessible through a HTTP interface using standard REST techniques.

#### Implementation Notes

This should include an OpenAPI schema.
The express lib should be used for HTTP services (HTTPS support will be the responsibility of any target host). Server code will appear under `src/api/http/server`

### 4. HTTP Forms

Interactions will be available through simple statically-served HTML, CSS and vanilla JS forms.

#### Implementation Notes

Server code will appear under `src/api/http/server`, client code under `src/api/http/client`

### 5. RDF Turtle DSL

This is not an immediate requirement but consideration should be given to how it might be supported.

## Interaction Feature Sets

To give some intuition as to the nature of interactions, they will be grouped as follows :

1. Selfie - self-analysis, monitoring and testing
2. Passive -
3. Active

### 1. Selfie (self-analysis, monitoring and testing)

- storage metrics
- performance metrics
- error reporting

#### Implementation Notes

Only a very minimal implementation is needed at this point, but it should be created in such a way to support OpenTelemetry specifications. (If relevant, note that during development of Semem, Jasmine and chai are used for unit & integration tests). The automatic posting of reports to a SPARQL store is requirement for a future phase.

### 2. Passive (functionality of units in isolation)

This should support things like posting data to, and querying the SPARQL store as well as direct chat with LLMs and semantic search of stored SPARQL data using embeddings.

### 3. Active (functionality of units in concert)

This will be the typical end-user style of interaction.

---

Remember to generate complete artifacts corresponding to files containing full source code and give their name and path. Use 'ThisCase.js' style file & ES class naming.

================
File: docs/handover_2024-12-30.md
================
# SPARQL Storage Implementation Handover

## Overview
Implementation of persistent storage for the Memory Context Protocol (MCP) using SPARQL and RDF. The storage system uses named graphs for memory isolation and implements ACID transactions through graph backups.

## Core Components

### SPARQLStore
- Extends BaseStore for SPARQL endpoint interaction
- Uses named graphs for memory storage
- Implements transaction support through graph backups
- Supports both basic and federated queries

### CachedSPARQLStore
- Extends SPARQLStore with caching functionality
- Implements LRU cache with TTL for query results
- Handles cache invalidation during updates
- Includes automatic cache cleanup

## Data Model

### Core Vocabularies Used
- MCP (http://purl.org/stuff/mcp/): Core memory protocol concepts
- Data Cube (http://purl.org/linked-data/cube#): Statistical data structures
- SKOS (http://www.w3.org/2004/02/skos/core#): Concept relationships

### Graph Structure
- Main Memory Graph: Stores current memory state
- Backup Graph: Used for transaction management
- Metadata Graph: Stores version information and statistics

## Implementation Details

### Transaction Management
1. Begin Transaction: Creates backup graph copy
2. Operations: Performed on main graph
3. Commit: Removes backup graph
4. Rollback: Restores from backup graph

### Query Federation
- Cross-graph queries for metadata integration
- SERVICE keyword usage for endpoint federation
- Performance optimization through caching

### Backup Strategy
- Full graph copying for transactions
- Incremental updates tracking
- Corruption detection and recovery
- Concurrent access protection

## Testing

### Core Tests
- Basic storage operations
- Transaction management
- Query federation
- Cache performance

### Integration Tests
- Basic backup operations
- Advanced backup scenarios 
- Federation patterns
- Large dataset handling

## Configuration

### SPARQL Endpoints
- Query endpoint: /test/query
- Update endpoint: /test/update
- Graph store protocol: /test/get

### Authentication
- Basic auth supported
- Credentials in Config.js
- Connection pooling recommended

## Known Limitations
1. Large dataset performance requires caching
2. No built-in compression
3. Single endpoint backup only
4. Basic auth only

## Future Improvements
1. Backup compression
2. Multi-endpoint replication
3. Advanced authentication
4. Query optimization
5. Backup versioning
6. SHACL validation

## Critical Notes
- Always use transactions for updates
- Monitor backup graph cleanup
- Handle concurrent access
- Verify data integrity

## Dependencies
- Node.js fetch API
- SPARQL 1.1 endpoint
- Graph store protocol support

## Support
- Source: src/stores/SPARQLStore.js
- Tests: spec/integration/
- Documentation: docs/
- Issues: GitHub repository

================
File: docs/handover_2024-12-30.ttl
================
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix doap: <http://usefulinc.com/ns/doap#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix prj: <http://example.org/project/> .
@prefix mcp: <http://purl.org/stuff/mcp/> .
@prefix qb: <http://purl.org/linked-data/cube#> .

prj:SPARQLStore
    a doap:Project ;
    dc:title "SPARQL Storage Implementation"@en ;
    dc:description "Persistent storage implementation for Memory Context Protocol using SPARQL and RDF"@en ;
    dc:created "2024-12-30"^^xsd:date ;
    dc:status "Production"@en ;
    doap:programming-language "JavaScript" ;
    doap:repository [
        a doap:GitRepository ;
        doap:location <https://github.com/danja/semem>
    ] ;
    prj:mainFile "src/stores/SPARQLStore.js" ;
    prj:testFile "spec/integration/SPARQLStore.spec.js" .

prj:CoreComponents
    a prj:ComponentSet ;
    prj:includes prj:SPARQLStore, prj:CachedSPARQLStore ;
    prj:mainFeatures (
        "SPARQL endpoint interaction"
        "Named graph storage"
        "Transaction support"
        "Query federation"
        "Result caching"
    ) .

prj:DataModel
    a prj:ArchitectureComponent ;
    prj:usesVocabulary mcp:, qb:, skos: ;
    prj:graphTypes (
        [
            rdfs:label "Main Memory Graph" ;
            dc:description "Stores current memory state"
        ]
        [
            rdfs:label "Backup Graph" ;
            dc:description "Used for transaction management"
        ]
        [
            rdfs:label "Metadata Graph" ;
            dc:description "Stores version information and statistics"
        ]
    ) .

prj:TransactionManagement
    a prj:Feature ;
    rdfs:label "Transaction Management" ;
    prj:steps (
        "Create backup graph copy"
        "Perform operations on main graph"
        "Remove backup on commit"
        "Restore from backup on rollback"
    ) .

prj:QueryFederation
    a prj:Feature ;
    rdfs:label "Query Federation" ;
    prj:aspects (
        "Cross-graph queries"
        "SERVICE keyword usage"
        "Cache optimization"
    ) .

prj:TestingFramework
    a prj:TestSuite ;
    prj:includes [
        rdfs:label "Core Tests" ;
        prj:covers (
            "Storage operations"
            "Transactions"
            "Federation"
            "Caching"
        )
    ], [
        rdfs:label "Integration Tests" ;
        prj:covers (
            "Backup operations"
            "Advanced scenarios"
            "Federation patterns"
            "Large datasets"
        )
    ] .

prj:Configuration
    a prj:SystemConfig ;
    prj:endpoints [
        rdfs:label "SPARQL Endpoints" ;
        prj:queryEndpoint "/test/query" ;
        prj:updateEndpoint "/test/update" ;
        prj:gspEndpoint "/test/get"
    ] ;
    prj:authentication [
        a prj:AuthConfig ;
        rdfs:label "Basic Authentication" ;
        prj:configLocation "Config.js"
    ] .

prj:Limitations
    a prj:KnownIssues ;
    prj:issues (
        "Large dataset performance requires caching"
        "No built-in compression"
        "Single endpoint backup only"
        "Basic auth only"
    ) .

prj:FutureImprovements
    a prj:Roadmap ;
    prj:planned (
        "Backup compression"
        "Multi-endpoint replication"
        "Advanced authentication"
        "Query optimization"
        "Backup versioning"
        "SHACL validation"
    ) ;
    prj:priority "High" ;
    dc:date "2025-06"^^xsd:gYearMonth .

prj:CriticalNotes
    a prj:Documentation ;
    prj:notes (
        "Always use transactions for updates"
        "Monitor backup graph cleanup"
        "Handle concurrent access"
        "Verify data integrity"
    ) ;
    dc:importance "Critical" .

prj:Dependencies
    a prj:Requirements ;
    prj:requires (
        "Node.js fetch API"
        "SPARQL 1.1 endpoint"
        "Graph store protocol support"
    ) .

prj:Support
    a prj:Documentation ;
    prj:sourceCode "src/stores/SPARQLStore.js" ;
    prj:tests "spec/integration/" ;
    prj:documentation "docs/" ;
    prj:issueTracker <https://github.com/danja/semem/issues> .

================
File: docs/notes.md
================
/home/danny/github-danny/hyperdata/workspaces/hyperdata/articles/semem/plan.md

/home/danny/github-danny/hyperdata/workspaces/danny.ayers.name/entries/2024-11-18_semem.md

/home/danny/github-danny/hyperdata/workspaces/hyperdata/journal/2024-11-18.md

/home/danny/github-danny/hyperdata/workspaces/hyperdata/articles/semem/links.md

- semem is lightweight, componentized trustgraph

[Porting Python Memory Management Project to JavaScript](https://claude.ai/chat/0decba46-fb0b-4c13-a0b6-4fd645cd3113)

#:packer is related

Also look at how Aider does things - it's got a nice code repo format

The #:storm abstraction should be in here somewhere,

/home/danny/github-danny/hyperdata/workspaces/ns/articles/fs-abstraction.md

/home/danny/github-danny/hyperdata/packages/ns/docs/postcraft/content-raw/articles/fs-abstraction.md

================
File: docs/sparql-prompt_2024-12-30.md
================
I would like you to create `src/stores/SPARQLStore.js`. This will use a remote SPARQL for storage, following the pattern of `src/stores/JSONStore.js`. The SPARQL endpoint URLs for update and query together with user credentials can be found in Config.js
The core vocabularies to use in the queries should be mcp, datacube (qb) that you can find in project knowledge. skos is in there too, just in case you might need it. Feel free to use other vocabularies you know of it they seem justified. Please give me any code as complete individual full source file artifacts.

================
File: misc/scripts/ollama-embedding-test.sh
================
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "The sky is blue because of Rayleigh scattering"
}'

================
File: misc/scripts/sparql-auth-test.sh
================
curl -X POST \
  -H "Authorization: Basic $(echo -n 'invalid:credentials' | base64)" \
  -H "Content-Type: application/sparql-query" \
  -H "Accept: application/json" \
  --data 'SELECT * WHERE { ?s ?p ?o } LIMIT 1' \
  'http://localhost:4030/test/query'

================
File: misc/scripts/sparql-upload-test.sh
================
curl -X POST \
  -H "Authorization: Basic $(echo -n 'admin:admin123' | base64)" \
  -H "Content-Type: text/turtle" \
  --data-binary '@-' \
  'http://localhost:4030/test/data?graph=http://example.org/test-graph' << 'EOF'
@prefix ex: <http://example.org/> .
ex:subject ex:predicate "test value" .
EOF

================
File: src/api/cli/about.md
================
# Command line Interfaces

================
File: src/api/cli/CLIHandler.js
================
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import chalk from 'chalk';
import log from 'loglevel';
import { APIRegistry } from '../common/APIRegistry.js';
import BaseAPI from '../common/BaseAPI.js';

export default class CLIHandler extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.registry = new APIRegistry();
        this.setupCommands();
    }

    setupCommands() {
        this.yargs = yargs(hideBin(process.argv))
            .command('chat', 'Chat with the system', {
                prompt: {
                    alias: 'p',
                    type: 'string',
                    demandOption: true,
                    describe: 'Input prompt'
                },
                model: {
                    alias: 'm',
                    type: 'string',
                    default: 'qwen2:1.5b',
                    describe: 'Model to use'
                }
            })
            .command('store', 'Store data', {
                data: {
                    alias: 'd',
                    type: 'string',
                    demandOption: true,
                    describe: 'Data to store'
                },
                format: {
                    alias: 'f',
                    choices: ['text', 'turtle'],
                    default: 'text'
                }
            })
            .command('query', 'Query stored data', {
                query: {
                    alias: 'q',
                    type: 'string',
                    demandOption: true,
                    describe: 'Search query'
                },
                limit: {
                    alias: 'l',
                    type: 'number',
                    default: 10
                }
            })
            .command('metrics', 'Show system metrics', {
                format: {
                    choices: ['text', 'json'],
                    default: 'text'
                }
            })
            .option('color', {
                type: 'boolean',
                default: true,
                describe: 'Colorize output'
            })
            .option('verbose', {
                alias: 'v',
                type: 'boolean',
                describe: 'Run with verbose logging'
            })
            .help()
            .alias('h', 'help');
    }

    async initialize() {
        await super.initialize();


        if (this.yargs.argv.verbose) {
            log.setLevel('debug');
        }

        process.on('SIGINT', async () => {
            await this.shutdown();
            process.exit(0);
        });
    }

    async executeOperation(command, args) {
        try {
            switch (command) {
                case 'chat':
                    return this.handleChat(args);
                case 'store':
                    return this.handleStore(args);
                case 'query':
                    return this.handleQuery(args);
                case 'metrics':
                    return this.handleMetrics(args);
                default:
                    throw new Error(`Unknown command: ${command}`);
            }
        } catch (error) {
            this.logger.error('Operation failed:', error);
            this.formatOutput({
                success: false,
                error: error.message
            }, args);
        }
    }

    async handleChat({ prompt, model }) {
        const api = this.registry.get('chat');
        const response = await api.executeOperation('chat', {
            prompt,
            model
        });

        return this.formatOutput({
            success: true,
            data: response
        });
    }

    async handleStore({ data, format }) {
        const api = this.registry.get('storage');
        const stored = await api.storeInteraction({
            content: data,
            format,
            timestamp: Date.now()
        });

        return this.formatOutput({
            success: true,
            data: stored
        });
    }

    async handleQuery({ query, limit }) {
        const api = this.registry.get('storage');
        const results = await api.retrieveInteractions({
            text: query,
            limit
        });

        return this.formatOutput({
            success: true,
            data: results
        });
    }

    async handleMetrics({ format }) {
        const metrics = await this.getMetrics();
        return this.formatOutput({
            success: true,
            data: metrics
        }, { format });
    }

    formatOutput(result, { format = 'text', color = true } = {}) {
        const c = color ? chalk : (text => text);

        if (format === 'json') {
            return console.log(JSON.stringify(result, null, 2));
        }

        if (!result.success) {
            return console.error(c.red(`Error: ${result.error}`));
        }

        if (Array.isArray(result.data)) {
            result.data.forEach(item => {
                console.log(c.cyan('---'));
                Object.entries(item).forEach(([key, value]) => {
                    console.log(c.yellow(`${key}:`), value);
                });
            });
            return;
        }

        if (typeof result.data === 'object') {
            Object.entries(result.data).forEach(([key, value]) => {
                console.log(c.yellow(`${key}:`), value);
            });
            return;
        }

        console.log(result.data);
    }

    async run() {
        await this.initialize();
        const argv = await this.yargs.argv;
        const command = argv._[0];

        if (!command) {
            this.yargs.showHelp();
            process.exit(1);
        }

        await this.executeOperation(command, argv);
    }
}

================
File: src/api/common/APIRegistry.js
================
import log from 'loglevel';
import BaseAPI from './BaseAPI.js';





export default class APIRegistry {
    constructor() {
        if (APIRegistry.instance) {
            return APIRegistry.instance;
        }
        APIRegistry.instance = this;

        this.apis = new Map();
        this.logger = log.getLogger('APIRegistry');
        this.metrics = new Map();
    }







    async register(name, apiClass, config = {}) {
        if (this.apis.has(name)) {
            throw new Error(`API ${name} already registered`);
        }

        if (!(apiClass.prototype instanceof BaseAPI)) {
            throw new Error('API must extend BaseAPI');
        }

        try {
            const api = new apiClass(config);
            await api.initialize();


            api.on('metric', (metric) => {
                this.metrics.set(`${name}.${metric.name}`, {
                    value: metric.value,
                    timestamp: metric.timestamp
                });
            });

            this.apis.set(name, api);
            this.logger.info(`Registered API: ${name}`);

            return api;
        } catch (error) {
            this.logger.error(`Failed to register API ${name}:`, error);
            throw error;
        }
    }






    get(name) {
        const api = this.apis.get(name);
        if (!api) {
            throw new Error(`API ${name} not found`);
        }
        return api;
    }





    async unregister(name) {
        const api = this.apis.get(name);
        if (api) {
            await api.shutdown();
            this.apis.delete(name);
            this.logger.info(`Unregistered API: ${name}`);
        }
    }





    getAll() {
        return new Map(this.apis);
    }





    getMetrics() {
        return {
            timestamp: Date.now(),
            apiCount: this.apis.size,
            apis: Object.fromEntries(
                Array.from(this.apis.entries()).map(([name, api]) => [
                    name,
                    {
                        status: api.initialized ? 'active' : 'inactive',
                        metrics: Object.fromEntries(
                            Array.from(this.metrics.entries())
                                .filter(([key]) => key.startsWith(name))
                                .map(([key, value]) => [
                                    key.split('.')[1],
                                    value
                                ])
                        )
                    }
                ])
            )
        };
    }




    async shutdownAll() {
        const shutdowns = Array.from(this.apis.entries()).map(
            async ([name, api]) => {
                try {
                    await this.unregister(name);
                } catch (error) {
                    this.logger.error(`Error shutting down ${name}:`, error);
                }
            }
        );
        await Promise.all(shutdowns);
    }
}

================
File: src/api/common/BaseAPI.js
================
import log from 'loglevel';
import { EventEmitter } from 'events';





export default class BaseAPI extends EventEmitter {
    constructor(config = {}) {
        super();
        this.config = config;
        this.logger = log.getLogger(this.constructor.name);
        this.initialized = false;
    }





    async initialize() {
        if (this.initialized) {
            throw new Error('API already initialized');
        }
        this.initialized = true;
    }





    async shutdown() {
        if (!this.initialized) {
            throw new Error('API not initialized');
        }
        this.initialized = false;
    }







    async executeOperation(operation, params) {
        throw new Error('executeOperation must be implemented');
    }






    async storeInteraction(interaction) {
        throw new Error('storeInteraction must be implemented');
    }






    async retrieveInteractions(query) {
        throw new Error('retrieveInteractions must be implemented');
    }





    async getMetrics() {
        return {
            timestamp: Date.now(),
            status: this.initialized ? 'active' : 'inactive',
            memoryUsage: process.memoryUsage(),
            uptime: process.uptime()
        };
    }





    _validateParams(params, schema) {

        if (!params || typeof params !== 'object') {
            throw new Error('Invalid parameters');
        }
    }





    _emitMetric(name, value) {
        this.emit('metric', { name, value, timestamp: Date.now() });
    }
}

================
File: src/api/common/CustomValidators.js
================
export default class CustomValidators {
    constructor() {
        this.validators = new Map();
        this.registerBuiltins();
    }

    registerBuiltins() {

        this.register('uri', {
            validate: (value) => {
                try {
                    new URL(value);
                    return { valid: true };
                } catch {
                    return {
                        valid: false,
                        message: 'Invalid URI format'
                    };
                }
            }
        });

        this.register('language', {
            validate: (value) => {
                const langPattern = /^[a-zA-Z]{2,3}(-[a-zA-Z]{2,4})?$/;
                return {
                    valid: langPattern.test(value),
                    message: langPattern.test(value) ? null : 'Invalid language tag'
                };
            }
        });


        this.register('concept', {
            validate: (value, options = {}) => {
                if (!value.startsWith(options.namespace || 'http://')) {
                    return {
                        valid: false,
                        message: 'Concept URI must use correct namespace'
                    };
                }
                return { valid: true };
            }
        });


        this.register('timerange', {
            validate: (value, options = {}) => {
                const { start, end } = value;
                const startDate = new Date(start);
                const endDate = new Date(end);

                if (isNaN(startDate.getTime()) || isNaN(endDate.getTime())) {
                    return {
                        valid: false,
                        message: 'Invalid date format'
                    };
                }

                if (startDate > endDate) {
                    return {
                        valid: false,
                        message: 'Start date must be before end date'
                    };
                }

                return { valid: true };
            }
        });
    }






    register(name, validator) {
        if (typeof validator.validate !== 'function') {
            throw new Error('Validator must have a validate function');
        }

        this.validators.set(name, {
            ...validator,
            async: validator.validate.constructor.name === 'AsyncFunction'
        });
    }





    registerBatch(validators) {
        for (const [name, validator] of Object.entries(validators)) {
            this.register(name, validator);
        }
    }





    get(name) {
        const validator = this.validators.get(name);
        if (!validator) {
            throw new Error(`Validator not found: ${name}`);
        }
        return validator;
    }







    async execute(name, value, options = {}) {
        const validator = this.get(name);
        try {
            const result = await validator.validate(value, options);
            return {
                valid: result.valid,
                message: result.message,
                validator: name,
                value
            };
        } catch (error) {
            return {
                valid: false,
                message: error.message,
                validator: name,
                value,
                error
            };
        }
    }





    compose(validators) {
        return {
            validate: async (value, options = {}) => {
                const results = [];
                for (const validator of validators) {
                    const name = typeof validator === 'string' ?
                        validator : validator.name;
                    const validatorOptions = typeof validator === 'string' ?
                        options : { ...options, ...validator.options };

                    const result = await this.execute(name, value, validatorOptions);
                    results.push(result);

                    if (!result.valid) break;
                }

                const valid = results.every(r => r.valid);
                return {
                    valid,
                    results,
                    message: valid ? null : results.find(r => !r.valid)?.message
                };
            }
        };
    }






    conditional(condition, validator) {
        return {
            validate: async (value, options = {}) => {
                if (!await condition(value, options)) {
                    return { valid: true };
                }

                const name = typeof validator === 'string' ?
                    validator : validator.name;
                const validatorOptions = typeof validator === 'string' ?
                    options : { ...options, ...validator.options };

                return this.execute(name, value, validatorOptions);
            }
        };
    }






    recursive(validator, options = {}) {
        return {
            validate: async (value, validatorOptions = {}) => {
                const results = [];
                const maxDepth = options.maxDepth || 10;

                const validateNode = async (node, depth = 0) => {
                    if (depth > maxDepth) {
                        throw new Error('Maximum recursion depth exceeded');
                    }


                    const result = await this.execute(
                        typeof validator === 'string' ? validator : validator.name,
                        node,
                        typeof validator === 'string' ?
                            validatorOptions :
                            { ...validatorOptions, ...validator.options }
                    );
                    results.push(result);


                    if (node && typeof node === 'object') {
                        for (const child of Object.values(node)) {
                            if (child && typeof child === 'object') {
                                await validateNode(child, depth + 1);
                            }
                        }
                    }
                };

                await validateNode(value);

                const valid = results.every(r => r.valid);
                return {
                    valid,
                    results,
                    message: valid ? null : results.find(r => !r.valid)?.message
                };
            }
        };
    }
}

================
File: src/api/common/RDFParser.js
================
import { APIRegistry } from './APIRegistry.js';
import { SPARQLHelpers } from '../../utils/SPARQLHelpers.js';

export default class RDFParser {
    constructor(config = {}) {
        this.registry = new APIRegistry();
        this.prefixes = {
            mcp: 'http://purl.org/stuff/mcp/',
            rdf: 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
            rdfs: 'http://www.w3.org/2000/01/rdf-schema#',
            xsd: 'http://www.w3.org/2001/XMLSchema#',
            qb: 'http://purl.org/linked-data/cube#',
            skos: 'http://www.w3.org/2004/02/skos/core#',
            ...config.prefixes
        };
    }

    parse(input) {
        const lines = input.trim().split('\n');
        const commands = [];
        let currentCommand = '';

        for (const line of lines) {
            const trimmed = line.trim();
            if (!trimmed || trimmed.startsWith('#')) continue;

            if (trimmed.endsWith(';')) {
                currentCommand += ' ' + trimmed.slice(0, -1);
                commands.push(currentCommand.trim());
                currentCommand = '';
            } else {
                currentCommand += ' ' + trimmed;
            }
        }

        if (currentCommand) {
            commands.push(currentCommand.trim());
        }

        return commands.map(cmd => this.parseCommand(cmd));
    }

    parseCommand(command) {
        const tokens = command.split(' ');
        const action = tokens[0].toLowerCase();

        switch (action) {
            case 'store':
                return this.parseStoreCommand(tokens.slice(1));
            case 'query':
                return this.parseQueryCommand(tokens.slice(1));
            case 'update':
                return this.parseUpdateCommand(tokens.slice(1));
            case 'define':
                return this.parseDefineCommand(tokens.slice(1));
            default:
                throw new Error(`Unknown command: ${action}`);
        }
    }

    parseStoreCommand(tokens) {
        const options = this.parseOptions(tokens);
        const data = options.data || tokens.join(' ');

        return {
            type: 'store',
            data: this.expandPrefixes(data),
            format: options.format || 'turtle',
            graph: options.graph
        };
    }

    parseQueryCommand(tokens) {
        const options = this.parseOptions(tokens);
        let query = options.query || tokens.join(' ');


        if (!query.toLowerCase().startsWith('select') &&
            !query.toLowerCase().startsWith('ask') &&
            !query.toLowerCase().startsWith('construct')) {
            query = this.buildSimpleQuery(query, options);
        }

        return {
            type: 'query',
            query: this.expandPrefixes(query),
            format: options.format || 'json'
        };
    }

    parseUpdateCommand(tokens) {
        const options = this.parseOptions(tokens);
        let update = options.update || tokens.join(' ');


        if (!update.toLowerCase().startsWith('insert') &&
            !update.toLowerCase().startsWith('delete')) {
            update = this.buildSimpleUpdate(update, options);
        }

        return {
            type: 'update',
            update: this.expandPrefixes(update),
            graph: options.graph
        };
    }

    parseDefineCommand(tokens) {
        const name = tokens[0];
        const value = tokens.slice(1).join(' ');

        if (name && value) {
            this.prefixes[name] = value.replace(/[<>]/g, '');
        }

        return {
            type: 'define',
            prefix: name,
            uri: value
        };
    }

    parseOptions(tokens) {
        const options = {};
        let i = 0;

        while (i < tokens.length) {
            if (tokens[i].startsWith('--')) {
                const key = tokens[i].slice(2);
                i++;
                if (i < tokens.length && !tokens[i].startsWith('--')) {
                    options[key] = tokens[i];
                    i++;
                } else {
                    options[key] = true;
                }
            } else {
                i++;
            }
        }

        return options;
    }

    buildSimpleQuery(text, options) {
        const vars = options.vars?.split(',') || ['s', 'p', 'o'];
        const limit = options.limit || 10;
        const offset = options.offset || 0;

        return `
            SELECT ${vars.map(v => `?${v}`).join(' ')}
            ${options.graph ? `FROM <${options.graph}>` : ''}
            WHERE {
                ${text.includes(' ') ? text : `?s ?p ?o . FILTER(regex(str(?o), "${text}", "i"))`}
            }
            LIMIT ${limit}
            OFFSET ${offset}
        `;
    }

    buildSimpleUpdate(text, options) {
        const [subject, predicate, object] = text.split(' ');
        const graph = options.graph ? `GRAPH <${options.graph}>` : '';

        return `
            INSERT DATA {
                ${graph} {
                    ${this.expandPrefixes(`${subject} ${predicate} ${object}`)}
                }
            }
        `;
    }

    expandPrefixes(text) {
        let expanded = text;
        for (const [prefix, uri] of Object.entries(this.prefixes)) {
            const regex = new RegExp(`${prefix}:([\\w-]+)`, 'g');
            expanded = expanded.replace(regex, `<${uri}$1>`);
        }
        return expanded;
    }

    async execute(commands) {
        const results = [];
        const api = this.registry.get('storage');

        for (const command of commands) {
            try {
                switch (command.type) {
                    case 'store':
                        results.push(await api.storeInteraction({
                            content: command.data,
                            format: command.format,
                            graph: command.graph
                        }));
                        break;

                    case 'query':
                        results.push(await api.executeOperation('query', {
                            sparql: command.query,
                            format: command.format
                        }));
                        break;

                    case 'update':
                        results.push(await api.executeOperation('update', {
                            sparql: command.update,
                            graph: command.graph
                        }));
                        break;

                    case 'define':
                        results.push({
                            success: true,
                            prefix: command.prefix,
                            uri: command.uri
                        });
                        break;
                }
            } catch (error) {
                results.push({
                    success: false,
                    error: error.message,
                    command
                });
            }
        }

        return results;
    }
}

================
File: src/api/common/RDFValidator.js
================
import { SPARQLHelpers } from '../../utils/SPARQLHelpers.js';

export default class RDFValidator {
    constructor(config = {}) {
        this.shapes = new Map();
        this.constraints = new Map();
        this.loadShapes(config.shapes || {});
    }

    loadShapes(shapes) {
        for (const [name, shape] of Object.entries(shapes)) {
            this.registerShape(name, shape);
        }
    }

    registerShape(name, shape) {
        this.shapes.set(name, {
            ...shape,
            constraints: shape.constraints?.map(c => this.parseConstraint(c)) || []
        });
    }

    parseConstraint(constraint) {
        const parsed = {
            path: constraint.path,
            type: constraint.type,
            message: constraint.message
        };

        switch (constraint.type) {
            case 'datatype':
                parsed.datatype = constraint.datatype;
                break;
            case 'pattern':
                parsed.pattern = new RegExp(constraint.pattern);
                break;
            case 'range':
                parsed.min = constraint.min;
                parsed.max = constraint.max;
                break;
            case 'cardinality':
                parsed.min = constraint.min;
                parsed.max = constraint.max;
                break;
            case 'class':
                parsed.class = constraint.class;
                break;
            case 'in':
                parsed.values = new Set(constraint.values);
                break;
        }

        return parsed;
    }

    generateSHACL(shape) {
        const prefixes = {
            sh: 'http://www.w3.org/ns/shacl#',
            xsd: 'http://www.w3.org/2001/XMLSchema#'
        };

        let shacl = '';
        for (const prefix in prefixes) {
            shacl += `@prefix ${prefix}: <${prefixes[prefix]}> .\n`;
        }

        shacl += `\n${shape.targetClass} a sh:NodeShape ;\n`;

        for (const constraint of shape.constraints) {
            shacl += this.constraintToSHACL(constraint);
        }

        return shacl;
    }

    constraintToSHACL(constraint) {
        let shacl = `  sh:property [\n`;
        shacl += `    sh:path ${constraint.path} ;\n`;

        switch (constraint.type) {
            case 'datatype':
                shacl += `    sh:datatype ${constraint.datatype} ;\n`;
                break;
            case 'pattern':
                shacl += `    sh:pattern "${constraint.pattern.source}" ;\n`;
                break;
            case 'range':
                if (constraint.min !== undefined) {
                    shacl += `    sh:minInclusive ${constraint.min} ;\n`;
                }
                if (constraint.max !== undefined) {
                    shacl += `    sh:maxInclusive ${constraint.max} ;\n`;
                }
                break;
            case 'cardinality':
                if (constraint.min !== undefined) {
                    shacl += `    sh:minCount ${constraint.min} ;\n`;
                }
                if (constraint.max !== undefined) {
                    shacl += `    sh:maxCount ${constraint.max} ;\n`;
                }
                break;
            case 'class':
                shacl += `    sh:class ${constraint.class} ;\n`;
                break;
            case 'in':
                shacl += `    sh:in (${Array.from(constraint.values).join(' ')}) ;\n`;
                break;
        }

        if (constraint.message) {
            shacl += `    sh:message "${constraint.message}" ;\n`;
        }

        shacl += `  ] ;\n`;
        return shacl;
    }

    async validate(data, shapeName) {
        const shape = this.shapes.get(shapeName);
        if (!shape) {
            throw new Error(`Shape not found: ${shapeName}`);
        }

        const validationResults = {
            valid: true,
            errors: []
        };

        for (const constraint of shape.constraints) {
            try {
                const result = await this.validateConstraint(data, constraint);
                if (!result.valid) {
                    validationResults.valid = false;
                    validationResults.errors.push(result);
                }
            } catch (error) {
                validationResults.valid = false;
                validationResults.errors.push({
                    path: constraint.path,
                    message: error.message
                });
            }
        }

        return validationResults;
    }

    async validateConstraint(data, constraint) {
        const value = this.getValue(data, constraint.path);

        switch (constraint.type) {
            case 'datatype':
                return this.validateDatatype(value, constraint);
            case 'pattern':
                return this.validatePattern(value, constraint);
            case 'range':
                return this.validateRange(value, constraint);
            case 'cardinality':
                return this.validateCardinality(value, constraint);
            case 'class':
                return this.validateClass(value, constraint);
            case 'in':
                return this.validateIn(value, constraint);
            default:
                throw new Error(`Unknown constraint type: ${constraint.type}`);
        }
    }

    getValue(data, path) {
        const parts = path.split('.');
        let value = data;
        for (const part of parts) {
            value = value?.[part];
            if (value === undefined) break;
        }
        return value;
    }

    validateDatatype(value, constraint) {
        if (value === undefined) return { valid: true };

        const valid = this.checkDatatype(value, constraint.datatype);
        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Invalid datatype: expected ${constraint.datatype}`
        };
    }

    validatePattern(value, constraint) {
        if (value === undefined) return { valid: true };

        const valid = constraint.pattern.test(String(value));
        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Value does not match pattern: ${constraint.pattern}`
        };
    }

    validateRange(value, constraint) {
        if (value === undefined) return { valid: true };

        const num = Number(value);
        const valid = !isNaN(num) &&
            (constraint.min === undefined || num >= constraint.min) &&
            (constraint.max === undefined || num <= constraint.max);

        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Value out of range: ${constraint.min} - ${constraint.max}`
        };
    }

    validateCardinality(value, constraint) {
        const count = Array.isArray(value) ? value.length : (value === undefined ? 0 : 1);
        const valid = (constraint.min === undefined || count >= constraint.min) &&
                     (constraint.max === undefined || count <= constraint.max);

        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Cardinality violation: expected ${constraint.min}-${constraint.max}`
        };
    }

    validateClass(value, constraint) {
        if (value === undefined) return { valid: true };

        const valid = value.type === constraint.class;
        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Invalid class: expected ${constraint.class}`
        };
    }

    validateIn(value, constraint) {
        if (value === undefined) return { valid: true };

        const valid = constraint.values.has(value);
        return {
            valid,
            path: constraint.path,
            message: valid ? null :
                constraint.message || `Value not in allowed set: ${Array.from(constraint.values).join(', ')}`
        };
    }

    checkDatatype(value, type) {
        switch (type) {
            case 'xsd:string':
                return typeof value === 'string';
            case 'xsd:integer':
                return Number.isInteger(Number(value));
            case 'xsd:decimal':
            case 'xsd:float':
            case 'xsd:double':
                return !isNaN(Number(value));
            case 'xsd:boolean':
                return typeof value === 'boolean';
            case 'xsd:date':
            case 'xsd:dateTime':
                return !isNaN(Date.parse(value));
            default:
                return true;
        }
    }
}

================
File: src/api/common/types.d.ts.ts
================
import { EventEmitter } from 'events';

export interface APIConfig {
    storage?: StorageConfig;
    models?: ModelConfig;
    metrics?: MetricsConfig;
}

export interface StorageConfig {
    type: 'memory' | 'json' | 'sparql';
    options: {
        path?: string;
        endpoint?: string;
        graphName?: string;
    };
}

export interface ModelConfig {
    chat: {
        provider: 'ollama' | 'openai';
        model: string;
        options?: Record<string, any>;
    };
    embedding: {
        provider: 'ollama' | 'openai';
        model: string;
        options?: Record<string, any>;
    };
}

export interface MetricsConfig {
    enabled: boolean;
    interval?: number;
    storageEndpoint?: string;
}

export interface Interaction {
    id: string;
    prompt: string;
    output: string;
    embedding: number[];
    timestamp: number;
    accessCount: number;
    concepts: string[];
    decayFactor: number;
}

export interface Query {
    text?: string;
    concepts?: string[];
    similarity?: number;
    limit?: number;
    offset?: number;
}

export interface MetricEvent {
    name: string;
    value: number | string | boolean;
    timestamp: number;
    labels?: Record<string, string>;
}

export interface APIMetrics {
    timestamp: number;
    status: 'active' | 'inactive';
    memoryUsage: NodeJS.MemoryUsage;
    uptime: number;
}

export declare class BaseAPI extends EventEmitter {
    protected config: APIConfig;
    protected logger: any;
    protected initialized: boolean;

    constructor(config?: APIConfig);

    initialize(): Promise<void>;
    shutdown(): Promise<void>;

    executeOperation(operation: string, params: Record<string, any>): Promise<any>;
    storeInteraction(interaction: Interaction): Promise<void>;
    retrieveInteractions(query: Query): Promise<Interaction[]>;
    getMetrics(): Promise<APIMetrics>;

    protected _validateParams(params: unknown, schema: unknown): void;
    protected _emitMetric(name: string, value: MetricEvent['value']): void;
}

export declare class APIRegistry {
    private static instance: APIRegistry;
    private apis: Map<string, BaseAPI>;
    private metrics: Map<string, MetricEvent>;

    register(name: string, apiClass: typeof BaseAPI, config?: APIConfig): Promise<BaseAPI>;
    get(name: string): BaseAPI;
    unregister(name: string): Promise<void>;
    getAll(): Map<string, BaseAPI>;
    getMetrics(): Record<string, any>;
    shutdownAll(): Promise<void>;
}


export interface CommandOptions {
    operation: string;
    params: Record<string, any>;
    format?: 'text' | 'json';
    color?: boolean;
}


export interface APIResponse<T = any> {
    success: boolean;
    data?: T;
    error?: string;
    metadata?: {
        timestamp: number;
        version: string;
    };
}


export interface REPLContext {
    api: BaseAPI;
    history: string[];
    mode: 'chat' | 'rdf';
}


export interface SelfieMetrics {
    storage: {
        size: number;
        operations: number;
        latency: number;
    };
    performance: {
        memory: NodeJS.MemoryUsage;
        cpu: number;
        uptime: number;
    };
    errors: Array<{
        type: string;
        count: number;
        lastOccurred: number;
    }>;
}

================
File: src/api/features/ActiveHandler.js
================
import BaseAPI from '../common/BaseAPI.js';
import { APIRegistry } from '../common/APIRegistry.js';
import { logger } from '../../Utils.js';

export default class ActiveHandler extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.registry = new APIRegistry();
        this.contextWindow = config.contextWindow || 3;
        this.similarityThreshold = config.similarityThreshold || 40;
    }

    async executeOperation(operation, params) {
        switch (operation) {
            case 'interact':
                return this.handleInteraction(params);
            case 'search':
                return this.handleSearch(params);
            case 'analyze':
                return this.handleAnalysis(params);
            default:
                throw new Error(`Unknown operation: ${operation}`);
        }
    }

    async handleInteraction({ prompt, context = [], options = {} }) {
        try {
            const memoryManager = this.registry.get('memory');
            const passive = this.registry.get('passive');


            const retrievals = await memoryManager.retrieveRelevantInteractions(
                prompt,
                this.similarityThreshold
            );


            const response = await passive.executeOperation('chat', {
                prompt,
                context: this._buildContext(context, retrievals),
                ...options
            });


            const embedding = await memoryManager.generateEmbedding(
                `${prompt} ${response}`
            );
            const concepts = await memoryManager.extractConcepts(
                `${prompt} ${response}`
            );

            await memoryManager.addInteraction(prompt, response, embedding, concepts);

            this._emitMetric('interaction.count', 1);
            return { response, concepts, retrievals };
        } catch (error) {
            this._emitMetric('interaction.errors', 1);
            throw error;
        }
    }

    async handleSearch({ query, type = 'semantic', limit = 10 }) {
        try {
            const memoryManager = this.registry.get('memory');
            const passive = this.registry.get('passive');

            let results;
            if (type === 'semantic') {
                const embedding = await memoryManager.generateEmbedding(query);
                results = await memoryManager.retrieveRelevantInteractions(
                    query,
                    this.similarityThreshold,
                    0,
                    limit
                );
            } else {
                results = await passive.executeOperation('query', {
                    sparql: this._buildSearchQuery(query, limit)
                });
            }

            this._emitMetric('search.count', 1);
            return results;
        } catch (error) {
            this._emitMetric('search.errors', 1);
            throw error;
        }
    }

    async handleAnalysis({ content, type = 'concept' }) {
        try {
            const memoryManager = this.registry.get('memory');

            let results;
            switch (type) {
                case 'concept':
                    results = await memoryManager.extractConcepts(content);
                    break;
                case 'embedding':
                    results = await memoryManager.generateEmbedding(content);
                    break;
                default:
                    throw new Error(`Unknown analysis type: ${type}`);
            }

            this._emitMetric('analysis.count', 1);
            return results;
        } catch (error) {
            this._emitMetric('analysis.errors', 1);
            throw error;
        }
    }

    _buildContext(context, retrievals) {
        return {
            previous: context.slice(-this.contextWindow),
            relevant: retrievals
                .slice(0, this.contextWindow)
                .map(r => ({
                    prompt: r.interaction.prompt,
                    response: r.interaction.output
                }))
        };
    }

    _buildSearchQuery(query, limit) {
        return `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            SELECT ?interaction ?prompt ?output ?timestamp
            WHERE {
                ?interaction a mcp:Interaction ;
                    mcp:prompt ?prompt ;
                    mcp:output ?output ;
                    mcp:timestamp ?timestamp .
                FILTER(CONTAINS(LCASE(?prompt), LCASE("${query}")) ||
                       CONTAINS(LCASE(?output), LCASE("${query}")))
            }
            ORDER BY DESC(?timestamp)
            LIMIT ${limit}
        `;
    }

    async getMetrics() {
        const baseMetrics = await super.getMetrics();
        return {
            ...baseMetrics,
            operations: {
                interaction: await this._getOperationMetrics('interaction'),
                search: await this._getOperationMetrics('search'),
                analysis: await this._getOperationMetrics('analysis')
            }
        };
    }

    async _getOperationMetrics(operation) {
        return {
            count: await this._getMetricValue(`${operation}.count`),
            errors: await this._getMetricValue(`${operation}.errors`),
            latency: await this._getMetricValue(`${operation}.latency`)
        };
    }
}

================
File: src/api/features/PassiveHandler.js
================
import BaseAPI from '../common/BaseAPI.js';
import { APIRegistry } from '../common/APIRegistry.js';
import { logger } from '../../Utils.js';

export default class PassiveHandler extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.registry = new APIRegistry();
        this.llmProvider = config.llmProvider;
        this.sparqlEndpoint = config.sparqlEndpoint;
    }

    async executeOperation(operation, params) {
        switch (operation) {
            case 'chat':
                return this.handleChat(params);
            case 'query':
                return this.handleQuery(params);
            case 'store':
                return this.handleStore(params);
            default:
                throw new Error(`Unknown operation: ${operation}`);
        }
    }

    async handleChat({ prompt, model = 'qwen2:1.5b', options = {} }) {
        try {
            const response = await this.llmProvider.generateChat(model, [{
                role: 'user',
                content: prompt
            }], options);

            this._emitMetric('chat.requests', 1);
            return response;
        } catch (error) {
            this._emitMetric('chat.errors', 1);
            throw error;
        }
    }

    async handleQuery({ sparql, format = 'json' }) {
        try {
            const storage = this.registry.get('storage');
            const results = await storage.executeOperation('query', {
                sparql,
                format
            });

            this._emitMetric('query.requests', 1);
            return results;
        } catch (error) {
            this._emitMetric('query.errors', 1);
            throw error;
        }
    }

    async handleStore({ content, format = 'text' }) {
        try {
            const storage = this.registry.get('storage');
            await storage.storeInteraction({
                content,
                format,
                timestamp: Date.now()
            });

            this._emitMetric('store.requests', 1);
            return { success: true };
        } catch (error) {
            this._emitMetric('store.errors', 1);
            throw error;
        }
    }

    async getMetrics() {
        const baseMetrics = await super.getMetrics();
        return {
            ...baseMetrics,
            operations: {
                chat: await this._getOperationMetrics('chat'),
                query: await this._getOperationMetrics('query'),
                store: await this._getOperationMetrics('store')
            }
        };
    }

    async _getOperationMetrics(operation) {
        return {
            requests: await this._getMetricValue(`${operation}.requests`),
            errors: await this._getMetricValue(`${operation}.errors`),
            latency: await this._getMetricValue(`${operation}.latency`)
        };
    }
}

================
File: src/api/features/SelfieHandler.js
================
import { EventEmitter } from 'events';
import log from 'loglevel';
import { APIRegistry } from '../common/APIRegistry.js';
import BaseAPI from '../common/BaseAPI.js';

export default class SelfieHandler extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.registry = new APIRegistry();
        this.metrics = new Map();
        this.errors = new Map();
        this.interval = config.interval || 60000;
        this.eventBus = new EventEmitter();
        this.setupMetricCollectors();
    }

    setupMetricCollectors() {
        this.collectors = {
            storage: {
                collect: async () => {
                    const api = this.registry.get('storage');
                    const metrics = await api.getMetrics();
                    return {
                        size: metrics.size || 0,
                        operations: metrics.operations || 0,
                        latency: metrics.latency || 0
                    };
                },
                labels: ['size', 'operations', 'latency']
            },
            performance: {
                collect: () => ({
                    memory: process.memoryUsage(),
                    cpu: process.cpuUsage(),
                    uptime: process.uptime()
                }),
                labels: ['memory', 'cpu', 'uptime']
            },
            api: {
                collect: async () => {
                    const apis = this.registry.getAll();
                    const metrics = {};
                    for (const [name, api] of apis) {
                        metrics[name] = await api.getMetrics();
                    }
                    return metrics;
                },
                labels: ['status', 'requests', 'errors']
            }
        };
    }

    async initialize() {
        await super.initialize();


        this.collectionTimer = setInterval(
            () => this.collectMetrics(),
            this.interval
        );


        process.on('uncaughtException', (error) => {
            this.trackError('uncaughtException', error);
        });

        process.on('unhandledRejection', (error) => {
            this.trackError('unhandledRejection', error);
        });


        if (this.config.openTelemetry) {
            await this.setupOpenTelemetry();
        }

        this.logger.info('SelfieHandler initialized');
    }

    async setupOpenTelemetry() {

        const { trace, metrics } = await import('@opentelemetry/api');
        const { Resource } = await import('@opentelemetry/resources');
        const { SemanticResourceAttributes } = await import('@opentelemetry/semantic-conventions');

        const resource = new Resource({
            [SemanticResourceAttributes.SERVICE_NAME]: 'semem'
        });


        if (this.config.openTelemetry.metrics) {
            const meter = metrics.getMeter('semem-metrics');
            this.setupMetricInstruments(meter);
        }
    }

    setupMetricInstruments(meter) {
        this.instruments = {
            memoryUsage: meter.createHistogram('memory_usage', {
                description: 'Memory usage statistics'
            }),
            apiLatency: meter.createHistogram('api_latency', {
                description: 'API request latency'
            }),
            storageOperations: meter.createCounter('storage_operations', {
                description: 'Storage operation count'
            })
        };
    }

    async collectMetrics() {
        try {
            for (const [name, collector] of Object.entries(this.collectors)) {
                const metrics = await collector.collect();
                this.metrics.set(name, {
                    timestamp: Date.now(),
                    values: metrics
                });


                this.eventBus.emit('metrics', {
                    name,
                    metrics,
                    timestamp: Date.now()
                });


                if (this.instruments) {
                    this.updateOpenTelemetryMetrics(name, metrics);
                }
            }


            if (this.config.storageEndpoint) {
                await this.storeMetrics();
            }
        } catch (error) {
            this.logger.error('Error collecting metrics:', error);
            this.trackError('metricCollection', error);
        }
    }

    updateOpenTelemetryMetrics(name, metrics) {
        switch (name) {
            case 'performance':
                this.instruments.memoryUsage.record(
                    metrics.memory.heapUsed,
                    { type: 'heap_used' }
                );
                break;
            case 'api':
                Object.entries(metrics).forEach(([api, apiMetrics]) => {
                    this.instruments.apiLatency.record(
                        apiMetrics.latency || 0,
                        { api }
                    );
                });
                break;
            case 'storage':
                this.instruments.storageOperations.add(
                    metrics.operations,
                    { type: 'total' }
                );
                break;
        }
    }

    async storeMetrics() {
        try {
            const metricsData = this.formatMetricsForStorage();
            const response = await fetch(this.config.storageEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(metricsData)
            });

            if (!response.ok) {
                throw new Error(`Failed to store metrics: ${response.status}`);
            }
        } catch (error) {
            this.logger.error('Error storing metrics:', error);
            this.trackError('metricStorage', error);
        }
    }

    formatMetricsForStorage() {
        return {
            timestamp: Date.now(),
            metrics: Object.fromEntries(this.metrics),
            errors: Array.from(this.errors.values())
        };
    }

    trackError(type, error) {
        const errorKey = `${type}:${error.message}`;
        const existing = this.errors.get(errorKey) || {
            type,
            message: error.message,
            count: 0,
            firstOccurred: Date.now(),
            lastOccurred: Date.now()
        };

        existing.count++;
        existing.lastOccurred = Date.now();
        this.errors.set(errorKey, existing);

        this.eventBus.emit('error', {
            type,
            error,
            count: existing.count
        });
    }

    getMetrics() {
        return {
            timestamp: Date.now(),
            collectors: Object.fromEntries(this.metrics),
            errors: Array.from(this.errors.values())
        };
    }

    onMetrics(callback) {
        this.eventBus.on('metrics', callback);
    }

    onError(callback) {
        this.eventBus.on('error', callback);
    }

    async shutdown() {
        if (this.collectionTimer) {
            clearInterval(this.collectionTimer);
        }


        if (this.config.storageEndpoint) {
            await this.storeMetrics();
        }

        this.eventBus.removeAllListeners();
        await super.shutdown();
    }
}

================
File: src/api/http/client/SememClient.js
================
export default class SememClient {
    constructor(config = {}) {
        this.baseUrl = config.baseUrl || 'http://localhost:3000/api';
        this.apiKey = config.apiKey;
        this.timeout = config.timeout || 30000;
        this.retries = config.retries || 3;
        this.retryDelay = config.retryDelay || 1000;
    }

    async request(endpoint, options = {}) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), this.timeout);

        try {
            const response = await this.retryRequest(endpoint, {
                ...options,
                signal: controller.signal,
                headers: {
                    'Content-Type': 'application/json',
                    'X-API-Key': this.apiKey,
                    ...options.headers
                }
            });

            const data = await response.json();
            if (!data.success) {
                throw new Error(data.error || 'API request failed');
            }

            return data.data;
        } finally {
            clearTimeout(timeoutId);
        }
    }

    async retryRequest(endpoint, options) {
        let lastError;
        for (let attempt = 0; attempt < this.retries; attempt++) {
            try {
                const response = await fetch(`${this.baseUrl}${endpoint}`, options);
                if (response.ok) return response;

                lastError = new Error(`HTTP ${response.status}: ${response.statusText}`);
                if (!this.isRetryable(response.status)) throw lastError;
            } catch (error) {
                lastError = error;
                if (!this.isRetryable(error)) throw error;
            }

            await new Promise(resolve =>
                setTimeout(resolve, this.retryDelay * Math.pow(2, attempt))
            );
        }
        throw lastError;
    }

    isRetryable(statusOrError) {
        if (typeof statusOrError === 'number') {
            return [408, 429, 500, 502, 503, 504].includes(statusOrError);
        }
        return statusOrError.name === 'AbortError' ||
               statusOrError.name === 'NetworkError';
    }


    async chat(prompt, options = {}) {
        return this.request('/chat', {
            method: 'POST',
            body: JSON.stringify({
                prompt,
                model: options.model || 'qwen2:1.5b',
                ...options
            })
        });
    }


    async store(data, format = 'text') {
        return this.request('/store', {
            method: 'POST',
            body: JSON.stringify({ content: data, format })
        });
    }

    async storeInteraction(interaction) {
        return this.request('/store', {
            method: 'POST',
            body: JSON.stringify(interaction)
        });
    }


    async query(options = {}) {
        const params = new URLSearchParams();
        if (options.text) params.set('text', options.text);
        if (options.concepts) params.set('concepts', JSON.stringify(options.concepts));
        if (options.similarity) params.set('similarity', options.similarity);
        if (options.limit) params.set('limit', options.limit);
        if (options.offset) params.set('offset', options.offset);

        return this.request(`/query?${params}`);
    }

    async sparqlQuery(query) {
        return this.request('/query', {
            method: 'POST',
            body: JSON.stringify({ sparql: query })
        });
    }


    async getMetrics() {
        return this.request('/metrics');
    }


    async *streamChat(prompt, options = {}) {
        const response = await fetch(`${this.baseUrl}/chat/stream`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'X-API-Key': this.apiKey
            },
            body: JSON.stringify({
                prompt,
                model: options.model || 'qwen2:1.5b',
                ...options
            })
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        try {
            while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value, { stream: true });
                yield JSON.parse(chunk);
            }
        } finally {
            reader.releaseLock();
        }
    }


    async batchStore(interactions) {
        return this.request('/store/batch', {
            method: 'POST',
            body: JSON.stringify(interactions)
        });
    }

    async batchQuery(queries) {
        return this.request('/query/batch', {
            method: 'POST',
            body: JSON.stringify(queries)
        });
    }


    formatInteraction(prompt, response, options = {}) {
        return {
            prompt,
            output: response,
            timestamp: Date.now(),
            ...options
        };
    }

    buildQuery(text, options = {}) {
        return {
            text,
            similarity: 0.7,
            limit: 10,
            ...options
        };
    }
}

================
File: src/api/http/server/HTTPServer.js
================
import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import compression from 'compression';
import { rateLimit } from 'express-rate-limit';
import swaggerUi from 'swagger-ui-express';
import BaseAPI from '../../common/BaseAPI.js';
import { APIRegistry } from '../../common/APIRegistry.js';

export default class HTTPServer extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.app = express();
        this.registry = new APIRegistry();
        this.port = config.port || 3000;
        this.setupMiddleware();
        this.setupRoutes();
    }

    setupMiddleware() {

        this.app.use(helmet());
        this.app.use(cors());
        this.app.use(compression());
        this.app.use(express.json());


        const limiter = rateLimit({
            windowMs: 15 * 60 * 1000,
            max: 100,
            standardHeaders: true,
            legacyHeaders: false
        });
        this.app.use(limiter);


        this.app.use((req, res, next) => {
            this.logger.debug(`${req.method} ${req.path}`);
            const start = Date.now();
            res.on('finish', () => {
                const duration = Date.now() - start;
                this._emitMetric('http.request.duration', duration);
                this._emitMetric('http.request.status', res.statusCode);
            });
            next();
        });


        this.app.use((err, req, res, next) => {
            this.logger.error('Server error:', err);
            res.status(500).json({
                success: false,
                error: err.message,
                metadata: {
                    timestamp: Date.now(),
                    path: req.path
                }
            });
        });
    }

    setupRoutes() {

        if (this.config.openapi) {
            this.app.use('/docs', swaggerUi.serve,
                swaggerUi.setup(this.config.openapi));
        }


        this.app.post('/api/chat', async (req, res) => {
            try {
                const api = this.registry.get('chat');
                const response = await api.executeOperation('chat', req.body);
                res.json({
                    success: true,
                    data: response
                });
            } catch (error) {
                res.status(400).json({
                    success: false,
                    error: error.message
                });
            }
        });


        this.app.post('/api/store', async (req, res) => {
            try {
                const api = this.registry.get('storage');
                await api.storeInteraction(req.body);
                res.json({ success: true });
            } catch (error) {
                res.status(400).json({
                    success: false,
                    error: error.message
                });
            }
        });


        this.app.get('/api/query', async (req, res) => {
            try {
                const api = this.registry.get('storage');
                const results = await api.retrieveInteractions(req.query);
                res.json({
                    success: true,
                    data: results
                });
            } catch (error) {
                res.status(400).json({
                    success: false,
                    error: error.message
                });
            }
        });


        this.app.get('/api/metrics', async (req, res) => {
            try {
                const metrics = await this.getMetrics();
                res.json({
                    success: true,
                    data: metrics
                });
            } catch (error) {
                res.status(500).json({
                    success: false,
                    error: error.message
                });
            }
        });


        this.app.get('/health', (req, res) => {
            res.json({
                status: 'healthy',
                timestamp: Date.now(),
                uptime: process.uptime()
            });
        });
    }

    async initialize() {
        await super.initialize();
        return new Promise((resolve) => {
            this.server = this.app.listen(this.port, () => {
                this.logger.info(`HTTP server listening on port ${this.port}`);
                resolve();
            });
        });
    }

    async shutdown() {
        if (this.server) {
            await new Promise((resolve) => {
                this.server.close(resolve);
            });
            this.logger.info('HTTP server shut down');
        }
        await super.shutdown();
    }
}

================
File: src/api/http/server/openapi-schema.js
================
export default {
    openapi: '3.0.0',
    info: {
        title: 'Semem API',
        version: '1.0.0',
        description: 'Semantic Memory Management System API'
    },
    servers: [
        {
            url: 'http://localhost:3000',
            description: 'Development server'
        }
    ],
    components: {
        schemas: {
            Interaction: {
                type: 'object',
                required: ['prompt', 'output', 'embedding'],
                properties: {
                    id: { type: 'string', format: 'uuid' },
                    prompt: { type: 'string' },
                    output: { type: 'string' },
                    embedding: {
                        type: 'array',
                        items: { type: 'number' }
                    },
                    timestamp: { type: 'integer' },
                    accessCount: { type: 'integer' },
                    concepts: {
                        type: 'array',
                        items: { type: 'string' }
                    },
                    decayFactor: { type: 'number' }
                }
            },
            Query: {
                type: 'object',
                properties: {
                    text: { type: 'string' },
                    concepts: {
                        type: 'array',
                        items: { type: 'string' }
                    },
                    similarity: { type: 'number', minimum: 0, maximum: 100 },
                    limit: { type: 'integer', minimum: 1 },
                    offset: { type: 'integer', minimum: 0 }
                }
            },
            APIResponse: {
                type: 'object',
                required: ['success'],
                properties: {
                    success: { type: 'boolean' },
                    data: { type: 'object' },
                    error: { type: 'string' },
                    metadata: {
                        type: 'object',
                        properties: {
                            timestamp: { type: 'integer' },
                            version: { type: 'string' }
                        }
                    }
                }
            },
            Metrics: {
                type: 'object',
                properties: {
                    timestamp: { type: 'integer' },
                    status: {
                        type: 'string',
                        enum: ['active', 'inactive']
                    },
                    memoryUsage: {
                        type: 'object',
                        properties: {
                            heapTotal: { type: 'number' },
                            heapUsed: { type: 'number' },
                            external: { type: 'number' }
                        }
                    },
                    uptime: { type: 'number' }
                }
            }
        },
        securitySchemes: {
            apiKey: {
                type: 'apiKey',
                in: 'header',
                name: 'X-API-Key'
            }
        }
    },
    paths: {
        '/api/chat': {
            post: {
                summary: 'Chat with the system',
                tags: ['Chat'],
                security: [{ apiKey: [] }],
                requestBody: {
                    required: true,
                    content: {
                        'application/json': {
                            schema: {
                                type: 'object',
                                required: ['prompt'],
                                properties: {
                                    prompt: { type: 'string' },
                                    model: { type: 'string' },
                                    options: { type: 'object' }
                                }
                            }
                        }
                    }
                },
                responses: {
                    '200': {
                        description: 'Successful response',
                        content: {
                            'application/json': {
                                schema: { $ref: '#/components/schemas/APIResponse' }
                            }
                        }
                    }
                }
            }
        },
        '/api/store': {
            post: {
                summary: 'Store an interaction',
                tags: ['Storage'],
                security: [{ apiKey: [] }],
                requestBody: {
                    required: true,
                    content: {
                        'application/json': {
                            schema: { $ref: '#/components/schemas/Interaction' }
                        }
                    }
                },
                responses: {
                    '200': {
                        description: 'Successfully stored',
                        content: {
                            'application/json': {
                                schema: { $ref: '#/components/schemas/APIResponse' }
                            }
                        }
                    }
                }
            }
        },
        '/api/query': {
            get: {
                summary: 'Query stored interactions',
                tags: ['Storage'],
                security: [{ apiKey: [] }],
                parameters: [
                    {
                        name: 'text',
                        in: 'query',
                        schema: { type: 'string' }
                    },
                    {
                        name: 'concepts',
                        in: 'query',
                        schema: {
                            type: 'array',
                            items: { type: 'string' }
                        }
                    },
                    {
                        name: 'similarity',
                        in: 'query',
                        schema: { type: 'number' }
                    },
                    {
                        name: 'limit',
                        in: 'query',
                        schema: { type: 'integer' }
                    },
                    {
                        name: 'offset',
                        in: 'query',
                        schema: { type: 'integer' }
                    }
                ],
                responses: {
                    '200': {
                        description: 'Query results',
                        content: {
                            'application/json': {
                                schema: { $ref: '#/components/schemas/APIResponse' }
                            }
                        }
                    }
                }
            }
        },
        '/api/metrics': {
            get: {
                summary: 'Get system metrics',
                tags: ['Monitoring'],
                security: [{ apiKey: [] }],
                responses: {
                    '200': {
                        description: 'System metrics',
                        content: {
                            'application/json': {
                                schema: { $ref: '#/components/schemas/Metrics' }
                            }
                        }
                    }
                }
            }
        },
        '/health': {
            get: {
                summary: 'Health check',
                tags: ['Monitoring'],
                responses: {
                    '200': {
                        description: 'System health status',
                        content: {
                            'application/json': {
                                schema: {
                                    type: 'object',
                                    properties: {
                                        status: { type: 'string' },
                                        timestamp: { type: 'integer' },
                                        uptime: { type: 'number' }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
};

================
File: src/api/repl/REPLHandler.js
================
import { createInterface } from 'readline';
import chalk from 'chalk';
import BaseAPI from '../common/BaseAPI.js';
import { APIRegistry } from '../common/APIRegistry.js';

export default class REPLHandler extends BaseAPI {
    constructor(config = {}) {
        super(config);
        this.registry = new APIRegistry();
        this.history = [];
        this.mode = 'chat';
        this.commands = this.setupCommands();
    }

    setupCommands() {
        return {
            help: {
                desc: 'Show help menu',
                handler: () => this.showHelp()
            },
            mode: {
                desc: 'Switch mode (chat/rdf)',
                handler: (args) => this.switchMode(args[0])
            },
            clear: {
                desc: 'Clear screen',
                handler: () => console.clear()
            },
            history: {
                desc: 'Show command history',
                handler: () => this.showHistory()
            },
            exit: {
                desc: 'Exit REPL',
                handler: () => this.shutdown()
            }
        };
    }

    async initialize() {
        await super.initialize();

        this.rl = createInterface({
            input: process.stdin,
            output: process.stdout,
            prompt: this.getPrompt(),
            historySize: 100,
            removeHistoryDuplicates: true
        });

        this.rl.on('line', async (line) => {
            if (line.trim()) {
                this.history.push(line);
                await this.processInput(line);
            }
            this.rl.prompt();
        });

        this.rl.on('close', () => {
            this.shutdown();
        });

        console.clear();
        this.showWelcome();
        this.rl.prompt();
    }

    getPrompt() {
        return chalk.cyan(`semem(${this.mode})> `);
    }

    showWelcome() {
        console.log(chalk.green('Welcome to Semem Interactive Shell'));
        console.log(chalk.gray('Type "help" for available commands'));
        console.log();
    }

    showHelp() {
        console.log(chalk.yellow('\nAvailable Commands:'));
        Object.entries(this.commands).forEach(([cmd, info]) => {
            console.log(chalk.cyan(`  ${cmd.padEnd(10)} - ${info.desc}`));
        });
        console.log(chalk.yellow('\nModes:'));
        console.log(chalk.cyan('  chat      - Natural language interactions'));
        console.log(chalk.cyan('  rdf       - RDF/SPARQL queries'));
        console.log();
    }

    showHistory() {
        if (this.history.length === 0) {
            console.log(chalk.gray('No history available'));
            return;
        }

        console.log(chalk.yellow('\nCommand History:'));
        this.history.slice(-10).forEach((cmd, i) => {
            console.log(chalk.gray(`  ${i + 1}. ${cmd}`));
        });
        console.log();
    }

    switchMode(newMode) {
        const validModes = ['chat', 'rdf'];
        if (!validModes.includes(newMode)) {
            console.log(chalk.red(`Invalid mode. Valid modes: ${validModes.join(', ')}`));
            return;
        }

        this.mode = newMode;
        this.rl.setPrompt(this.getPrompt());
        console.log(chalk.green(`Switched to ${newMode} mode`));
    }

    async processInput(input) {
        const trimmed = input.trim();
        if (!trimmed) return;

        const [command, ...args] = trimmed.split(' ');


        if (this.commands[command]) {
            await this.commands[command].handler(args);
            return;
        }

        try {

            switch (this.mode) {
                case 'chat':
                    await this.handleChat(trimmed);
                    break;
                case 'rdf':
                    await this.handleRDF(trimmed);
                    break;
            }
        } catch (error) {
            console.error(chalk.red('Error:'), error.message);
        }
    }

    async handleChat(input) {
        try {
            const api = this.registry.get('chat');
            const response = await api.executeOperation('chat', {
                prompt: input,
                mode: 'chat'
            });


            console.log(chalk.green('\nAssistant:'), response);
            console.log();


            const storageApi = this.registry.get('storage');
            await storageApi.storeInteraction({
                prompt: input,
                output: response,
                timestamp: Date.now()
            });
        } catch (error) {
            console.error(chalk.red('Chat error:'), error.message);
        }
    }

    async handleRDF(input) {
        try {
            const api = this.registry.get('storage');
            let response;

            if (input.toLowerCase().startsWith('select') ||
                input.toLowerCase().startsWith('ask') ||
                input.toLowerCase().startsWith('construct')) {

                response = await api.executeOperation('query', {
                    sparql: input
                });
            } else {

                response = await api.executeOperation('update', {
                    sparql: input
                });
            }


            if (Array.isArray(response)) {
                console.log(chalk.yellow('\nResults:'));
                response.forEach(result => {
                    console.log(chalk.gray('-'), result);
                });
            } else {
                console.log(chalk.green('\nOperation completed successfully'));
            }
            console.log();
        } catch (error) {
            console.error(chalk.red('RDF error:'), error.message);
        }
    }

    async shutdown() {
        console.log(chalk.yellow('\nShutting down...'));
        if (this.rl) {
            this.rl.close();
        }
        await super.shutdown();
        process.exit(0);
    }
}

================
File: src/api/about.md
================
src/api/
├── common/
│ ├── BaseAPI.js # Abstract base interface
│ ├── APIRegistry.js # API registration/discovery
│ └── types.d.ts # TypeScript definitions
├── cli/
│ └── CLIHandler.js # Command line interface
├── repl/
│ └── REPLHandler.js # Interactive shell
├── http/
│ ├── server/
│ │ ├── HTTPServer.js # Express server
│ │ └── routes/ # API endpoints
│ └── client/
│ └── forms/ # Web interface
├── features/
│ ├── SelfieHandler.js # Metrics & monitoring
│ ├── PassiveHandler.js # Individual operations
│ └── ActiveHandler.js # Combined operations
└── utils/
├── MetricsCollector.js # Performance tracking
└── APILogger.js # Logging wrapper

================
File: src/api/APILogger.js
================
import log from 'loglevel';
import { EventEmitter } from 'events';

export default class APILogger extends EventEmitter {
    constructor(options = {}) {
        super();
        this.name = options.name || 'API';
        this.level = options.level || 'info';
        this.maxEntries = options.maxEntries || 1000;
        this.logEntries = [];

        this.logger = log.getLogger(this.name);
        this.logger.setLevel(this.level);

        this.setupMethods();
    }

    setupMethods() {
        const levels = ['trace', 'debug', 'info', 'warn', 'error'];

        levels.forEach(level => {
            this[level] = (...args) => {
                const entry = this.createLogEntry(level, ...args);
                this.logEntries.push(entry);

                if (this.logEntries.length > this.maxEntries) {
                    this.logEntries.shift();
                }

                this.emit('log', entry);
                this.logger[level](...args);

                return entry;
            };
        });
    }

    createLogEntry(level, ...args) {
        const entry = {
            timestamp: new Date().toISOString(),
            level,
            message: args.map(arg =>
                typeof arg === 'object' ? JSON.stringify(arg) : String(arg)
            ).join(' '),
            metadata: {
                pid: process.pid,
                hostname: require('os').hostname()
            }
        };


        const error = args.find(arg => arg instanceof Error);
        if (error) {
            entry.error = {
                name: error.name,
                message: error.message,
                stack: error.stack
            };
        }

        return entry;
    }

    getEntries(options = {}) {
        let entries = [...this.logEntries];

        if (options.level) {
            entries = entries.filter(entry => entry.level === options.level);
        }

        if (options.since) {
            entries = entries.filter(entry =>
                new Date(entry.timestamp) >= new Date(options.since)
            );
        }

        if (options.until) {
            entries = entries.filter(entry =>
                new Date(entry.timestamp) <= new Date(options.until)
            );
        }

        if (options.limit) {
            entries = entries.slice(-options.limit);
        }

        return entries;
    }

    clearEntries() {
        this.logEntries = [];
    }

    setLevel(level) {
        this.level = level;
        this.logger.setLevel(level);
    }

    getLevel() {
        return this.level;
    }

    createChild(name, options = {}) {
        return new APILogger({
            ...options,
            name: `${this.name}:${name}`,
            level: options.level || this.level
        });
    }

    dispose() {
        this.removeAllListeners();
        this.clearEntries();
    }
}

================
File: src/api/MetricsCollector.js
================
import { EventEmitter } from 'events';
import { logger } from '../Utils.js';

export default class MetricsCollector extends EventEmitter {
    constructor(options = {}) {
        super();
        this.metrics = new Map();
        this.interval = options.interval || 60000;
        this.maxHistory = options.maxHistory || 1000;
        this.startTime = Date.now();
        this.setupCleanup();
    }

    setupCleanup() {
        this.cleanupInterval = setInterval(() => {
            this.pruneMetrics();
        }, this.interval);
    }

    collect(name, value, labels = {}) {
        const timestamp = Date.now();
        const key = this.generateKey(name, labels);

        if (!this.metrics.has(key)) {
            this.metrics.set(key, []);
        }

        const series = this.metrics.get(key);
        series.push({ timestamp, value });

        this.emit('metric', { name, value, timestamp, labels });

        if (series.length > this.maxHistory) {
            series.shift();
        }
    }

    generateKey(name, labels) {
        const labelStr = Object.entries(labels)
            .sort(([a], [b]) => a.localeCompare(b))
            .map(([k, v]) => `${k}=${v}`)
            .join(',');
        return labelStr ? `${name}{${labelStr}}` : name;
    }

    getMetric(name, labels = {}) {
        const key = this.generateKey(name, labels);
        return this.metrics.get(key) || [];
    }

    getSummary(name, labels = {}) {
        const series = this.getMetric(name, labels);
        if (series.length === 0) return null;

        const values = series.map(point => point.value);
        return {
            count: values.length,
            min: Math.min(...values),
            max: Math.max(...values),
            avg: values.reduce((a, b) => a + b, 0) / values.length,
            last: values[values.length - 1]
        };
    }

    pruneMetrics() {
        const cutoff = Date.now() - this.interval;

        for (const [key, series] of this.metrics.entries()) {
            const filtered = series.filter(point => point.timestamp >= cutoff);
            if (filtered.length === 0) {
                this.metrics.delete(key);
            } else {
                this.metrics.set(key, filtered);
            }
        }
    }

    getSnapshot() {
        const snapshot = {
            timestamp: Date.now(),
            uptime: Date.now() - this.startTime,
            metrics: {}
        };

        for (const [key, series] of this.metrics.entries()) {
            snapshot.metrics[key] = this.getSummary(key);
        }

        return snapshot;
    }

    reset() {
        this.metrics.clear();
        this.startTime = Date.now();
    }

    dispose() {
        if (this.cleanupInterval) {
            clearInterval(this.cleanupInterval);
        }
        this.removeAllListeners();
        this.metrics.clear();
    }
}

================
File: src/connectors/OllamaConnector.js
================
import fetch from 'node-fetch';

export default class OllamaConnector {
    constructor(baseUrl = 'http://localhost:11434') {
        this.baseUrl = baseUrl;
    }

    async generateEmbedding(model, input) {
        const response = await fetch(`${this.baseUrl}/api/embeddings`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                prompt: input,
                options: {
                    num_ctx: 8192
                }
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.embedding;
    }

    async generateChat(model, messages, options = {}) {
        const response = await fetch(`${this.baseUrl}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                messages,
                stream: false,
                options
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.message.content;
    }

    async generateCompletion(model, prompt, options = {}) {
        const response = await fetch(`${this.baseUrl}/api/generate`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model,
                prompt,
                stream: false,
                options
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        return data.response;
    }
}

================
File: src/stores/BaseStore.js
================
export default class BaseStore {
    async loadHistory() {
        throw new Error('Method loadHistory() must be implemented');
    }

    async saveMemoryToHistory(memoryStore) {
        throw new Error('Method saveMemoryToHistory() must be implemented');
    }

    async beginTransaction() {
        throw new Error('Method beginTransaction() must be implemented');
    }

    async commitTransaction() {
        throw new Error('Method commitTransaction() must be implemented');
    }

    async rollbackTransaction() {
        throw new Error('Method rollbackTransaction() must be implemented');
    }

    async verify() {
        throw new Error('Method verify() must be implemented');
    }

    async close() {
        throw new Error('Method close() must be implemented');
    }
}

================
File: src/stores/CachedSPARQLStore.js
================
import SPARQLStore from './SPARQLStore.js';
import { logger } from '../Utils.js';

export default class CachedSPARQLStore extends SPARQLStore {
    constructor(endpoint, options = {}) {
        super(endpoint, options);


        this.cacheEnabled = options.cacheEnabled ?? true;
        this.cacheTTL = options.cacheTTL || 300000;
        this.maxCacheSize = options.maxCacheSize || 100;


        this.queryCache = new Map();
        this.cacheTimestamps = new Map();


        this.cleanupInterval = setInterval(() => {
            this.cleanupCache();
        }, this.cacheTTL / 2);
    }

    async _executeSparqlQuery(query, endpoint) {
        if (!this.cacheEnabled) {
            return super._executeSparqlQuery(query, endpoint);
        }

        const cacheKey = this._generateCacheKey(query);


        const cachedResult = this.queryCache.get(cacheKey);
        if (cachedResult) {
            const timestamp = this.cacheTimestamps.get(cacheKey);
            if (Date.now() - timestamp < this.cacheTTL) {
                logger.debug('Cache hit:', cacheKey);
                return JSON.parse(JSON.stringify(cachedResult));
            }
        }


        const result = await super._executeSparqlQuery(query, endpoint);


        this.queryCache.set(cacheKey, result);
        this.cacheTimestamps.set(cacheKey, Date.now());


        if (this.queryCache.size > this.maxCacheSize) {
            this.cleanupCache();
        }

        return result;
    }

    _generateCacheKey(query) {

        return query.replace(/\s+/g, ' ').trim();
    }

    cleanupCache() {
        const now = Date.now();


        for (const [key, timestamp] of this.cacheTimestamps.entries()) {
            if (now - timestamp > this.cacheTTL) {
                this.queryCache.delete(key);
                this.cacheTimestamps.delete(key);
            }
        }


        while (this.queryCache.size > this.maxCacheSize) {
            let oldestKey = null;
            let oldestTime = Infinity;

            for (const [key, timestamp] of this.cacheTimestamps.entries()) {
                if (timestamp < oldestTime) {
                    oldestTime = timestamp;
                    oldestKey = key;
                }
            }

            if (oldestKey) {
                this.queryCache.delete(oldestKey);
                this.cacheTimestamps.delete(oldestKey);
            }
        }
    }

    invalidateCache() {
        this.queryCache.clear();
        this.cacheTimestamps.clear();
    }

    async saveMemoryToHistory(memoryStore) {

        this.invalidateCache();
        return super.saveMemoryToHistory(memoryStore);
    }

    async close() {
        if (this.cleanupInterval) {
            clearInterval(this.cleanupInterval);
        }

        this.invalidateCache();
        return super.close();
    }
}

================
File: src/stores/InMemoryStore.js
================
import BaseStore from './BaseStore.js';
import { logger } from '../Utils.js';

export default class InMemoryStore extends BaseStore {
    constructor() {
        super();
        this.history = {
            shortTermMemory: [],
            longTermMemory: []
        };
    }

    async loadHistory() {
        logger.info('Loading history from in-memory storage');
        return [
            this.history.shortTermMemory || [],
            this.history.longTermMemory || []
        ];
    }

    async saveMemoryToHistory(memoryStore) {
        logger.info('Saving history to in-memory storage');

        this.history = {
            shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                id: item.id,
                prompt: item.prompt,
                output: item.output,
                embedding: Array.from(memoryStore.embeddings[idx].flat()),
                timestamp: memoryStore.timestamps[idx],
                accessCount: memoryStore.accessCounts[idx],
                concepts: Array.from(memoryStore.conceptsList[idx]),
                decayFactor: item.decayFactor || 1.0
            })),
            longTermMemory: [...memoryStore.longTermMemory]
        };

        logger.info(`Saved ${this.history.shortTermMemory.length} short-term and ${this.history.longTermMemory.length} long-term memories`);
    }
}

================
File: src/stores/JSONStore.js
================
import { promises as fs } from 'fs';
import { dirname, join } from 'path';
import BaseStore from './BaseStore.js';
import { logger } from '../Utils.js';

export default class JSONStore extends BaseStore {
    constructor(filePath = 'interaction_history.json') {
        super();
        this.filePath = filePath;
        this.tempPath = null;
        this.backupPath = `${filePath}.bak`;
        this.inTransaction = false;
    }

    async ensureDirectory() {
        const dir = dirname(this.filePath);
        await fs.mkdir(dir, { recursive: true });
    }

    async loadHistory() {
        try {
            await this.ensureDirectory();
            const exists = await fs.access(this.filePath).then(() => true).catch(() => false);

            if (!exists) {
                logger.info('No existing interaction history found in JSON. Starting fresh.');
                return [[], []];
            }


            try {
                logger.info('Loading existing interaction history from JSON...');
                const data = await fs.readFile(this.filePath, 'utf8');
                const history = JSON.parse(data);
                return [
                    history.shortTermMemory || [],
                    history.longTermMemory || []
                ];
            } catch (mainError) {

                logger.warn('Main file corrupted, attempting to load backup...');
                const backupExists = await fs.access(this.backupPath).then(() => true).catch(() => false);

                if (backupExists) {
                    const backupData = await fs.readFile(this.backupPath, 'utf8');
                    const history = JSON.parse(backupData);

                    await fs.copyFile(this.backupPath, this.filePath);
                    return [
                        history.shortTermMemory || [],
                        history.longTermMemory || []
                    ];
                }

                throw mainError;
            }
        } catch (error) {
            logger.error('Error loading history:', error);
            return [[], []];
        }
    }

    async beginTransaction() {
        if (this.inTransaction) {
            throw new Error('Transaction already in progress');
        }
        this.inTransaction = true;
        this.tempPath = `${this.filePath}.tmp`;
    }

    async commitTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        try {

            const exists = await fs.access(this.filePath).then(() => true).catch(() => false);
            if (exists) {
                await fs.copyFile(this.filePath, this.backupPath);
            }


            await fs.rename(this.tempPath, this.filePath);


            if (exists) {
                await fs.unlink(this.backupPath).catch(() => { });
            }
        } finally {
            this.inTransaction = false;
            this.tempPath = null;
        }
    }

    async rollbackTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        try {
            if (this.tempPath) {
                await fs.unlink(this.tempPath).catch(() => { });
            }
        } finally {
            this.inTransaction = false;
            this.tempPath = null;
        }
    }

    async verify() {
        try {
            const data = await fs.readFile(this.filePath, 'utf8');
            JSON.parse(data);
            return true;
        } catch {
            return false;
        }
    }

    async saveMemoryToHistory(memoryStore) {
        try {
            await this.ensureDirectory();
            await this.beginTransaction();

            const history = {
                shortTermMemory: memoryStore.shortTermMemory.map((item, idx) => ({
                    id: item.id,
                    prompt: item.prompt,
                    output: item.output,
                    embedding: Array.from(memoryStore.embeddings[idx]),
                    timestamp: memoryStore.timestamps[idx],
                    accessCount: memoryStore.accessCounts[idx],
                    concepts: Array.from(memoryStore.conceptsList[idx]),
                    decayFactor: item.decayFactor || 1.0
                })),
                longTermMemory: memoryStore.longTermMemory
            };


            await fs.writeFile(this.tempPath, JSON.stringify(history, null, 2));


            if (!await this.verify()) {
                throw new Error('Data verification failed');
            }


            await this.commitTransaction();

            logger.info(`Saved interaction history to JSON. Short-term: ${history.shortTermMemory.length}, Long-term: ${history.longTermMemory.length}`);
        } catch (error) {
            await this.rollbackTransaction();
            logger.error('Error saving history:', error);
            throw error;
        }
    }

    async close() {
        if (this.inTransaction) {
            await this.rollbackTransaction();
        }
        return Promise.resolve();
    }
}

================
File: src/stores/MemoryStore.js
================
import faiss from 'faiss-node';
import { createRequire } from 'module';
import { kmeans } from 'ml-kmeans';
import { logger, vectorOps } from '../Utils.js';

const require = createRequire(import.meta.url);
const { Graph } = require('graphology');

export default class MemoryStore {
    constructor(dimension = 1536) {
        this.dimension = dimension;
        this.initializeIndex();
        this.shortTermMemory = [];
        this.longTermMemory = [];
        this.embeddings = [];
        this.timestamps = [];
        this.accessCounts = [];
        this.conceptsList = [];
        this.graph = new Graph();
        this.semanticMemory = new Map();
        this.clusterLabels = [];
    }

    initializeIndex() {
        try {
            this.index = new faiss.IndexFlatL2(this.dimension);
            if (!this.index || !this.index.getDimension) {
                throw new Error('Failed to initialize FAISS index');
            }
            logger.info(`Initialized FAISS index with dimension ${this.dimension}`);
        } catch (error) {
            logger.error('FAISS index initialization failed:', error);
            throw new Error('Failed to initialize FAISS index: ' + error.message);
        }
    }

    validateEmbedding(embedding) {
        if (!Array.isArray(embedding)) {
            throw new TypeError('Embedding must be an array');
        }
        if (embedding.length !== this.dimension) {
            throw new Error(`Embedding dimension mismatch: expected ${this.dimension}, got ${embedding.length}`);
        }
        if (!embedding.every(x => typeof x === 'number' && !isNaN(x))) {
            throw new TypeError('Embedding must contain only valid numbers');
        }
    }

    addInteraction(interaction) {
        const { id, prompt, output, embedding, timestamp = Date.now(),
            accessCount = 1, concepts = [], decayFactor = 1.0 } = interaction;

        try {
            this.validateEmbedding(embedding);
            logger.info(`Adding interaction: '${prompt}'`);

            this.shortTermMemory.push({
                id, prompt, output, timestamp, accessCount, decayFactor
            });

            const embeddingArray = Float32Array.from(embedding);
            this.embeddings.push(embeddingArray);
            this.index.add(embedding);
            this.timestamps.push(timestamp);
            this.accessCounts.push(accessCount);
            this.conceptsList.push(new Set(concepts));

            this.updateGraph(new Set(concepts));
            this.clusterInteractions();
        } catch (error) {
            logger.error('Failed to add interaction:', error);
            throw error;
        }
    }

    updateGraph(concepts) {
        for (const concept of concepts) {
            if (!this.graph.hasNode(concept)) {
                this.graph.addNode(concept);
            }
        }

        for (const concept1 of concepts) {
            for (const concept2 of concepts) {
                if (concept1 !== concept2) {
                    const edgeKey = `${concept1}--${concept2}`;
                    if (this.graph.hasEdge(edgeKey)) {
                        const weight = this.graph.getEdgeAttribute(edgeKey, 'weight');
                        this.graph.setEdgeAttribute(edgeKey, 'weight', weight + 1);
                    } else {
                        this.graph.addEdge(concept1, concept2, { weight: 1 });
                    }
                }
            }
        }
    }

    classifyMemory() {
        this.shortTermMemory.forEach((interaction, idx) => {
            if (this.accessCounts[idx] > 10 &&
                !this.longTermMemory.some(ltm => ltm.id === interaction.id)) {
                this.longTermMemory.push(interaction);
                logger.info(`Moved interaction ${interaction.id} to long-term memory`);
            }
        });
    }

    async retrieve(queryEmbedding, queryConcepts, similarityThreshold = 40, excludeLastN = 0) {
        if (this.shortTermMemory.length === 0) {
            logger.info('No interactions available');
            return [];
        }

        logger.info('Retrieving relevant interactions...');
        const relevantInteractions = [];
        const currentTime = Date.now();
        const decayRate = 0.0001;
        const relevantIndices = new Set();

        const normalizedQuery = vectorOps.normalize(queryEmbedding.flat());
        const normalizedEmbeddings = this.embeddings.map(e => vectorOps.normalize(Array.from(e)));

        for (let idx = 0; idx < this.shortTermMemory.length - excludeLastN; idx++) {
            const similarity = vectorOps.cosineSimilarity(normalizedQuery, normalizedEmbeddings[idx]) * 100;
            const timeDiff = (currentTime - this.timestamps[idx]) / 1000;
            const decayFactor = this.shortTermMemory[idx].decayFactor * Math.exp(-decayRate * timeDiff);
            const reinforcementFactor = Math.log1p(this.accessCounts[idx]);
            const adjustedSimilarity = similarity * decayFactor * reinforcementFactor;

            if (adjustedSimilarity >= similarityThreshold) {
                relevantIndices.add(idx);
                this.accessCounts[idx]++;
                this.timestamps[idx] = currentTime;
                this.shortTermMemory[idx].decayFactor *= 1.1;

                relevantInteractions.push({
                    similarity: adjustedSimilarity,
                    interaction: this.shortTermMemory[idx],
                    concepts: this.conceptsList[idx]
                });
            }
        }


        this.shortTermMemory.forEach((item, idx) => {
            if (!relevantIndices.has(idx)) {
                item.decayFactor *= 0.9;
            }
        });

        const activatedConcepts = await this.spreadingActivation(queryConcepts);


        return this.combineResults(relevantInteractions, activatedConcepts, normalizedQuery);
    }

    async spreadingActivation(queryConcepts) {
        const activatedNodes = new Map();
        const initialActivation = 1.0;
        const decayFactor = 0.5;

        queryConcepts.forEach(concept => {
            activatedNodes.set(concept, initialActivation);
        });


        for (let step = 0; step < 2; step++) {
            const newActivations = new Map();

            for (const [node, activation] of activatedNodes) {
                if (this.graph.hasNode(node)) {
                    this.graph.forEachNeighbor(node, (neighbor, attributes) => {
                        if (!activatedNodes.has(neighbor)) {
                            const weight = attributes.weight;
                            const newActivation = activation * decayFactor * weight;
                            newActivations.set(neighbor,
                                (newActivations.get(neighbor) || 0) + newActivation);
                        }
                    });
                }
            }

            newActivations.forEach((value, key) => {
                activatedNodes.set(key, value);
            });
        }

        return Object.fromEntries(activatedNodes);
    }

    clusterInteractions() {
        if (this.embeddings.length < 2) return;

        const embeddingsMatrix = this.embeddings.map(e => Array.from(e));
        const numClusters = Math.min(10, this.embeddings.length);

        const { clusters } = kmeans(embeddingsMatrix, numClusters);
        this.clusterLabels = clusters;

        this.semanticMemory.clear();
        clusters.forEach((label, idx) => {
            if (!this.semanticMemory.has(label)) {
                this.semanticMemory.set(label, []);
            }
            this.semanticMemory.get(label).push({
                embedding: this.embeddings[idx],
                interaction: this.shortTermMemory[idx]
            });
        });
    }

    combineResults(relevantInteractions, activatedConcepts, normalizedQuery) {
        const combined = relevantInteractions.map(({ similarity, interaction, concepts }) => {
            const activationScore = Array.from(concepts)
                .reduce((sum, c) => sum + (activatedConcepts[c] || 0), 0);
            return {
                ...interaction,
                totalScore: similarity + activationScore
            };
        });

        combined.sort((a, b) => b.totalScore - a.totalScore);


        const semanticResults = this.retrieveFromSemanticMemory(normalizedQuery);
        return [...combined, ...semanticResults];
    }

    retrieveFromSemanticMemory(normalizedQuery) {
        if (this.semanticMemory.size === 0) return [];


        let bestCluster = -1;
        let bestSimilarity = -1;

        this.semanticMemory.forEach((items, label) => {
            const centroid = this.calculateCentroid(items.map(i => i.embedding));
            const similarity = vectorOps.cosineSimilarity(normalizedQuery, centroid);

            if (similarity > bestSimilarity) {
                bestSimilarity = similarity;
                bestCluster = label;
            }
        });

        if (bestCluster === -1) return [];


        return this.semanticMemory.get(bestCluster)
            .map(({ embedding, interaction }) => ({
                ...interaction,
                similarity: vectorOps.cosineSimilarity(normalizedQuery,
                    vectorOps.normalize(Array.from(embedding)))
            }))
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, 5);
    }

    calculateCentroid(embeddings) {
        const sum = embeddings.reduce((acc, curr) => {
            const arr = Array.from(curr);
            return acc.map((val, idx) => val + arr[idx]);
        }, new Array(this.dimension).fill(0));

        return sum.map(val => val / embeddings.length);
    }
}

================
File: src/stores/SPARQLStore.js
================
import BaseStore from './BaseStore.js';
import { logger } from '../Utils.js';

export default class SPARQLStore extends BaseStore {
    constructor(endpoint, options = {}) {
        super();
        this.endpoint = endpoint;
        this.credentials = {
            user: options.user || 'admin',
            password: options.password || 'admin'
        };
        this.graphName = options.graphName || 'http://example.org/mcp/memory';
        this.inTransaction = false;
        this.dimension = options.dimension || 1536;
    }

    async _executeSparqlQuery(query, endpoint) {
        const auth = Buffer.from(`${this.credentials.user}:${this.credentials.password}`).toString('base64');

        try {
            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Basic ${auth}`,
                    'Content-Type': 'application/sparql-query',
                    'Accept': 'application/json'
                },
                body: query,
                credentials: 'include'
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`SPARQL query failed: ${response.status} - ${errorText}`);
            }

            return await response.json();
        } catch (error) {
            logger.error('SPARQL query error:', error);
            throw error;
        }
    }

    async _executeSparqlUpdate(update, endpoint) {
        const auth = Buffer.from(`${this.credentials.user}:${this.credentials.password}`).toString('base64');

        try {
            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Basic ${auth}`,
                    'Content-Type': 'application/sparql-update',
                    'Accept': 'application/json'
                },
                body: update,
                credentials: 'include'
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`SPARQL update failed: ${response.status} - ${errorText}`);
            }

            return response;
        } catch (error) {
            logger.error('SPARQL update error:', error);
            throw error;
        }
    }

    validateEmbedding(embedding) {
        if (!Array.isArray(embedding)) {
            throw new TypeError('Embedding must be an array');
        }
        if (embedding.length !== this.dimension) {
            throw new Error(`Embedding dimension mismatch: expected ${this.dimension}, got ${embedding.length}`);
        }
        if (!embedding.every(x => typeof x === 'number' && !isNaN(x))) {
            throw new TypeError('Embedding must contain only valid numbers');
        }
    }

    async verify() {
        try {
            try {
                const createQuery = `
                    CREATE SILENT GRAPH <${this.graphName}>;
                    INSERT DATA { GRAPH <${this.graphName}> {
                        <${this.graphName}> a <http://example.org/mcp/MemoryStore>
                    }}
                `;
                await this._executeSparqlUpdate(createQuery, this.endpoint.update);
            } catch (error) {
                logger.debug('Graph creation skipped:', error.message);
            }

            const checkQuery = `ASK { GRAPH <${this.graphName}> { ?s ?p ?o } }`;
            const result = await this._executeSparqlQuery(checkQuery, this.endpoint.query);
            return result.boolean;
        } catch (error) {
            logger.error('Graph verification failed:', error);
            throw error;
        }
    }

    async loadHistory() {
        await this.verify();

        const query = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

            SELECT ?id ?prompt ?output ?embedding ?timestamp ?accessCount ?concepts ?decayFactor ?memoryType
            FROM <${this.graphName}>
            WHERE {
                ?interaction a mcp:Interaction ;
                    mcp:id ?id ;
                    mcp:prompt ?prompt ;
                    mcp:output ?output ;
                    mcp:embedding ?embedding ;
                    mcp:timestamp ?timestamp ;
                    mcp:accessCount ?accessCount ;
                    mcp:decayFactor ?decayFactor ;
                    mcp:memoryType ?memoryType .
                OPTIONAL { ?interaction mcp:concepts ?concepts }
            }`;

        try {
            const result = await this._executeSparqlQuery(query, this.endpoint.query);
            const shortTermMemory = [];
            const longTermMemory = [];

            for (const binding of result.results.bindings) {
                try {
                    let embedding = new Array(this.dimension).fill(0);
                    if (binding.embedding?.value && binding.embedding.value !== 'undefined') {
                        try {
                            embedding = JSON.parse(binding.embedding.value.trim());
                            this.validateEmbedding(embedding);
                        } catch (embeddingError) {
                            logger.error('Invalid embedding format:', embeddingError);
                        }
                    }

                    let concepts = [];
                    if (binding.concepts?.value && binding.concepts.value !== 'undefined') {
                        try {
                            concepts = JSON.parse(binding.concepts.value.trim());
                            if (!Array.isArray(concepts)) {
                                throw new Error('Concepts must be an array');
                            }
                        } catch (conceptsError) {
                            logger.error('Invalid concepts format:', conceptsError);
                        }
                    }

                    const interaction = {
                        id: binding.id.value,
                        prompt: binding.prompt.value,
                        output: binding.output.value,
                        embedding,
                        timestamp: parseInt(binding.timestamp.value) || Date.now(),
                        accessCount: parseInt(binding.accessCount.value) || 1,
                        concepts,
                        decayFactor: parseFloat(binding.decayFactor.value) || 1.0
                    };

                    if (binding.memoryType.value === 'short-term') {
                        shortTermMemory.push(interaction);
                    } else {
                        longTermMemory.push(interaction);
                    }
                } catch (parseError) {
                    logger.error('Failed to parse interaction:', parseError, binding);
                }
            }

            logger.info(`Loaded ${shortTermMemory.length} short-term and ${longTermMemory.length} long-term memories from store ${this.endpoint.query} graph <${this.graphName}>`);
            return [shortTermMemory, longTermMemory];
        } catch (error) {
            logger.error('Error loading history:', error);
            return [[], []];
        }
    }

    async saveMemoryToHistory(memoryStore) {
        if (this.inTransaction) {
            throw new Error('Transaction already in progress');
        }

        try {
            await this.verify();
            await this.beginTransaction();

            const clearQuery = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>
                CLEAR GRAPH <${this.graphName}>
            `;
            await this._executeSparqlUpdate(clearQuery, this.endpoint.update);

            const insertQuery = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>
                PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

                INSERT DATA {
                    GRAPH <${this.graphName}> {
                        ${this._generateInsertStatements(memoryStore.shortTermMemory, 'short-term')}
                        ${this._generateInsertStatements(memoryStore.longTermMemory, 'long-term')}
                    }
                }
            `;

            await this._executeSparqlUpdate(insertQuery, this.endpoint.update);
            await this.commitTransaction();

            logger.info(`Saved memory to SPARQL store ${this.endpoint.update} graph <${this.graphName}>. Stats: ${memoryStore.shortTermMemory.length} short-term, ${memoryStore.longTermMemory.length} long-term memories`);
        } catch (error) {
            await this.rollbackTransaction();
            logger.error('Error saving to SPARQL store:', error);
            throw error;
        }
    }

    _generateInsertStatements(memories, type) {
        return memories.map((interaction, index) => {

            let embeddingStr = '[]';
            if (Array.isArray(interaction.embedding)) {
                try {
                    this.validateEmbedding(interaction.embedding);
                    embeddingStr = JSON.stringify(interaction.embedding);
                } catch (error) {
                    logger.error('Invalid embedding in memory:', error);
                }
            }


            let conceptsStr = '[]';
            if (Array.isArray(interaction.concepts)) {
                conceptsStr = JSON.stringify(interaction.concepts);
            }

            return `
                _:interaction${type}${index} a mcp:Interaction ;
                    mcp:id "${interaction.id}" ;
                    mcp:prompt "${this._escapeSparqlString(interaction.prompt)}" ;
                    mcp:output "${this._escapeSparqlString(interaction.output)}" ;
                    mcp:embedding """${embeddingStr}""" ;
                    mcp:timestamp "${interaction.timestamp}"^^xsd:integer ;
                    mcp:accessCount "${interaction.accessCount}"^^xsd:integer ;
                    mcp:concepts """${conceptsStr}""" ;
                    mcp:decayFactor "${interaction.decayFactor}"^^xsd:decimal ;
                    mcp:memoryType "${type}" .
            `;
        }).join('\n');
    }

    _escapeSparqlString(str) {
        return str.replace(/["\\]/g, '\\$&').replace(/\n/g, '\\n');
    }

    async beginTransaction() {
        if (this.inTransaction) {
            throw new Error('Transaction already in progress');
        }

        this.inTransaction = true;

        const backupQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            COPY GRAPH <${this.graphName}> TO GRAPH <${this.graphName}.backup>
        `;
        await this._executeSparqlUpdate(backupQuery, this.endpoint.update);
    }

    async commitTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        try {
            const dropBackup = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>
                DROP SILENT GRAPH <${this.graphName}.backup>
            `;
            await this._executeSparqlUpdate(dropBackup, this.endpoint.update);
        } finally {
            this.inTransaction = false;
        }
    }

    async rollbackTransaction() {
        if (!this.inTransaction) {
            throw new Error('No transaction in progress');
        }

        try {
            const restoreQuery = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>
                DROP SILENT GRAPH <${this.graphName}> ;
                MOVE GRAPH <${this.graphName}.backup> TO GRAPH <${this.graphName}>
            `;
            await this._executeSparqlUpdate(restoreQuery, this.endpoint.update);
        } finally {
            this.inTransaction = false;
        }
    }

    async close() {
        if (this.inTransaction) {
            await this.rollbackTransaction();
        }
    }
}

================
File: src/utils/SPARQLHelpers.js
================
import { Buffer } from 'buffer';

export class SPARQLHelpers {
    static createAuthHeader(username, password) {
        return `Basic ${Buffer.from(`${username}:${password}`).toString('base64')}`;
    }

    static async executeSPARQLQuery(endpoint, query, auth, accept = 'application/json') {
        try {
            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Authorization': auth,
                    'Content-Type': 'application/sparql-query',
                    'Accept': accept
                },
                body: query
            });

            if (!response.ok) {
                const errorText = await response.text();
                const errorMessage = this.parseFusekiError(response.status, errorText);
                throw new Error(errorMessage);
            }

            return response;
        } catch (error) {
            if (error.name === 'TypeError') {
                throw new Error(`Connection failed to endpoint: ${endpoint}`);
            }
            throw error;
        }
    }

    static async executeSPARQLUpdate(endpoint, update, auth) {
        const response = await fetch(endpoint, {
            method: 'POST',
            headers: {
                'Authorization': auth,
                'Content-Type': 'application/sparql-update'
            },
            body: update
        });

        if (!response.ok) {
            throw new Error(`SPARQL update failed: ${response.status}`);
        }

        return response;
    }

    static async uploadTurtle(baseUrl, turtle, auth, graphUri) {

        const uploadUrl = baseUrl.endsWith('/') ? `${baseUrl}data` : `${baseUrl}/data`;
        const url = graphUri ? `${uploadUrl}?graph=${encodeURIComponent(graphUri)}` : uploadUrl;

        try {
            const response = await fetch(url, {
                method: 'POST',
                headers: {
                    'Authorization': auth,
                    'Content-Type': 'text/turtle',
                    'Accept': 'application/json'
                },
                body: turtle
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`Turtle upload failed: ${response.status} - ${errorText}`);
            }

            const result = await response.json();
            return {
                success: true,
                counts: {
                    triples: result.tripleCount || 0,
                    quads: result.quadCount || 0,
                    total: result.count || 0
                }
            };
        } catch (error) {
            throw new Error(`Upload failed: ${error.message}`);
        }
    }

    static parseFusekiError(status, errorText) {
        switch (status) {
            case 400:
                return `Invalid SPARQL syntax: ${errorText}`;
            case 401:
                return 'Authentication required';
            case 403:
                return 'Access forbidden - check credentials';
            case 404:
                return 'Dataset or endpoint not found';
            case 405:
                return 'Method not allowed - check endpoint URL';
            case 500:
                return `Fuseki server error: ${errorText}`;
            case 503:
                return 'Fuseki server unavailable';
            default:
                return `SPARQL operation failed (${status}): ${errorText}`;
        }
    }
}

================
File: src/Config.js
================
export default class Config {
    static defaults = {
        storage: {
            type: 'memory',
            options: {
                path: 'interaction_history.json',

                endpoint: 'http://localhost:8080',
                apiKey: '',
                timeout: 5000
            }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'qwen2:1.5b',
                options: {}
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text',
                options: {}
            }
        },
        memory: {
            dimension: 1536,
            similarityThreshold: 40,
            contextWindow: 3,
            decayRate: 0.0001
        },
        sparqlEndpoints: [
            {
                label: "tbox-test",
                user: "admin",
                password: "admin123",
                urlBase: "http://localhost:4030",
                upload: "/test/upload",
                gspRead: "/test/get",
                query: "/test/query",
                update: "/test/update"
            }
        ]

    };

    constructor(userConfig = {}) {
        this.config = this.mergeConfigs(Config.defaults, userConfig);
    }

    mergeConfigs(defaults, user) {
        const merged = { ...defaults };
        for (const [key, value] of Object.entries(user)) {
            if (value && typeof value === 'object') {
                merged[key] = this.mergeConfigs(defaults[key] || {}, value);
            } else {
                merged[key] = value;
            }
        }
        return merged;
    }

    get(path) {
        return path.split('.').reduce((obj, key) => obj && obj[key], this.config);
    }

    set(path, value) {
        const keys = path.split('.');
        const last = keys.pop();
        const target = keys.reduce((obj, key) => obj[key] = obj[key] || {}, this.config);
        target[last] = value;
    }
}

================
File: src/ContextManager.js
================
import ContextWindowManager from './ContextWindowManager.js'
import { logger } from './Utils.js';

export default class ContextManager {
    constructor(options = {}) {
        this.maxTokens = options.maxTokens || 8192;
        this.maxTimeWindow = options.maxTimeWindow || 24 * 60 * 60 * 1000;
        this.relevanceThreshold = options.relevanceThreshold || 0.7;
        this.maxContextSize = options.maxContextSize || 5;
        this.contextBuffer = [];

        this.windowManager = new ContextWindowManager({
            maxWindowSize: this.maxTokens,
            minWindowSize: Math.floor(this.maxTokens / 4),
            overlapRatio: options.overlapRatio || 0.1
        });
    }

    addToContext(interaction, similarity = 1.0) {
        this.contextBuffer.push({
            ...interaction,
            similarity,
            addedAt: Date.now()
        });


        if (this.contextBuffer.length > this.maxContextSize * 2) {
            this.pruneContext();
        }
    }

    pruneContext() {
        const now = Date.now();
        this.contextBuffer = this.contextBuffer
            .filter(item => {
                const age = now - item.addedAt;
                return age < this.maxTimeWindow && item.similarity >= this.relevanceThreshold;
            })
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, this.maxContextSize);
    }

    summarizeContext(interactions) {

        const groupedInteractions = {};

        for (const interaction of interactions) {
            const mainConcept = interaction.concepts?.[0] || 'general';
            if (!groupedInteractions[mainConcept]) {
                groupedInteractions[mainConcept] = [];
            }
            groupedInteractions[mainConcept].push(interaction);
        }


        const summaries = [];
        for (const [concept, group] of Object.entries(groupedInteractions)) {
            if (group.length === 1) {
                summaries.push(this.formatSingleInteraction(group[0]));
            } else {
                summaries.push(this.formatGroupSummary(concept, group));
            }
        }

        return summaries.join('\n\n');
    }

    formatSingleInteraction(interaction) {
        return `Q: ${interaction.prompt}\nA: ${interaction.output}`;
    }

    formatGroupSummary(concept, interactions) {
        const summary = `Topic: ${concept}\n` +
            interactions
                .slice(0, 3)
                .map(i => `- ${i.prompt} → ${i.output.substring(0, 50)}...`)
                .join('\n');
        return summary;
    }

    buildContext(currentPrompt, retrievals = [], recentInteractions = [], options = {}) {
        this.pruneContext();


        retrievals.forEach(retrieval => {
            this.addToContext(retrieval.interaction, retrieval.similarity);
        });


        recentInteractions.forEach(interaction => {
            this.addToContext(interaction, 0.9);
        });

        const contextParts = [];


        if (options.systemContext) {
            contextParts.push(`System Context: ${options.systemContext}`);
        }


        const historicalContext = this.summarizeContext(
            this.contextBuffer.slice(0, this.maxContextSize)
        );

        if (historicalContext) {
            contextParts.push('Relevant Context:', historicalContext);
        }

        const fullContext = contextParts.join('\n\n');


        if (this.windowManager.estimateTokens(fullContext) > this.maxTokens) {
            const windows = this.windowManager.processContext(fullContext);
            return this.windowManager.mergeOverlappingContent(windows);
        }

        return fullContext;
    }
}

================
File: src/ContextWindowManager.js
================
import { logger } from './Utils.js';

export default class ContextWindowManager {
    constructor(options = {}) {
        this.minWindowSize = options.minWindowSize || 1024;
        this.maxWindowSize = options.maxWindowSize || 8192;
        this.overlapRatio = options.overlapRatio || 0.1;
        this.avgTokenLength = options.avgTokenLength || 4;
    }


    estimateTokens(text) {
        return Math.ceil(text.length / this.avgTokenLength);
    }


    calculateWindowSize(input) {
        const estimatedTokens = this.estimateTokens(input);


        let windowSize = Math.min(
            this.maxWindowSize,
            Math.max(
                this.minWindowSize,
                estimatedTokens * 1.2
            )
        );

        logger.debug(`Calculated window size: ${windowSize} for input length: ${input.length}`);
        return windowSize;
    }


    createWindows(text, windowSize) {
        const windows = [];
        const overlapSize = Math.floor(windowSize * this.overlapRatio);
        const stride = windowSize - overlapSize;

        let position = 0;
        while (position < text.length) {
            const window = {
                text: text.slice(position, position + windowSize),
                start: position,
                end: Math.min(position + windowSize, text.length)
            };

            windows.push(window);
            position += stride;

            if (position + windowSize >= text.length) {

                if (position < text.length) {
                    windows.push({
                        text: text.slice(position),
                        start: position,
                        end: text.length
                    });
                }
                break;
            }
        }

        return windows;
    }


    mergeOverlappingContent(windows) {
        if (windows.length === 0) return '';
        if (windows.length === 1) return windows[0].text;

        let merged = windows[0].text;
        for (let i = 1; i < windows.length; i++) {
            const overlap = this._findBestOverlap(
                merged.slice(-this.maxWindowSize),
                windows[i].text
            );
            merged += windows[i].text.slice(overlap);
        }

        return merged;
    }


    _findBestOverlap(end, start, minOverlap = 10) {

        for (let overlap = Math.min(end.length, start.length); overlap >= minOverlap; overlap--) {
            const endSlice = end.slice(-overlap);
            const startSlice = start.slice(0, overlap);

            if (endSlice === startSlice) {
                return overlap;
            }
        }

        return 0;
    }


    processContext(context, options = {}) {
        const windowSize = this.calculateWindowSize(context);
        const windows = this.createWindows(context, windowSize);

        logger.debug(`Created ${windows.length} windows with size ${windowSize}`);


        if (options.includeMetadata) {
            return windows.map(window => ({
                ...window,
                tokenEstimate: this.estimateTokens(window.text)
            }));
        }

        return windows;
    }
}

================
File: src/MemoryManager.js
================
import { v4 as uuidv4 } from 'uuid';
import MemoryStore from './stores/MemoryStore.js';
import InMemoryStore from './stores/InMemoryStore.js';
import ContextManager from './ContextManager.js';
import PromptTemplates from './PromptTemplates.js';
import { logger } from './Utils.js';

export default class MemoryManager {
    constructor({
        llmProvider,
        chatModel = 'llama2',
        embeddingModel = 'nomic-embed-text',
        storage = null,
        dimension = 1536,
        contextOptions = {
            maxTokens: embeddingModel === 'nomic-embed-text' ? 8192 : 4096
        },
        cacheOptions = {
            maxSize: 1000,
            ttl: 3600000
        }
    }) {
        if (!llmProvider) {
            throw new Error('LLM provider is required');
        }

        this.llmProvider = llmProvider;
        this.chatModel = chatModel;
        this.embeddingModel = embeddingModel;
        this.dimension = dimension;
        this.cacheOptions = cacheOptions;


        this.embeddingCache = new Map();
        this.cacheTimestamps = new Map();

        try {
            this.memoryStore = new MemoryStore(this.dimension);
            this.storage = storage || new InMemoryStore();
            this.contextManager = new ContextManager(contextOptions);
        } catch (error) {
            logger.error('Failed to initialize MemoryManager:', error);
            throw new Error('Memory manager initialization failed: ' + error.message);
        }

        this.initialize();


        this.cleanupInterval = setInterval(() => {
            this.cleanupCache();
        }, cacheOptions.ttl / 2);
    }

    async initialize() {
        try {
            const [shortTerm, longTerm] = await this.storage.loadHistory();

            for (const interaction of shortTerm) {
                const embedding = this.standardizeEmbedding(interaction.embedding);
                interaction.embedding = embedding;
                this.memoryStore.addInteraction(interaction);
            }

            this.memoryStore.longTermMemory.push(...longTerm);
            this.memoryStore.clusterInteractions();

            logger.info(`Memory initialized with ${shortTerm.length} short-term and ${longTerm.length} long-term memories`);
        } catch (error) {
            logger.error('Memory initialization failed:', error);
            throw error;
        }
    }

    cleanupCache() {
        const now = Date.now();
        for (const [key, timestamp] of this.cacheTimestamps.entries()) {
            if (now - timestamp > this.cacheOptions.ttl) {
                this.embeddingCache.delete(key);
                this.cacheTimestamps.delete(key);
            }
        }


        while (this.embeddingCache.size > this.cacheOptions.maxSize) {
            let oldestKey = null;
            let oldestTime = Infinity;

            for (const [key, timestamp] of this.cacheTimestamps.entries()) {
                if (timestamp < oldestTime) {
                    oldestTime = timestamp;
                    oldestKey = key;
                }
            }

            if (oldestKey) {
                this.embeddingCache.delete(oldestKey);
                this.cacheTimestamps.delete(oldestKey);
            }
        }
    }

    getCacheKey(text) {

        return `${this.embeddingModel}:${text.slice(0, 100)}`;
    }

    async generateEmbedding(text) {
        const cacheKey = this.getCacheKey(text);


        if (this.embeddingCache.has(cacheKey)) {
            const cached = this.embeddingCache.get(cacheKey);

            this.cacheTimestamps.set(cacheKey, Date.now());
            return cached;
        }

        try {
            const embedding = await this.llmProvider.generateEmbedding(
                this.embeddingModel,
                text
            );


            this.embeddingCache.set(cacheKey, embedding);
            this.cacheTimestamps.set(cacheKey, Date.now());


            if (this.embeddingCache.size > this.cacheOptions.maxSize) {
                this.cleanupCache();
            }

            return embedding;
        } catch (error) {
            logger.error('Error generating embedding:', error);
            throw error;
        }
    }

    validateEmbedding(embedding) {
        if (!Array.isArray(embedding)) {
            throw new TypeError('Embedding must be an array');
        }
        if (!embedding.every(x => typeof x === 'number' && !isNaN(x))) {
            throw new TypeError('Embedding must contain only valid numbers');
        }
    }

    standardizeEmbedding(embedding) {
        this.validateEmbedding(embedding);
        const current = embedding.length;
        if (current === this.dimension) return embedding;

        if (current < this.dimension) {
            return [...embedding, ...new Array(this.dimension - current).fill(0)];
        }
        return embedding.slice(0, this.dimension);
    }

    async addInteraction(prompt, output, embedding, concepts) {
        try {
            this.validateEmbedding(embedding);
            const standardizedEmbedding = this.standardizeEmbedding(embedding);

            const interaction = {
                id: uuidv4(),
                prompt,
                output,
                embedding: standardizedEmbedding,
                timestamp: Date.now(),
                accessCount: 1,
                concepts,
                decayFactor: 1.0
            };

            this.memoryStore.addInteraction(interaction);
            await this.storage.saveMemoryToHistory(this.memoryStore);
        } catch (error) {
            logger.error('Failed to add interaction:', error);
            throw error;
        }
    }

    async retrieveRelevantInteractions(query, similarityThreshold = 40, excludeLastN = 0) {
        try {
            const queryEmbedding = await this.generateEmbedding(query);
            const queryConcepts = await this.extractConcepts(query);
            return this.memoryStore.retrieve(queryEmbedding, queryConcepts, similarityThreshold, excludeLastN);
        } catch (error) {
            logger.error('Failed to retrieve relevant interactions:', error);
            throw error;
        }
    }

    async extractConcepts(text) {
        logger.info('Extracting concepts...');
        try {
            const prompt = PromptTemplates.formatConceptPrompt(this.chatModel, text);
            const response = await this.llmProvider.generateCompletion(
                this.chatModel,
                prompt,
                { temperature: 0.2 }
            );

            const match = response.match(/\[.*\]/);
            if (match) {
                const concepts = JSON.parse(match[0]);
                logger.info('Extracted concepts:', concepts);
                return concepts;
            }

            logger.info('No concepts extracted, returning empty array');
            return [];
        } catch (error) {
            logger.error('Error extracting concepts:', error);
            return [];
        }
    }

    async generateResponse(prompt, lastInteractions = [], retrievals = [], contextWindow = 3) {
        const context = this.contextManager.buildContext(
            prompt,
            retrievals,
            lastInteractions,
            { systemContext: "You're a helpful assistant with memory of past interactions." }
        );

        try {
            const messages = PromptTemplates.formatChatPrompt(
                this.chatModel,
                "You're a helpful assistant with memory of past interactions.",
                context,
                prompt
            );

            const response = await this.llmProvider.generateChat(
                this.chatModel,
                messages,
                { temperature: 0.7 }
            );

            return response.trim();
        } catch (error) {
            logger.error('Error generating response:', error);
            throw error;
        }
    }


    async dispose() {
        logger.info('Starting MemoryManager shutdown...');


        if (this.cleanupInterval) {
            clearInterval(this.cleanupInterval);
        }


        try {
            await this.storage.saveMemoryToHistory(this.memoryStore);
            logger.info('Final memory state saved');
        } catch (error) {
            logger.error('Error saving final memory state:', error);
        }


        this.embeddingCache.clear();
        this.cacheTimestamps.clear();


        if (this.storage && typeof this.storage.close === 'function') {
            await this.storage.close();
        }


        this.memoryStore = null;
        this.llmProvider = null;

        logger.info('MemoryManager shutdown complete');
    }
}

================
File: src/OllamaExample.js
================
import MemoryManager from './MemoryManager.js';
import JSONStore from './stores/JSONStore.js';
import Config from './Config.js';
import OllamaConnector from './connectors/OllamaConnector.js';


let memoryManager = null;

async function shutdown(signal) {
    console.log(`\nReceived ${signal}, starting graceful shutdown...`);
    if (memoryManager) {
        try {
            await memoryManager.dispose();
            console.log('Cleanup complete');
            process.exit(0);
        } catch (error) {
            console.error('Error during cleanup:', error);
            process.exit(1);
        }
    } else {
        process.exit(0);
    }
}


process.on('SIGTERM', () => shutdown('SIGTERM'));
process.on('SIGINT', () => shutdown('SIGINT'));
process.on('uncaughtException', async (error) => {
    console.error('Uncaught Exception:', error);
    await shutdown('uncaughtException');
});
process.on('unhandledRejection', async (reason, promise) => {
    console.error('Unhandled Rejection at:', promise, 'reason:', reason);
    await shutdown('unhandledRejection');
});

async function main() {
    const config = new Config({
        storage: {
            type: 'json',
            options: {
                path: 'data/memory.json'
            }
        },
        models: {
            chat: {
                provider: 'ollama',
                model: 'qwen2:1.5b'
            },
            embedding: {
                provider: 'ollama',
                model: 'nomic-embed-text'
            }
        }
    });

    const storage = new JSONStore(config.get('storage.options.path'));
    const ollama = new OllamaConnector();

    memoryManager = new MemoryManager({
        llmProvider: ollama,
        chatModel: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.model'),
        storage
    });

    const prompt = "What's the current state of AI technology?";

    try {
        const relevantInteractions = await memoryManager.retrieveRelevantInteractions(prompt);
        const response = await memoryManager.generateResponse(prompt, [], relevantInteractions);
        console.log('Response:', response);

        const embedding = await memoryManager.generateEmbedding(`${prompt} ${response}`);
        const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
        await memoryManager.addInteraction(prompt, response, embedding, concepts);
    } catch (error) {
        console.error('Error during execution:', error);
        await shutdown('error');
    }
}


main().catch(async (error) => {
    console.error('Fatal error:', error);
    await shutdown('fatal error');
});

================
File: src/PromptTemplates.js
================
export default class PromptTemplates {
    static templates = {
        'llama2': {
            chat: (system, context, query) => {
                const messages = [{
                    role: 'system',
                    content: system
                }];

                if (context) {
                    messages.push({
                        role: 'user',
                        content: context
                    });
                    messages.push({
                        role: 'assistant',
                        content: 'I understand the context provided. How can I help with your query?'
                    });
                }

                messages.push({
                    role: 'user',
                    content: query
                });

                return messages;
            },
            completion: (context, query) => {
                return `[INST] ${context ? `Context:\n${context}\n\n` : ''}Query: ${query} [/INST]`;
            },
            extractConcepts: (text) => {
                return `[INST] Extract key concepts from the following text and return them as a JSON array of strings only. Example: ["concept1", "concept2"]. Text: "${text}" [/INST]`;
            }
        },

        'mistral': {
            chat: (system, context, query) => {
                const messages = [{
                    role: 'system',
                    content: system
                }];

                if (context) {
                    messages.push({
                        role: 'user',
                        content: `Previous Context:\n${context}`
                    });
                    messages.push({
                        role: 'assistant',
                        content: 'Context received. What would you like to know?'
                    });
                }

                messages.push({
                    role: 'user',
                    content: query
                });

                return messages;
            },
            completion: (context, query) => {
                return `<s>[INST] ${context ? `${context}\n\n` : ''}${query} [/INST]`;
            },
            extractConcepts: (text) => {
                return `<s>[INST] Extract and return only a JSON array of key concepts from: "${text}" [/INST]`;
            }
        }
    };

    static getTemplateForModel(modelName) {
        // Handle model name variants
        const baseModel = modelName.split(':')[0].toLowerCase();
        const modelFamily = baseModel.replace(/[\d.]/g, ''); // Remove version numbers
        return this.templates[modelFamily] || this.templates['llama2'];
    }

    static formatChatPrompt(modelName, system, context, query) {
        const template = this.getTemplateForModel(modelName);
        return template.chat(system, context, query);
    }

    static formatCompletionPrompt(modelName, context, query) {
        const template = this.getTemplateForModel(modelName);
        return template.completion(context, query);
    }

    static formatConceptPrompt(modelName, text) {
        const template = this.getTemplateForModel(modelName);
        return template.extractConcepts(text);
    }

    static registerTemplate(modelName, template) {
        if (!template.chat || !template.completion || !template.extractConcepts) {
            throw new Error('Template must implement chat, completion, and extractConcepts methods');
        }
        this.templates[modelName.toLowerCase()] = template;
    }
}

================
File: src/SPARQLExample.js
================
import Config from './Config.js';
import SPARQLStore from './stores/SPARQLStore.js';
import MemoryManager from './MemoryManager.js';
import OllamaConnector from './connectors/OllamaConnector.js';

async function main() {

    const config = new Config({
        storage: {
            type: 'sparql',
            options: {
                graphName: 'http://example.org/mcp/memory'
            }
        }
    });


    const sparqlConfig = config.get('sparqlEndpoints')[0];


    const store = new SPARQLStore({
        query: `${sparqlConfig.urlBase}${sparqlConfig.query}`,
        update: `${sparqlConfig.urlBase}${sparqlConfig.update}`
    }, {
        user: sparqlConfig.user,
        password: sparqlConfig.password,
        graphName: config.get('storage.options.graphName')
    });


    const ollama = new OllamaConnector();
    const memoryManager = new MemoryManager({
        llmProvider: ollama,
        chatModel: config.get('models.chat.model'),
        embeddingModel: config.get('models.embedding.model'),
        storage: store
    });


    const prompt = "How can Semantic Web technologies be used with AI?";
    try {
        const relevantInteractions = await memoryManager.retrieveRelevantInteractions(prompt);
        const response = await memoryManager.generateResponse(prompt, [], relevantInteractions);
        console.log('Response:', response);

        const embedding = await memoryManager.generateEmbedding(`${prompt} ${response}`);
        const concepts = await memoryManager.extractConcepts(`${prompt} ${response}`);
        await memoryManager.addInteraction(prompt, response, embedding, concepts);
    } catch (error) {
        console.error('Error:', error);
    } finally {
        await store.close();
    }
}

main().catch(console.error);

================
File: src/Utils.js
================
export const logger = {
    info: (...args) => console.log('[INFO]', ...args),
    error: (...args) => console.error('[ERROR]', ...args),
    debug: (...args) => console.debug('[DEBUG]', ...args)
};


export const vectorOps = {
    normalize: (vector) => {
        const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
        return vector.map(val => val / magnitude);
    },

    cosineSimilarity: (vec1, vec2) => {
        const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
        const mag1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));
        const mag2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));
        return dotProduct / (mag1 * mag2);
    }
};

================
File: tests/helpers/jasmine_examples/SpecHelper.js
================
beforeEach(function () {
  jasmine.addMatchers({
    toBePlaying: function () {
      return {
        compare: function (actual, expected) {
          const player = actual;

          return {
            pass: player.currentlyPlayingSong === expected && player.isPlaying
          };
        }
      };
    }
  });
});

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter';

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true
            }
        });
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments);
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments);
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments);
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments);
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments);
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments);
    }
}

export default CustomReporter;

================
File: tests/integration/llms/Ollama.spec.js
================
import OllamaConnector from '../../../../src/connectors/OllamaConnector.js';

describe('OllamaConnector Integration', () => {
    let api;

    beforeEach(() => {
        api = new OllamaConnector('http://localhost:11434');
    });

    it('should generate chat response', async () => {
        const messages = [{
            role: 'user',
            content: 'Hello, how are you?'
        }];

        const response = await api.generateChat('llama2', messages);
        expect(typeof response).toBe('string');
        expect(response.length).toBeGreaterThan(0);
    });

    it('should generate embeddings', async () => {
        const embedding = await api.generateEmbedding(
            'nomic-embed-text',
            'Test text for embedding'
        );

        expect(Array.isArray(embedding)).toBe(true);
        expect(embedding.length).toBe(1536);
    });

    it('should handle API errors gracefully', async () => {
        try {
            await api.generateChat('nonexistent-model', []);
            fail('Should have thrown an error');
        } catch (error) {
            expect(error.message).toContain('Ollama API error');
        }
    });
});

================
File: tests/integration/sparql/sparql-advanced-backup-spec.js
================
import Config from '../../../../src/Config.js';
import SPARQLStore from '../../../../src/stores/SPARQLStore.js';
import { logger } from '../../../../src/Utils.js';

describe('SPARQLStore Advanced Backup Integration', () => {
    let store;
    let config;
    const testGraph = 'http://example.org/mcp/test-backup-advanced';
    let originalData;

    beforeAll(async () => {
        config = new Config();
        const sparqlConfig = config.get('sparqlEndpoints')[0];

        store = new SPARQLStore({
            query: `${sparqlConfig.urlBase}${sparqlConfig.query}`,
            update: `${sparqlConfig.urlBase}${sparqlConfig.update}`
        }, {
            user: sparqlConfig.user,
            password: sparqlConfig.password,
            graphName: testGraph
        });


        originalData = {
            shortTermMemory: [{
                id: 'advanced-backup-1',
                prompt: 'advanced backup test',
                output: 'original output',
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['advanced', 'backup'],
                decayFactor: 1.0
            }],
            longTermMemory: []
        };


        try {
            await store.beginTransaction();
            const setupQuery = `
                DROP SILENT GRAPH <${testGraph}>;
                CREATE GRAPH <${testGraph}>
            `;
            await store._executeSparqlUpdate(setupQuery, store.endpoint.update);
            await store.commitTransaction();


            await store.saveMemoryToHistory(originalData);
        } catch (error) {
            logger.error('Error in advanced backup test setup:', error);
            throw error;
        }
    });

    afterAll(async () => {
        try {
            await store.beginTransaction();
            const cleanupQuery = `
                DROP SILENT GRAPH <${testGraph}>;
                DROP SILENT GRAPH <${testGraph}.backup>
            `;
            await store._executeSparqlUpdate(cleanupQuery, store.endpoint.update);
            await store.commitTransaction();
        } finally {
            await store.close();
        }
    });

    it('should handle backup corruption', async () => {
        await store.beginTransaction();


        const corruptQuery = `
            INSERT DATA {
                GRAPH <${testGraph}.backup> {
                    _:corrupt a mcp:Invalid ;
                        mcp:invalidProp "test" .
                }
            }
        `;
        await store._executeSparqlUpdate(corruptQuery, store.endpoint.update);


        const modifiedData = {
            shortTermMemory: [{
                ...originalData.shortTermMemory[0],
                output: 'corrupt test output'
            }],
            longTermMemory: []
        };


        try {
            await store.saveMemoryToHistory(modifiedData);
            fail('Should have detected corruption');
        } catch (error) {

            const [shortTerm] = await store.loadHistory();
            expect(shortTerm[0].output).toBe('original output');
        }

        await store.rollbackTransaction();
    });

    it('should perform incremental backups', async () => {
        await store.beginTransaction();


        const updates = [
            { id: 'incremental-1', output: 'first update' },
            { id: 'incremental-2', output: 'second update' }
        ];

        for (const update of updates) {
            const incrementalData = {
                shortTermMemory: [
                    ...originalData.shortTermMemory,
                    {
                        ...originalData.shortTermMemory[0],
                        ...update
                    }
                ],
                longTermMemory: []
            };

            await store.saveMemoryToHistory(incrementalData);


            const verifyQuery = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>
                ASK {
                    GRAPH <${testGraph}.backup> {
                        ?s mcp:id "${update.id}" ;
                           mcp:output "${update.output}" .
                    }
                }
            `;
            const result = await store._executeSparqlQuery(verifyQuery, store.endpoint.query);
            expect(result.boolean).toBe(true);
        }


        await store.rollbackTransaction();

        const [shortTerm] = await store.loadHistory();
        expect(shortTerm.length).toBe(1);
        expect(shortTerm[0].id).toBe('advanced-backup-1');
    });

    it('should handle concurrent backup operations', async () => {
        const store2 = new SPARQLStore({
            query: store.endpoint.query,
            update: store.endpoint.update
        }, {
            user: store.credentials.user,
            password: store.credentials.password,
            graphName: testGraph
        });

        await store.beginTransaction();


        await expectAsync(store2.beginTransaction())
            .toBeRejectedWithError(/Transaction already in progress/);

        await store.rollbackTransaction();
        await store2.close();
    });

    it('should verify backup integrity', async () => {
        await store.beginTransaction();


        const verifyQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            SELECT ?prop ?value
            WHERE {
                GRAPH <${testGraph}> {
                    ?s1 mcp:id "advanced-backup-1" ;
                        ?prop ?value .
                }
                GRAPH <${testGraph}.backup> {
                    ?s2 mcp:id "advanced-backup-1" ;
                        ?prop ?value2 .
                    FILTER(?value = ?value2)
                }
            }
        `;

        const results = await store._executeSparqlQuery(verifyQuery, store.endpoint.query);
        expect(results.results.bindings.length).toBeGreaterThan(0);

        await store.rollbackTransaction();
    });

    it('should handle large backup operations', async () => {
        await store.beginTransaction();


        const largeData = {
            shortTermMemory: Array(100).fill(null).map((_, i) => ({
                id: `large-backup-${i}`,
                prompt: `large backup test ${i}`,
                output: `test output ${i}`,
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['large', 'backup', `test-${i}`],
                decayFactor: 1.0
            })),
            longTermMemory: []
        };

        await store.saveMemoryToHistory(largeData);


        const countQuery = `
            SELECT (COUNT(?s) as ?count)
            WHERE {
                GRAPH <${testGraph}.backup> {
                    ?s a mcp:Interaction
                }
            }
        `;

        const results = await store._executeSparqlQuery(countQuery, store.endpoint.query);
        expect(parseInt(results.results.bindings[0].count.value)).toBe(100);

        await store.rollbackTransaction();
    });
});

================
File: tests/integration/sparql/sparql-basic-backup-spec.js
================
import Config from '../../../../src/Config.js';
import SPARQLStore from '../../../../src/stores/SPARQLStore.js';
import { logger } from '../../../../src/Utils.js';

describe('SPARQLStore Basic Backup Integration', () => {
    let store;
    let config;
    const testGraph = 'http://example.org/mcp/test-backup-basic';
    let originalData;

    beforeAll(async () => {
        config = new Config();
        const sparqlConfig = config.get('sparqlEndpoints')[0];

        store = new SPARQLStore({
            query: `${sparqlConfig.urlBase}${sparqlConfig.query}`,
            update: `${sparqlConfig.urlBase}${sparqlConfig.update}`
        }, {
            user: sparqlConfig.user,
            password: sparqlConfig.password,
            graphName: testGraph
        });


        originalData = {
            shortTermMemory: [{
                id: 'backup-test-1',
                prompt: 'backup test prompt',
                output: 'backup test output',
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['backup', 'test'],
                decayFactor: 1.0
            }],
            longTermMemory: []
        };


        try {
            await store.beginTransaction();
            const setupQuery = `
                DROP SILENT GRAPH <${testGraph}>;
                CREATE GRAPH <${testGraph}>
            `;
            await store._executeSparqlUpdate(setupQuery, store.endpoint.update);
            await store.commitTransaction();


            await store.saveMemoryToHistory(originalData);
        } catch (error) {
            logger.error('Error in backup test setup:', error);
            throw error;
        }
    });

    afterAll(async () => {
        try {
            await store.beginTransaction();
            const cleanupQuery = `
                DROP SILENT GRAPH <${testGraph}>;
                DROP SILENT GRAPH <${testGraph}.backup>
            `;
            await store._executeSparqlUpdate(cleanupQuery, store.endpoint.update);
            await store.commitTransaction();
        } finally {
            await store.close();
        }
    });

    it('should create backup during transaction', async () => {
        await store.beginTransaction();


        const verifyQuery = `
            ASK { GRAPH <${testGraph}.backup> { ?s ?p ?o } }
        `;
        const result = await store._executeSparqlQuery(verifyQuery, store.endpoint.query);
        expect(result.boolean).toBe(true);

        await store.commitTransaction();
    });

    it('should restore from backup on rollback', async () => {
        await store.beginTransaction();


        const modifiedData = {
            shortTermMemory: [{
                ...originalData.shortTermMemory[0],
                output: 'modified output'
            }],
            longTermMemory: []
        };

        await store.saveMemoryToHistory(modifiedData);


        let [shortTerm] = await store.loadHistory();
        expect(shortTerm[0].output).toBe('modified output');


        await store.rollbackTransaction();


        [shortTerm] = await store.loadHistory();
        expect(shortTerm[0].output).toBe('backup test output');
    });

    it('should cleanup backup graphs after commit', async () => {
        await store.beginTransaction();
        await store.commitTransaction();


        const verifyQuery = `
            ASK { GRAPH <${testGraph}.backup> { ?s ?p ?o } }
        `;
        const result = await store._executeSparqlQuery(verifyQuery, store.endpoint.query);
        expect(result.boolean).toBe(false);
    });

    it('should handle nested transaction attempts', async () => {
        await store.beginTransaction();

        await expectAsync(store.beginTransaction())
            .toBeRejectedWithError('Transaction already in progress');

        await store.rollbackTransaction();
    });

    it('should preserve backup during multiple operations', async () => {
        await store.beginTransaction();


        const modifications = [
            { output: 'first modification' },
            { output: 'second modification' },
            { output: 'third modification' }
        ];

        for (const mod of modifications) {
            const modData = {
                shortTermMemory: [{
                    ...originalData.shortTermMemory[0],
                    ...mod
                }],
                longTermMemory: []
            };
            await store.saveMemoryToHistory(modData);
        }


        await store.rollbackTransaction();

        const [shortTerm] = await store.loadHistory();
        expect(shortTerm[0].output).toBe('backup test output');
    });
});

================
File: tests/integration/sparql/sparql-federation-spec.js
================
import Config from '../../../../src/Config.js';
import SPARQLStore from '../../../../src/stores/SPARQLStore.js';
import { logger } from '../../../../src/Utils.js';

describe('SPARQLStore Federation Integration', () => {
    let store;
    let config;
    const testGraphs = {
        main: 'http://example.org/mcp/test-memory',
        metadata: 'http://example.org/mcp/test-metadata',
        archive: 'http://example.org/mcp/test-archive'
    };

    beforeAll(async () => {
        config = new Config();
        const sparqlConfig = config.get('sparqlEndpoints')[0];

        store = new SPARQLStore({
            query: `${sparqlConfig.urlBase}${sparqlConfig.query}`,
            update: `${sparqlConfig.urlBase}${sparqlConfig.update}`
        }, {
            user: sparqlConfig.user,
            password: sparqlConfig.password,
            graphName: testGraphs.main
        });


        try {
            await store.beginTransaction();
            const setupQuery = `
                DROP SILENT GRAPH <${testGraphs.main}>;
                DROP SILENT GRAPH <${testGraphs.metadata}>;
                DROP SILENT GRAPH <${testGraphs.archive}>;
                CREATE GRAPH <${testGraphs.main}>;
                CREATE GRAPH <${testGraphs.metadata}>;
                CREATE GRAPH <${testGraphs.archive}>
            `;
            await store._executeSparqlUpdate(setupQuery, store.endpoint.update);


            const metadataQuery = `
                INSERT DATA {
                    GRAPH <${testGraphs.metadata}> {
                        <${testGraphs.main}> a mcp:MemoryStore ;
                            mcp:hasVersion "1.0" ;
                            mcp:lastUpdated "${new Date().toISOString()}"^^xsd:dateTime .
                    }
                }
            `;
            await store._executeSparqlUpdate(metadataQuery, store.endpoint.update);
            await store.commitTransaction();
        } catch (error) {
            logger.error('Error in federation test setup:', error);
            throw error;
        }
    });

    afterAll(async () => {
        try {
            await store.beginTransaction();
            const cleanupQuery = `
                DROP SILENT GRAPH <${testGraphs.main}>;
                DROP SILENT GRAPH <${testGraphs.metadata}>;
                DROP SILENT GRAPH <${testGraphs.archive}>
            `;
            await store._executeSparqlUpdate(cleanupQuery, store.endpoint.update);
            await store.commitTransaction();
        } finally {
            await store.close();
        }
    });

    it('should query across multiple graphs', async () => {

        const testMemory = {
            shortTermMemory: [{
                id: 'federation-test-1',
                prompt: 'federation test prompt',
                output: 'federation test output',
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                concepts: ['federation', 'test'],
                accessCount: 1,
                decayFactor: 1.0
            }],
            longTermMemory: []
        };

        await store.saveMemoryToHistory(testMemory);


        const federatedQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            SELECT ?interaction ?version ?updated
            WHERE {
                GRAPH <${testGraphs.main}> {
                    ?interaction a mcp:Interaction ;
                        mcp:id "federation-test-1" .
                }
                GRAPH <${testGraphs.metadata}> {
                    <${testGraphs.main}> mcp:hasVersion ?version ;
                        mcp:lastUpdated ?updated .
                }
            }
        `;

        const results = await store._executeSparqlQuery(federatedQuery, store.endpoint.query);
        expect(results.results.bindings.length).toBe(1);
        expect(results.results.bindings[0].version.value).toBe('1.0');
    });

    it('should handle cross-graph data relationships', async () => {

        const setupQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            PREFIX qb: <http://purl.org/linked-data/cube#>

            INSERT DATA {
                GRAPH <${testGraphs.main}> {
                    _:interaction1 a mcp:Interaction ;
                        mcp:id "related-test-1" ;
                        mcp:relatedCube <cube1> .
                }

                GRAPH <${testGraphs.metadata}> {
                    <cube1> a qb:DataSet ;
                        qb:structure <dsd1> ;
                        rdfs:label "Test Cube" .
                }
            }
        `;

        await store._executeSparqlUpdate(setupQuery, store.endpoint.update);


        const relationQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            PREFIX qb: <http://purl.org/linked-data/cube#>

            SELECT ?id ?cubeLabel
            WHERE {
                GRAPH <${testGraphs.main}> {
                    ?interaction mcp:id ?id ;
                        mcp:relatedCube ?cube .
                }
                GRAPH <${testGraphs.metadata}> {
                    ?cube rdfs:label ?cubeLabel .
                }
            }
        `;

        const results = await store._executeSparqlQuery(relationQuery, store.endpoint.query);
        expect(results.results.bindings.length).toBe(1);
        expect(results.results.bindings[0].cubeLabel.value).toBe('Test Cube');
    });

    it('should support federated updates across graphs', async () => {
        await store.beginTransaction();
        try {
            const federatedUpdate = `
                PREFIX mcp: <http://purl.org/stuff/mcp/>

                WITH <${testGraphs.main}>
                DELETE { ?i mcp:accessCount ?oldCount }
                INSERT { ?i mcp:accessCount ?newCount }
                WHERE {
                    ?i mcp:id "federation-test-1" ;
                       mcp:accessCount ?oldCount .
                    BIND(?oldCount + 1 AS ?newCount)
                };

                WITH <${testGraphs.metadata}>
                DELETE { <${testGraphs.main}> mcp:lastUpdated ?old }
                INSERT { <${testGraphs.main}> mcp:lastUpdated "${new Date().toISOString()}"^^xsd:dateTime }
                WHERE {
                    <${testGraphs.main}> mcp:lastUpdated ?old
                }
            `;

            await store._executeSparqlUpdate(federatedUpdate, store.endpoint.update);
            await store.commitTransaction();


            const [shortTerm] = await store.loadHistory();
            expect(shortTerm[0].accessCount).toBe(2);
        } catch (error) {
            await store.rollbackTransaction();
            throw error;
        }
    });

    it('should handle service-based federation', async () => {

        const serviceQuery = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>

            SELECT ?interaction ?metadata
            WHERE {
                SERVICE <${store.endpoint.query}> {
                    GRAPH <${testGraphs.main}> {
                        ?interaction mcp:id "federation-test-1"
                    }
                }
                SERVICE <${store.endpoint.query}> {
                    GRAPH <${testGraphs.metadata}> {
                        <${testGraphs.main}> ?p ?metadata
                    }
                }
            }
        `;

        const results = await store._executeSparqlQuery(serviceQuery, store.endpoint.query);
        expect(results.results.bindings.length).toBeGreaterThan(0);
    });
});

================
File: tests/integration/sparql/sparql-store-integration-spec.js
================
import Config from '../../../../src/Config.js';
import SPARQLStore from '../../../../src/stores/SPARQLStore.js';
import { logger } from '../../../../src/Utils.js';

describe('SPARQLStore Integration', () => {
    let store;
    let config;
    let testMemory;

    beforeAll(async () => {

        config = new Config();
        const sparqlConfig = config.get('sparqlEndpoints')[0];

        store = new SPARQLStore({
            query: `${sparqlConfig.urlBase}${sparqlConfig.query}`,
            update: `${sparqlConfig.urlBase}${sparqlConfig.update}`
        }, {
            user: sparqlConfig.user,
            password: sparqlConfig.password,
            graphName: 'http://example.org/mcp/test-memory'
        });


        testMemory = {
            shortTermMemory: [{
                id: 'test-integration-1',
                prompt: 'integration test prompt',
                output: 'integration test output',
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['test', 'integration'],
                decayFactor: 1.0
            }],
            longTermMemory: []
        };


        try {
            await store.beginTransaction();
            const clearQuery = `
                DROP SILENT GRAPH <http://example.org/mcp/test-memory>;
                CREATE GRAPH <http://example.org/mcp/test-memory>
            `;
            await store._executeSparqlUpdate(clearQuery, `${sparqlConfig.urlBase}${sparqlConfig.update}`);
            await store.commitTransaction();
        } catch (error) {
            logger.error('Error in test setup:', error);
            throw error;
        }
    });

    afterAll(async () => {

        try {
            await store.beginTransaction();
            const dropQuery = `DROP SILENT GRAPH <http://example.org/mcp/test-memory>`;
            await store._executeSparqlUpdate(dropQuery, `${config.get('sparqlEndpoints')[0].urlBase}${config.get('sparqlEndpoints')[0].update}`);
            await store.commitTransaction();
        } finally {
            await store.close();
        }
    });

    it('should verify empty graph exists', async () => {
        const exists = await store.verify();
        expect(exists).toBe(true);
    });

    it('should save and load memory data', async () => {

        await store.saveMemoryToHistory(testMemory);


        const [shortTerm, longTerm] = await store.loadHistory();

        expect(shortTerm.length).toBe(1);
        expect(longTerm.length).toBe(0);

        const loaded = shortTerm[0];
        expect(loaded.id).toBe(testMemory.shortTermMemory[0].id);
        expect(loaded.prompt).toBe(testMemory.shortTermMemory[0].prompt);
        expect(loaded.concepts).toEqual(testMemory.shortTermMemory[0].concepts);
        expect(loaded.embedding.length).toBe(1536);
    });

    it('should handle transaction rollback', async () => {
        await store.beginTransaction();

        const badMemory = {
            shortTermMemory: [{
                id: 'test-rollback',
                prompt: 'should not persist',
                output: 'rollback test',
                embedding: new Array(1536).fill(0),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['rollback'],
                decayFactor: 1.0
            }],
            longTermMemory: []
        };

        try {

            await store.saveMemoryToHistory(badMemory);

            throw new Error('Test rollback');
        } catch (error) {
            await store.rollbackTransaction();
        }


        const [shortTerm] = await store.loadHistory();
        expect(shortTerm.length).toBe(1);
        expect(shortTerm[0].id).toBe('test-integration-1');
    });

    it('should handle concurrent transactions', async () => {
        const store2 = new SPARQLStore(store.endpoint, {
            user: store.credentials.user,
            password: store.credentials.password,
            graphName: store.graphName
        });

        await store.beginTransaction();


        await expectAsync(store2.beginTransaction())
            .toBeRejectedWithError(/Transaction already in progress/);

        await store.commitTransaction();
        await store2.close();
    });

    it('should support query pagination', async () => {

        const bulkMemory = {
            shortTermMemory: Array(5).fill(null).map((_, i) => ({
                id: `bulk-test-${i}`,
                prompt: `bulk test prompt ${i}`,
                output: `bulk test output ${i}`,
                embedding: new Array(1536).fill(0).map(() => Math.random()),
                timestamp: Date.now(),
                accessCount: 1,
                concepts: ['bulk', `test-${i}`],
                decayFactor: 1.0
            })),
            longTermMemory: []
        };

        await store.saveMemoryToHistory(bulkMemory);


        const pageSize = 2;
        const query = `
            PREFIX mcp: <http://purl.org/stuff/mcp/>
            SELECT ?id ?prompt
            FROM <${store.graphName}>
            WHERE {
                ?s mcp:id ?id ;
                   mcp:prompt ?prompt .
            }
            LIMIT ${pageSize}
        `;

        const results = await store._executeSparqlQuery(query, store.endpoint.query);
        expect(results.results.bindings.length).toBe(pageSize);
    });
});

================
File: tests/mocks/Ollama.js
================
export class MockOllamaConnector {
    async generateEmbedding(model, input) {
        return new Array(1536).fill(0).map(() => Math.random());
    }

    async generateChat(model, messages) {
        return 'Mock response';
    }

    async generateCompletion(model, prompt) {
        return 'Mock completion';
    }
}

================
File: tests/support/jasmine.json
================
{
  "spec_dir": "spec",
  "spec_files": [
    "**/*[sS]pec.?(m)js"
  ],
  "helpers": [
    "helpers/**/*.?(m)js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "random": true
  }
}

================
File: tests/unit/cached-sparql-store-spec.js
================
import CachedSPARQLStore from '../../src/stores/CachedSPARQLStore.js';

describe('CachedSPARQLStore', () => {
    let store;
    let mockFetch;
    let originalSetInterval;
    let mockSetInterval;

    const endpoint = {
        query: 'http://example.org/sparql/query',
        update: 'http://example.org/sparql/update'
    };

    beforeEach(() => {

        originalSetInterval = global.setInterval;
        mockSetInterval = jasmine.createSpy('setInterval').and.returnValue(123);
        global.setInterval = mockSetInterval;


        mockFetch = jasmine.createSpy('fetch').and.returnValue(
            Promise.resolve({
                ok: true,
                json: () => Promise.resolve({
                    results: { bindings: [] }
                })
            })
        );
        global.fetch = mockFetch;
        global.Buffer = {
            from: (str) => ({ toString: () => 'mock-base64' })
        };

        store = new CachedSPARQLStore(endpoint, {
            user: 'testuser',
            password: 'testpass',
            graphName: 'http://test.org/memory',
            cacheTTL: 1000,
            maxCacheSize: 2
        });

        jasmine.clock().install();
    });

    afterEach(() => {
        delete global.fetch;
        delete global.Buffer;
        global.setInterval = originalSetInterval;
        jasmine.clock().uninstall();
    });

    describe('cache operations', () => {
        it('should cache query results', async () => {
            const query = 'SELECT * WHERE { ?s ?p ?o }';
            const mockResult = { results: { bindings: [{ s: { value: 'test' } }] } };

            mockFetch.and.returnValue(
                Promise.resolve({
                    ok: true,
                    json: () => Promise.resolve(mockResult)
                })
            );


            await store._executeSparqlQuery(query, endpoint.query);
            expect(mockFetch).toHaveBeenCalledTimes(1);


            mockFetch.calls.reset();
            const cachedResult = await store._executeSparqlQuery(query, endpoint.query);
            expect(mockFetch).not.toHaveBeenCalled();
            expect(cachedResult).toEqual(mockResult);
        });

        it('should expire cache entries after TTL', async () => {
            const query = 'SELECT * WHERE { ?s ?p ?o }';


            await store._executeSparqlQuery(query, endpoint.query);
            expect(mockFetch).toHaveBeenCalledTimes(1);


            jasmine.clock().tick(1001);


            mockFetch.calls.reset();
            await store._executeSparqlQuery(query, endpoint.query);
            expect(mockFetch).toHaveBeenCalledTimes(1);
        });

        it('should respect max cache size', async () => {

            await store._executeSparqlQuery('query1', endpoint.query);
            await store._executeSparqlQuery('query2', endpoint.query);
            await store._executeSparqlQuery('query3', endpoint.query);

            expect(store.queryCache.size).toBeLessThanOrEqual(2);
        });

        it('should invalidate cache on data updates', async () => {

            await store._executeSparqlQuery('SELECT * WHERE { ?s ?p ?o }', endpoint.query);
            expect(store.queryCache.size).toBe(1);


            await store.saveMemoryToHistory({ shortTermMemory: [], longTermMemory: [] });
            expect(store.queryCache.size).toBe(0);
        });
    });

    describe('cache cleanup', () => {
        it('should remove expired entries during cleanup', () => {

            store.queryCache.set('test1', { data: 1 });
            store.cacheTimestamps.set('test1', Date.now() - 2000);
            store.queryCache.set('test2', { data: 2 });
            store.cacheTimestamps.set('test2', Date.now());

            store.cleanupCache();

            expect(store.queryCache.has('test1')).toBeFalse();
            expect(store.queryCache.has('test2')).toBeTrue();
        });

        it('should remove oldest entries when over size limit', () => {

            store.queryCache.set('test1', { data: 1 });
            store.cacheTimestamps.set('test1', 1000);
            store.queryCache.set('test2', { data: 2 });
            store.cacheTimestamps.set('test2', 2000);
            store.queryCache.set('test3', { data: 3 });
            store.cacheTimestamps.set('test3', 3000);

            store.cleanupCache();

            expect(store.queryCache.size).toBe(2);
            expect(store.queryCache.has('test1')).toBeFalse();
            expect(store.queryCache.has('test3')).toBeTrue();
        });
    });

    describe('cleanup on close', () => {
        it('should clear interval and cache on close', async () => {
            const mockClearInterval = jasmine.createSpy('clearInterval');
            global.clearInterval = mockClearInterval;

            await store.close();

            expect(mockClearInterval).toHaveBeenCalledWith(123);
            expect(store.queryCache.size).toBe(0);
            expect(store.cacheTimestamps.size).toBe(0);
        });
    });
});

================
File: tests/unit/ContextWindowManager.spec.js
================
import ContextWindowManager from '../../src/ContextWindowManager.js';

describe('ContextWindowManager', () => {
    let windowManager;

    beforeEach(() => {
        windowManager = new ContextWindowManager({
            maxWindowSize: 1000,
            minWindowSize: 250,
            overlapRatio: 0.1
        });
    });

    it('should calculate correct window size', () => {
        const size = windowManager.calculateWindowSize('x'.repeat(1000));
        expect(size).toBeLessThanOrEqual(1000);
        expect(size).toBeGreaterThanOrEqual(250);
    });

    it('should create overlapping windows', () => {
        const text = 'x'.repeat(2000);
        const windows = windowManager.createWindows(text, 1000);
        expect(windows.length).toBeGreaterThan(1);
        expect(windows[0].text.length).toBeLessThanOrEqual(1000);
    });

    it('should merge overlapping content', () => {
        const windows = [
            { text: 'Hello world' },
            { text: 'world and universe' }
        ];
        const merged = windowManager.mergeOverlappingContent(windows);
        expect(merged).toBe('Hello world and universe');
    });
});

================
File: tests/unit/MemoryManager.spec.js
================
import MemoryManager from '../../src/MemoryManager.js';
import { MockOllamaConnector } from '../mocks/Ollama.js';
import InMemoryStore from '../../src/stores/InMemoryStore.js';

describe('MemoryManager', () => {
    let manager;
    let mockOllama;

    beforeEach(() => {
        mockOllama = new MockOllamaConnector();
        manager = new MemoryManager({
            llmProvider: mockOllama,
            chatModel: 'qwen2:1.5b',
            embeddingModel: 'nomic-embed-text',
            storage: new InMemoryStorage()
        });
    });

    it('should generate embeddings', async () => {
        const embedding = await manager.getEmbedding('test text');
        expect(embedding.length).toBe(1536);
        expect(Array.isArray(embedding)).toBe(true);
    });

    it('should extract concepts', async () => {
        const concepts = await manager.extractConcepts('AI and machine learning');
        expect(Array.isArray(concepts)).toBe(true);
        expect(concepts.length).toBeGreaterThan(0);
    });

    it('should add and retrieve interactions', async () => {
        const prompt = 'test prompt';
        const response = 'test response';
        const embedding = new Array(1536).fill(0);
        const concepts = ['test'];

        await manager.addInteraction(prompt, response, embedding, concepts);
        const retrievals = await manager.retrieveRelevantInteractions(prompt);

        expect(retrievals.length).toBeGreaterThan(0);
        expect(retrievals[0].interaction.prompt).toBe(prompt);
    });
});

================
File: tests/unit/sparql-endpoint-spec.js
================
import Config from '../../src/Config.js';
import { SPARQLHelpers } from '../../src/utils/SPARQLHelpers.js';

describe('SPARQL Endpoint Integration', () => {
    let config;
    let endpoint;
    let auth;
    let baseUrl;
    const testGraph = 'http://example.org/test-graph';

    beforeAll(() => {
        config = new Config();
        const sparqlConfig = config.get('sparqlEndpoints')[0];

        baseUrl = `${sparqlConfig.urlBase}/test`;
        endpoint = {
            query: `${baseUrl}/query`,
            update: `${baseUrl}/update`
        };
        auth = SPARQLHelpers.createAuthHeader(sparqlConfig.user, sparqlConfig.password);
    });

    beforeEach(async () => {

        const clearQuery = `
            DROP SILENT GRAPH <${testGraph}>;
            CREATE GRAPH <${testGraph}>
        `;
        await SPARQLHelpers.executeSPARQLUpdate(endpoint.update, clearQuery, auth);
    });

    afterAll(async () => {

        const dropQuery = `DROP SILENT GRAPH <${testGraph}>`;
        await SPARQLHelpers.executeSPARQLUpdate(endpoint.update, dropQuery, auth);
    });

    describe('SPARQL UPDATE operations', () => {
        it('should insert data into graph', async () => {
            const insertQuery = `
                PREFIX ex: <http://example.org/>
                INSERT DATA {
                    GRAPH <${testGraph}> {
                        ex:subject ex:predicate "test object" .
                    }
                }
            `;

            await expectAsync(
                SPARQLHelpers.executeSPARQLUpdate(endpoint.update, insertQuery, auth)
            ).toBeResolved();
        });

        it('should delete data from graph', async () => {
            const deleteQuery = `
                PREFIX ex: <http://example.org/>
                DELETE DATA {
                    GRAPH <${testGraph}> {
                        ex:subject ex:predicate "test object" .
                    }
                }
            `;

            await expectAsync(
                SPARQLHelpers.executeSPARQLUpdate(endpoint.update, deleteQuery, auth)
            ).toBeResolved();
        });
    });

    describe('SPARQL SELECT operations', () => {
        beforeEach(async () => {

            const setupQuery = `
                PREFIX ex: <http://example.org/>
                INSERT DATA {
                    GRAPH <${testGraph}> {
                        ex:subject1 ex:predicate "value1" .
                        ex:subject2 ex:predicate "value2" .
                    }
                }
            `;
            await SPARQLHelpers.executeSPARQLUpdate(endpoint.update, setupQuery, auth);
        });

        it('should retrieve data with SELECT query', async () => {
            const selectQuery = `
                PREFIX ex: <http://example.org/>
                SELECT ?s ?o
                FROM <${testGraph}>
                WHERE {
                    ?s ex:predicate ?o .
                }
            `;

            const response = await SPARQLHelpers.executeSPARQLQuery(endpoint.query, selectQuery, auth);
            const data = await response.json();
            expect(data.results.bindings.length).toBe(2);
        });
    });

    describe('Turtle operations', () => {
        const testTurtle = `
            @prefix ex: <http://example.org/> .
            ex:subject ex:predicate "test value" .
        `;

        it('should upload Turtle data and return counts', async () => {
            const result = await SPARQLHelpers.uploadTurtle(baseUrl, testTurtle, auth, testGraph);

            expect(result.success).toBe(true);
            expect(result.counts.triples).toBe(1);
            expect(result.counts.total).toBe(1);


            const verifyQuery = `
                ASK FROM <${testGraph}>
                WHERE {
                    ?s ?p "test value"
                }
            `;
            const askResponse = await SPARQLHelpers.executeSPARQLQuery(endpoint.query, verifyQuery, auth);
            const askResult = await askResponse.json();
            expect(askResult.boolean).toBe(true);
        });

        it('should retrieve data as Turtle using CONSTRUCT', async () => {

            const insertQuery = `
                PREFIX ex: <http://example.org/>
                INSERT DATA {
                    GRAPH <${testGraph}> {
                        ex:subject ex:predicate "test value" .
                    }
                }
            `;
            await SPARQLHelpers.executeSPARQLUpdate(endpoint.update, insertQuery, auth);

            const constructQuery = `
                CONSTRUCT {
                    ?s ?p ?o
                }
                FROM <${testGraph}>
                WHERE {
                    ?s ?p ?o
                }
            `;

            const constructResponse = await SPARQLHelpers.executeSPARQLQuery(
                endpoint.query,
                constructQuery,
                auth,
                'text/turtle'
            );

            const turtle = await constructResponse.text();
            expect(turtle).toContain('http://example.org/subject');
        });
    });

    describe('Server interaction', () => {
        it('should handle authentication (note: auth currently not enforced)', async () => {
            const invalidAuth = SPARQLHelpers.createAuthHeader('invalid', 'credentials');
            const query = 'SELECT * WHERE { ?s ?p ?o } LIMIT 1';


            const queryResponse = await SPARQLHelpers.executeSPARQLQuery(endpoint.query, query, invalidAuth);
            const data = await queryResponse.json();
            expect(data.results.bindings.length).toBeGreaterThanOrEqual(0);
        });
    });
});

================
File: tests/unit/sparql-store-spec.js
================
import SPARQLStore from '../../src/stores/SPARQLStore.js';

describe('SPARQLStore', () => {
    let store;
    let mockFetch;

    const endpoint = {
        query: 'http://example.org/sparql/query',
        update: 'http://example.org/sparql/update'
    };

    beforeEach(() => {

        mockFetch = jasmine.createSpy('fetch').and.returnValue(
            Promise.resolve({
                ok: true,
                json: () => Promise.resolve({
                    results: {
                        bindings: [{
                            id: { value: 'test-id' },
                            prompt: { value: 'test prompt' },
                            output: { value: 'test output' },
                            embedding: { value: '[0,1,2]' },
                            timestamp: { value: '1234567890' },
                            accessCount: { value: '1' },
                            concepts: { value: '["test"]' },
                            decayFactor: { value: '1.0' },
                            memoryType: { value: 'short-term' }
                        }]
                    }
                })
            })
        );
        global.fetch = mockFetch;
        global.Buffer = {
            from: (str) => ({ toString: () => 'mock-base64' })
        };

        store = new SPARQLStore(endpoint, {
            user: 'testuser',
            password: 'testpass',
            graphName: 'http://test.org/memory'
        });
    });

    afterEach(() => {
        delete global.fetch;
        delete global.Buffer;
    });

    describe('loadHistory', () => {
        it('should load and parse memory data correctly', async () => {
            const [shortTerm, longTerm] = await store.loadHistory();

            expect(shortTerm.length).toBe(1);
            expect(longTerm.length).toBe(0);

            const memory = shortTerm[0];
            expect(memory.id).toBe('test-id');
            expect(memory.prompt).toBe('test prompt');
            expect(memory.embedding).toEqual([0,1,2]);
            expect(memory.timestamp).toBe(1234567890);
            expect(memory.concepts).toEqual(['test']);
        });

        it('should handle query errors', async () => {
            mockFetch.and.returnValue(Promise.resolve({ ok: false, status: 500 }));

            await expectAsync(store.loadHistory())
                .toBeRejectedWithError('SPARQL query failed: 500');
        });
    });

    describe('saveMemoryToHistory', () => {
        const mockMemoryStore = {
            shortTermMemory: [{
                id: 'test-id',
                prompt: 'test prompt',
                output: 'test output',
                embedding: [0,1,2],
                timestamp: 1234567890,
                accessCount: 1,
                concepts: ['test'],
                decayFactor: 1.0
            }],
            longTermMemory: []
        };

        it('should save memory data correctly', async () => {
            await store.saveMemoryToHistory(mockMemoryStore);

            expect(mockFetch).toHaveBeenCalledWith(
                endpoint.update,
                jasmine.objectContaining({
                    method: 'POST',
                    headers: jasmine.objectContaining({
                        'Content-Type': 'application/sparql-update'
                    })
                })
            );
        });

        it('should handle update errors', async () => {
            mockFetch.and.returnValue(Promise.resolve({ ok: false, status: 500 }));

            await expectAsync(store.saveMemoryToHistory(mockMemoryStore))
                .toBeRejectedWithError('SPARQL update failed: 500');
        });
    });

    describe('transaction handling', () => {
        it('should manage transactions correctly', async () => {
            await store.beginTransaction();
            expect(store.inTransaction).toBeTrue();

            await store.commitTransaction();
            expect(store.inTransaction).toBeFalse();
        });

        it('should handle transaction rollback', async () => {
            await store.beginTransaction();
            await store.rollbackTransaction();
            expect(store.inTransaction).toBeFalse();
        });

        it('should prevent nested transactions', async () => {
            await store.beginTransaction();
            await expectAsync(store.beginTransaction())
                .toBeRejectedWithError('Transaction already in progress');
        });
    });

    describe('verify', () => {
        it('should verify graph existence', async () => {
            mockFetch.and.returnValue(
                Promise.resolve({
                    ok: true,
                    json: () => Promise.resolve({ boolean: true })
                })
            );

            const isValid = await store.verify();
            expect(isValid).toBeTrue();
        });

        it('should handle verification failures', async () => {
            mockFetch.and.returnValue(Promise.resolve({ ok: false }));

            const isValid = await store.verify();
            expect(isValid).toBeFalse();
        });
    });

    describe('cleanup', () => {
        it('should clean up transaction state on close', async () => {
            await store.beginTransaction();
            await store.close();
            expect(store.inTransaction).toBeFalse();
        });
    });
});

================
File: .git
================
gitdir: ../../.git/modules/packages/semem

================
File: .gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: about.md
================
# About

Needs a SPARQL endpoint - like #:tbox

```sh
cd ~/github-danny/hyperdata/packages/tbox/

 docker-compose up -d
```

```sh
cd ~/github-danny/hyperdata/packages/semem

 node src/SPARQLExample.js
 ...
node src/OllamaExample.js

```

```sh
# ollama pull nomic-embed-text

curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "The sky is blue because of Rayleigh scattering"
}'
```

```sh
npm test -- --filter="SPARQL Endpoint Integration"
```

================
File: jasmine.json
================
{
    "spec_dir": "tests",
    "spec_files": [
        "**/*[sS]pec.js"
    ],
    "helpers": [
        "helpers/reporter.js"
    ],
    "stopSpecOnExpectationFailure": true,
    "random": false
}

================
File: jsdoc.json
================
{
    "source": {
        "include": [
            "src"
        ],
        "exclude": [
            "node_modules"
        ],
        "includePattern": ".+\\.js(doc|x)?$",
        "excludePattern": "(^|\\/|\\\\)_"
    },
    "opts": {
        "verbose": true,
        "recurse": true,
        "destination": "./docs/jsdoc"
    },
    "plugins": [
        "plugins/markdown"
    ]
}

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Danny Ayers

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: package.json
================
{
  "name": "semem",
  "version": "1.0.0",
  "description": "Semantic Memory",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "cov": "nyc -a --include=src --reporter=lcov npm run test",
    "docs": "jsdoc -c jsdoc.json",
    "rp": "repomix -c repomix.config.json . "
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/danja/semem.git"
  },
  "keywords": [
    "semantic",
    "memory",
    "llm",
    "rdf",
    "sparql"
  ],
  "author": "Danny Ayers",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/danja/semem/issues"
  },
  "homepage": "https://github.com/danja/semem#readme",
  "devDependencies": {
    "jasmine": "^5.5.0",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.4",
    "repomix": "^0.2.20"
  },
  "dependencies": {
    "@langchain/core": "^0.3.19",
    "@langchain/openai": "^0.3.14",
    "faiss-node": "^0.5.1",
    "graphology": "^0.25.4",
    "ml-kmeans": "^6.0.0",
    "ollama": "^0.5.10"
  }
}

================
File: repomix.config.json
================
{
    "output": {
        "filePath": "./repomix-semem.txt",
        "headerText": "Semem repo",
        "removeComments": true
    },
    "include": [
        "**/*",
        "docs/description_latest"
    ],
    "ignore": {
        "useDefaultPatterns": false,
        "customPatterns": [
            "data",
            "docs/jsdoc",
            ".nyc_output",
            ".env",
            "**/_*",
            "node_modules",
            "*.log",
            "**/*repomix*.txt",
            "**/*.html",
            "**/data/*",
            "**/*copy.js",
            "**/conversations.json"
        ]
    }
}
